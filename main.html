<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title></title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="main.tex"> 
<link rel="stylesheet" type="text/css" href="main.css"> 
</head><body 
>
<!--l. 118--><p class="noindent" >
                                                                  

                                                                  
<!--l. 118--><p class="indent" >
                                                                  

                                                                  
<div class="center" 
>
<!--l. 118--><p class="noindent" >
<!--l. 118--><p class="noindent" ><span 
class="ecrm-1728">Designing and Conducting Human-Robot</span>
<span 
class="ecrm-1728">Interaction Studies:</span><br />
<span 
class="ecrm-1728">Guidelines and an interactive specification</span>
<span 
class="ecrm-1728">approach</span>
<div class="tabular"> <table id="TBL-1" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-1-1g"><col 
id="TBL-1-1"></colgroup><tr  
 style="vertical-align:baseline;" id="TBL-1-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-1-1"  
class="td11"> <span 
class="ecrm-1200">Leimin Tian, Nicole Robinson, Pamela Carreno-Medrano, Wesley P. Chan, Maram Sakr, Tina Wu, Elahe Abdi, Elizabeth Croft, and Dana Kuli</span><span 
class="ecrm-1200">&#263;  </span></td>
</tr><tr 
class="vspace" style="font-size:9.99756pt"><td 
>&nbsp;</td></tr><tr  
 style="vertical-align:baseline;" id="TBL-1-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-1-2-1"  
class="td11">                                            <span 
class="ecrm-1200">Faculty of Engineering, Monash University                                                                     </span></td></tr></table>
</div>
<!--l. 118--><p class="noindent" ><span 
class="ecrm-1200">December 8, 2020</span></div>
   <div 
class="abstract" 
>
<div class="center" 
>
<!--l. 191--><p class="noindent" >
<!--l. 191--><p class="noindent" ><span 
class="ecbx-0900">Abstract</span></div>
     <!--l. 192--><p class="indent" >    <span 
class="ecrm-0900">Human-Robot Interaction (HRI) is a rapidly growing field. As personal</span>
     <span 
class="ecrm-0900">and  service  robotics  advance  into  a  range  of  markets,  the  success  of</span>
     <span 
class="ecrm-0900">HRI will determine the benefit society derives from robotics technology.</span>
     <span 
class="ecrm-0900">The field of HRI is facing a critical transformation that calls for clear</span>
     <span 
class="ecrm-0900">and measurable ways to evaluate the effectiveness and outcomes of an</span>
     <span 
class="ecrm-0900">HRI system, and to understand users&#8217; perceptions and experiences. To</span>
     <span 
class="ecrm-0900">improve the overall quality of HRI studies, high-quality and scientifically</span>
     <span 
class="ecrm-0900">rigorous methods are required to ensure the robustness of the results and</span>
     <span 
class="ecrm-0900">draw conclusions across studies. In addition, standardized methodologies</span>
     <span 
class="ecrm-0900">improve the validity, repeatability, and generalisability of the findings, and</span>
     <span 
class="ecrm-0900">provide guidance for the design of new systems. Thus, we are motivated</span>
     <span 
class="ecrm-0900">to  propose  a  set  of  HRI  study  methodology  guidelines  supported  by</span>
     <span 
class="ecrm-0900">reviews of existing methodological approaches in HRI, as well as in the</span>
     <span 
class="ecrm-0900">broader  fields  of  human-computer  interaction,  human-centred  artificial</span>
     <span 
class="ecrm-0900">intelligence, psychology and social science. Researchers are encouraged</span>
     <span 
class="ecrm-0900">to  follow  these  guidelines  when  designing  and  conducting  future  HRI</span>
     <span 
class="ecrm-0900">studies, which leads to high-quality study designs and robust outcomes</span>
     <span 
class="ecrm-0900">that advances the frontier of HRI research and applications.</span>
</div>
                                                                  

                                                                  
   <h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-10001"></a>Introduction</h3>
<!--l. 229--><p class="noindent" >The field of human-robot interaction (HRI) is a rapidly growing discipline of robotics
research, which focuses on how robots and humans interact or collaborate with one
another, and developing effective robot designs that facilitate such interaction or
collaboration. HRI studies often adopt a multidisciplinary theoretical and
experimental framework, including but not limited to robotics, behavioural and social
science. As a emergent field, existing HRI research contains mostly exploratory
studies with a less rigorous methodology. Most existing studies provide an initial
prototype of concepts and outcomes, for example, an uncontrolled pilot with a small
sample size or one key measurement to assess a factor or variable. As the field begins
to mature, it is vital to improve the quality of research, to allow findings to
be applied to a wider domain. Thus, we are motivated to identify a set of
methodological guidelines to improve the quality and robustness of HRI
studies.
<!--l. 232--><p class="indent" >   Beyond HRI, increased attention and importance has been placed on adopting
rigorous study methodology in multiple scientific fields. For example, the field of
behavioural and social science is experiencing a &#8220;replication crisis&#8221; [<a 
href="#Xmaxwell2015psychology">133</a>], in which
efforts to replicate existing studies often do not yield the same observations or
conclusions compared to the original experiment. This replication crisis damages the
validity of existing research findings, resulting in incorrect assumptions about
phenomena and subsequent waste of resources to revise or verify earlier work. Social
science researchers have been pushing towards awareness for good research
practices that ensure the validity, repeatability, and generalisability of the
studied phenomena, and human-robot interaction should follow in the same
path.
<!--l. 236--><p class="indent" >   There are a range of limitations and challenges in current HRI studies. A number
of reviews have been conducted to identify and address these challenges in HRI, such
as evaluation metrics (e.g.,&#x00A0;Belhassein et&#x00A0;al.&#x00A0;[<a 
href="#Xbelhassein2019towards">19</a>],&#x00A0;Leichtmann and Nitsch&#x00A0;[<a 
href="#Xleichtmann_how_2020">122</a>]).
However, an overview of the HRI study life cycle, which captures the whole process
from conceiving a research question to post-study follow-ups has not yet been
addressed. This is important because researchers following rigorous methodological
practice can help to reduce the prevalence of lower-quality scientific studies, helping
to avoid a possible replication crisis for HRI in the future. Moreover, HRI researchers,
especially those new to the field, can benefit from having a set of general HRI study
methodology guidelines that recommends best practices for each aspect of the
study life cycle. This paper proposes such guidelines, drawing on existing
HRI studies and research from related fields. For HRI researchers new to
the field, we offer an overview of the HRI study life cycle that introduces
them to good practices for conducting HRI studies. For experienced HRI
researchers, we provide in-depth analysis of strengths and weaknesses of
existing HRI methodological approaches illustrated with recent HRI studies, as
well as offering insights on novel methods that may be adopted in future
studies.
                                                                  

                                                                  
<!--l. 240--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">1.1   </span> <a 
 id="x1-20001.1"></a>Life cycle of an HRI study</h4>
<!--l. 241--><p class="noindent" >We propose a model the life cycle of a typical HRI study shown in
Figure&#x00A0;<a 
href="#x1-2001r1">1<!--tex4ht:ref: fig_Chronology --></a>. An interactive version of the guideline can be accessed at
<a 
href="https://tianleimin.github.io/HRI-Methodology-Guidelines/" class="url" ><span 
class="ectt-1000">https://tianleimin.github.io/HRI-Methodology-Guidelines/</span></a>. This tool can
assist researchers to develop their study design and to easily access relevant
guidelines at each step of the study. Each step in this HRI study life cycle will be
explained in this paper and we will discuss specific recommendations for key steps,
together with examples of existing HRI studies to illustrate these recommendations.
As HRI is a highly interdisciplinary research field, we will support our discussion
both with existing HRI studies and with methodological approaches from
complementary fields, including human-computer interaction, human-centred AI,
psychology and cognitive science.
<!--l. 355--><p class="indent" >   <hr class="figure"><div class="figure" 
>
                                                                  

                                                                  
                                                                  

                                                                  
<div class="center" 
>
<!--l. 356--><p class="noindent" >
<!--l. 357--><p class="noindent" ><img 
src="FlowChart.png" alt="PIC"  
width="345" height="345" >  <a 
 id="x1-2001r1"></a>
<a 
 id="x1-2002"></a>
<br />                                                                  <div class="caption" 
><span class="id">
Figure&#x00A0;1: : </span><span  
class="content">Life cycle of an HRI study (<a 
href="https://tianleimin.github.io/HRI-Methodology-Guidelines/" >click for an interactive version online</a>). </span></div><!--tex4ht:label?: x1-2001r1 -->
</div>
                                                                  

                                                                  
<!--l. 361--><p class="indent" >   </div><hr class="endfigure">
   <h4 class="subsectionHead"><span class="titlemark">1.2   </span> <a 
 id="x1-30001.2"></a>Aims and scope</h4>
<!--l. 364--><p class="noindent" >Unlike existing HRI methodology reviews that summarise current approaches, we
focus on identifying best practices for various types of HRI studies, and
aim to provide specific guidelines applicable to key steps during an HRI
study cycle. We begin with an overview of existing reviews and guidelines for
HRI study methodology in Section&#x00A0;<a 
href="#x1-40002">2<!--tex4ht:ref: sec:past --></a> to provide the rationale for such HRI
methodology guidelines. We then discuss the key steps of the proposed life
cycle methodology, illustrated with examples of HRI studies, methodological
approaches, and application domains in Sections&#x00A0;<a 
href="#x1-70003">3<!--tex4ht:ref: sec:type --></a>, <a 
href="#x1-120004">4<!--tex4ht:ref: sec:hri --></a>, <a 
href="#x1-270005">5<!--tex4ht:ref: sec:par --></a>, and <a 
href="#x1-420006">6<!--tex4ht:ref: sec:eva --></a>. Thus, our
work provides a set of applicable and practical guidelines to improve the
quality of HRI studies, particularly beneficial for researchers new to the
field.
<!--l. 373--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-40002"></a>Background</h3>
<!--l. 377--><p class="noindent" >Existing guidelines for conducting HRI studies either introduce researchers to
an overview of the study process (e.g.,&#x00A0;Belhassein et&#x00A0;al.&#x00A0;[<a 
href="#Xbelhassein2019towards">19</a>]), or focus on
reviewing a specific aspect of HRI study, such as one style of interaction (e.g.,
Wizard-of-Oz (WoZ) style studies [<a 
href="#Xriek2012wizard">162</a>]), one type of assessment approaches (e.g.,
psychophyciological assessment [<a 
href="#Xirfan2018social">102</a>]), one key step during the HRI study life cycle
(e.g., evaluation [<a 
href="#Xschrum2020four">180</a>]), one application domain (e.g., healthcare [<a 
href="#Xrobinson2019psychosocial">166</a>]), or one cohort
of participants (e.g. elderly [<a 
href="#Xzafrani_towards_2019">231</a>]). In this section, we will discuss representative
examples of existing reviews and guidelines for HRI methodology, and provide
motivation for this work. In particular, the HRI methodology guidelines we offer
draw from a variety of HRI studies to provide a holistic view, while focusing on
recommending suggestions applicable to all the core steps during an HRI study
cycle.
<!--l. 379--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.1   </span> <a 
 id="x1-50002.1"></a>Existing reviews and guidelines on HRI study methodology</h4>
<!--l. 381--><p class="noindent" >Current HRI research is in urgent need of a scientifically rigorous methodology that
ensures the validity and generalisability of the findings. A review of the literature
reveals that many HRI studies are designed and executed based on the preferences of
researchers and available resources. For example, less than 10% of studies on social
robots for healthcare have deployed randomized controlled trials for addressing
potential biases when testing the effectiveness of the social robots, with
even fewer conducting follow-up studies [<a 
href="#Xrobinson2019psychosocial">166</a>]. In the elder care domain,
two-thirds of the studies had fewer than 30 participants, while the majority
lasted a day, and only 11% lasted 30 days or more. Most importantly, 90% of
                                                                  

                                                                  
the studies lacked a theoretical framework derived from human research
[<a 
href="#Xzafrani_towards_2019">231</a>].
<!--l. 384--><p class="indent" >   <span 
class="ecbx-1000">Categorisation of HRI studies: </span>HRI studies have been categorised in multiple
review papers based on a number of attributes. Various taxonomies have been
proposed in the literature for classifying HRI. These include taxonomies classifying
HRI by the type of study [<a 
href="#Xdamacharla2018common">45</a>], the role of the robot [<a 
href="#Xcollins_drawing_2019">41</a>], its morphology [<a 
href="#Xyanco_classifying_2004">227</a>], level of
autonomy [<a 
href="#Xthrun2004toward">206</a>,&#x00A0;<a 
href="#XBeer_Fisk_Rogers_2014">18</a>], behaviour [<a 
href="#Xshim_taxonomy_2013">187</a>] and characteristics [<a 
href="#Xrahimi_research_1990">155</a>], the role of the human
[<a 
href="#Xscholtz2003theory">179</a>], type of task [<a 
href="#Xyanco_classifying_2004">227</a>,&#x00A0;<a 
href="#Xsteinfeld_common_2006">192</a>] and interaction [<a 
href="#Xseibt_towards_2017">183</a>,&#x00A0;<a 
href="#Xolsen2003metrics">146</a>] as well as human-robot
proximity [<a 
href="#Xgoodrich2007human">75</a>,&#x00A0;<a 
href="#Xagah_human_2000">3</a>] and coordination [<a 
href="#Xjiang_mixed-initiative_2015">104</a>,&#x00A0;<a 
href="#Xhoffman2019evaluating">90</a>]. The HRI study themes, types, and
challenges as of the year 2008 were reviewed by Goodrich and Schultz&#x00A0;[<a 
href="#Xgoodrich2007human">75</a>]. Five
attributes of HRI were highlighted including autonomy, task type and structure of
the human-robot team. In a similar approach, Baxter et&#x00A0;al.&#x00A0;[<a 
href="#Xbaxter2016characterising">17</a>] categorised papers
accepted at the ACM/IEEE International Conference on Human-Robot
Interaction between years 2013-2015 based on level of robot autonomy, participant
populations, evaluation environments, length of empirical studies, approach to
statistics and reproducibility. In Baxter et&#x00A0;al.&#x00A0;[<a 
href="#Xbaxter2016characterising">17</a>] the authors provide a set of
recommendations, highlighting the importance of clarity of the research goal, level
of robot autonomy and statistical analysis, as well as justification of the
number of participants, length of interaction and reproducibility of the study.
Establishment of standards and common metrics is reported as an emerging effort,
however, some years later, the HRI community is still moving towards this
goal.
<!--l. 387--><p class="indent" >   <span 
class="ecbx-1000">Hypotheses based on human-human interaction theories: </span>Hypotheses in
HRI studies are often grounded by theories from other relevant disciplines, such as
psychology and cognitive science. Thus, it is inevitable that the recent replication
crisis in psychology has also influenced the field of HRI. For example, Irfan
et&#x00A0;al.&#x00A0;[<a 
href="#Xirfan2018social">102</a>] presented their failures to replicate the social facilitation theory in HRI.
Based on this experience, they recommended HRI researchers who are consumers of
psychology literature to avoid older psychology literature with weak methods, and
encouraged bench-marking efforts in HRI. Their work highlighted the challenges of
observing and measuring social interaction in HRI. Similarly, Leichtmann and
Nitsch&#x00A0;[<a 
href="#Xleichtmann_how_2020">122</a>] reviewed HRI studies on human-robot physical and social distance, and
investigated whether it is appropriate to generate hypotheses for HRI studies based
on human-human interaction (HHI) theories. They found that HRI study
hypotheses are often derived from HHI under the assumption that robots
are treated as social actors in a manner similar to humans. However, this
assumption may not always be valid. Based on their review, the authors
recommended pre-registration of hypotheses, methodology, and analysis
protocol, as well as transparent reporting of HRI studies grounded with HHI
theories.
<!--l. 390--><p class="indent" >   <span 
class="ecbx-1000">Evaluation in HRI studies: </span>Most reviews of HRI methodology have focused on
the evaluation aspect, and provide recommendations regarding the design phase of
the HRI study life cycle. A decade ago, Bethel and Murphy [<a 
href="#Xbethel2010review">25</a>] identified study size
and lack of multiple assessment methods as the primary issues in existing studies.
They argued that a large sample size better represents the target population and
exhibits a higher probability for statistically significant results. They also suggested
                                                                  

                                                                  
that the deployment of three or more evaluation metrics for the same construct
produces more reliable and accurate results. Currently, many metrics used in HRI
studies remain &#8220;observed&#8221; with a lack of functional/generalizable measurement
mechanisms. For example, Murphy and Schreckenghost&#x00A0;[<a 
href="#Xmurphy2013survey">142</a>] proposed an
HRI metric taxonomy based on a review of 29 studies. A total of 42 metrics
were categorised into three groups: human (e.g., reliability, productivity
and awareness), robot (e.g., plan state and time in manual/autonomous
operation), and system (e.g., efficiency, safety and coactivity). Psychological
assessment metrics such as cardiovascular, electro-dermal and brain activity are
increasingly used to evaluate human responses in HRI [<a 
href="#Xtiberio2013psychophysiological">207</a>]. When studying
social interactions and human responses in HRI, the most commonly adopted
approach is to use the Likert scale to measure subjective judgements of
participants. However, a recent review on the use of the Likert scale in HRI
studies found that only 3 out of the 110 papers performed their analyses
correctly&#x00A0;[<a 
href="#Xschrum2020four">180</a>]. This demonstrated the urgent need for methodological guidelines
to ensure the quality and robustness of HRI studies, especially regarding
the validity of evaluation methods and appropriate analyses of different
metrics.
<!--l. 393--><p class="indent" >   <span 
class="ecbx-1000">Post-study and reproducibility: </span>In contrast to reviews focusing on the design
phase of HRI studies, Belhassein et&#x00A0;al.&#x00A0;[<a 
href="#Xbelhassein2019towards">19</a>] provided recommendations for the
experiment and post-study phases of HRI user studies with a focus on participants
and the replication crisis. Regarding the experiment phase, they suggested that the
outcome of the HRI study can be improved by recruiting more users with diverse
backgrounds, rigorous implementation of the protocol, allowing time for the user to
get accustomed to the robot, ensuring physical and psychological safety, assessing
suitability of the robot for the task, choosing the right and preferably standardized
measurements, deployment of theoretically solid tools and measures and finally
providing required details for reproducing the study. Regarding the post-study phase,
they suggested standardization of the technical components to address study
reproducibility.
<!--l. 396--><p class="indent" >   <span 
class="ecbx-1000">Evaluation of HRI systems&#8217; usability and technical limitations: </span>Many
HRI systems are designed with the goal of application to education, health,
and elderly care [<a 
href="#Xdautenhahn2007methodology">46</a>]. This relies on advancements in sensory technologies
and algorithms in robotics [<a 
href="#Xthomaz_computational_2016">204</a>]. Furthermore, to evaluate the outcomes of
such HRI systems, it is critical to evaluate HRI in-situ. For example, Weiss
et&#x00A0;al.&#x00A0;[<a 
href="#Xweiss2009usus">218</a>] proposed the usability, social acceptance, user experience, and
societal impact (USUS) framework. To envision new research questions beyond
existing technical restraints, the Wizard of Oz (WoZ) approach is often applied
in HRI studies, especially when studying complex social interactions. The
WoZ approach facilitates experiment safety and control of variable factors
in HRI. Riek&#x00A0;[<a 
href="#Xriek2012wizard">162</a>] reviewed how WoZ has been used in HRI in order to
identify valid WoZ designs. They stressed the importance of operator training
and instruction, as well as the formation of hypotheses on user and robot
behaviors.
<!--l. 399--><p class="indent" >   <span 
class="ecbx-1000">Focused reviews of HRI study components: </span>Existing reviews of HRI
methodology often focus on one particular aspect or evaluation method. For example,
                                                                  

                                                                  
Lasota et&#x00A0;al.&#x00A0;[<a 
href="#Xlasota_survey_2017">119</a>] reviewed factors influencing safety in HRI and different
measurements of safety; Young et&#x00A0;al.&#x00A0;[<a 
href="#Xyoung_evaluating_2011">230</a>] reviewed the use of biological features
and means to incorporate user variances; Prewett et&#x00A0;al.&#x00A0;[<a 
href="#Xprewett2009workload">151</a>] reviewed the
measurement of robot teleoperator workload in relation to task outcomes. It is
important to have such in-depth investigations focusing on one type of HRI or on one
aspect of HRI evaluation. However, identifying a set of guidelines that inform on core
steps of an HRI study cycle is crucial to improve the overall quality of HRI
studies.
<!--l. 402--><p class="indent" >   <span 
class="ecbx-1000">Comprehensive overviews of HRI studies: </span>Bartneck et&#x00A0;al.&#x00A0;[<a 
href="#Xbartneck2020human">16</a>] provided a
comprehensive overview of HRI studies. In this report of good practices in HRI
studies, research questions (explanatory vs. confirmatory), study designs (qualitative
vs. quantitative vs. mixed methods), participants (choice of population and sample
size), context (location, time and social unit), robot appearance and functionality,
mode of interaction (WoZ vs. physical vs. simulated) as well as direct and indirect
metrics, statistical analysis and ethical considerations are covered. Their
discussion about the (dis)advantages of each approach and analysis of the best
practices provides a comprehensive introduction to HRI study. Similarly,
Hoffman and Zhao&#x00A0;[<a 
href="#Xhoffman2020primer">92</a>] proposed an introduction to HRI methodology and a
tutorial for researchers new to the field. A hypothetical HRI study was used as
an example to illustrate the process of how an HRI study is conceived is
conducted.
<!--l. 405--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">2.2   </span> <a 
 id="x1-60002.2"></a>Summary</h4>
<!--l. 406--><p class="noindent" >The existing HRI study methodology review papers are mainly focused on reporting
the current common practices. They lack an overview of the full HRI study cycle and
an overall guideline for identifying the best study protocols and evaluation metrics.
Further, in these studies, discussion about the (dis)advantages of each approach and
analysis of the best practices are generally limited.
<!--l. 408--><p class="indent" >   Because of the wide variation in research questions and experimental settings
in HRI, a one-size-fits all methodological approach across all HRI studies
is not a reasonable expectation. However, we have identified five shared
recommendations emerging from existing reviews of HRI studies covered in this
section:
<!--l. 410--><p class="indent" >   <span 
class="ecbx-1000">Shared recommendations emerging from existing HRI reviews</span>
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-6002x1"><span 
class="ecbx-1000">Relevance  of  psychology  and  social  science  theories  to  HRI</span>:
     Grounding HRI hypotheses with theories in psychology and social science
     helps to increase the strength of the HRI hypotheses. However, researchers
     should also be aware of the differences between human-human interaction
     and HRI, which can limit the applicability of psychology or social science
     theories in HRI.
     </li>
                                                                  

                                                                  
     <li 
  class="enumerate" id="x1-6004x2"><span 
class="ecbx-1000">The importance of context</span>: HRI is context dependant. Researchers are
     encouraged to have an interdisciplinary perspective when conducting HRI
     studies, and to incorporate social and interactive context in their research,
     such as the target user population or the location of the interaction.
     </li>
     <li 
  class="enumerate" id="x1-6006x3"><span 
class="ecbx-1000">Pay attention to evaluation</span>: The validity of evaluation metrics used
     is  directly  related  to  the  validity  of  an  HRI  study.  Researchers  are
     recommended  to  use  multiple  metrics  and  evaluation  approaches  to
     measure a parameter, and to investigate the convergence of these metrics.
     In addition, researchers are encouraged to use appropriate statistical tests
     and conduct power analyses when studying experimental results.
     </li>
     <li 
  class="enumerate" id="x1-6008x4"><span 
class="ecbx-1000">Ecological validity of a study</span>: Researchers are encouraged to conduct
     field  studies  and  longitudinal  HRI  studies.  In  HRI  experiments,  it  is
     recommended to recruit participants from the target populations and pay
     attention to sample size when analyzing the results.
     </li>
     <li 
  class="enumerate" id="x1-6010x5"><span 
class="ecbx-1000">Rigour and transparency are key</span>: To improve the reproducibility and
     generalizability of HRI studies, researchers are encouraged to pre-register
     their hypotheses. They should also consult their target publication venues
     on  whether  or  not  pre-registration  is  requested.  Researchers  should
     cover both significant and non-significant results in their discussion, and
     open-source the code and data used in their work.</li></ol>
<!--l. 419--><p class="indent" >   In the following sections, we will investigate each of these aspects of HRI study
and discuss the best practices for different types of study. In particular, we will cover
hypotheses and study context in Section&#x00A0;<a 
href="#x1-70003">3<!--tex4ht:ref: sec:type --></a>, evaluation methods in Section&#x00A0;<a 
href="#x1-120004">4<!--tex4ht:ref: sec:hri --></a>,
participants and ecological validity in Section&#x00A0;<a 
href="#x1-270005">5<!--tex4ht:ref: sec:par --></a>, and reporting metrics in
Section&#x00A0;<a 
href="#x1-420006">6<!--tex4ht:ref: sec:eva --></a>.
<!--l. 652--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">3   </span> <a 
 id="x1-70003"></a>Hypothesis in HRI Studies</h3>
<!--l. 654--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.1   </span> <a 
 id="x1-80003.1"></a>From research question to hypothesis</h4>
<!--l. 655--><p class="noindent" >The experimental design process begins with refining the research question(s) that
the experimenter wishes to explore. An effective method for developing strong
research questions begins with a problem statement describing a gap in
knowledge. The identification of this gap can be developed through literature
review [<a 
href="#XHRItextbook">14</a>], personal observation, drawing similarities to other fields [<a 
href="#Xcollins_drawing_2019">41</a>] or
exploratory/pilot studies, among other methods. Once a research question is defined,
                                                                  

                                                                  
a hypothesis can be developed to formulate a testable claim to answer the research
question.
<!--l. 657--><p class="indent" >   Research studies can be generally classified into exploratory or confirmatory
studies [<a 
href="#XHRItextbook">14</a>]. In exploratory studies, the researcher aims to investigate a relatively new
idea or area where there is not much prior knowledge to draw expectations from. For
example, in a study exploring how humans would interact with robots when robots
begin to enter people&#8217;s homes in the future, the investigators designed five likely
domestic scenarios and studied how participants responded to different robot
behaviours in these scenarios [<a 
href="#XKoay2009">114</a>]. In confirmatory studies, also known as hypothesis
testing, the researcher aims to confirm certain expectations about how their system
or users would perform or behave. For example, based on the knowledge that
non-verbal communications play an important role in human-human collaborations,
the experimenters in [<a 
href="#Xbreazeal_effects_2005">31</a>] designed a study to confirm the importance of non-verbal
communication in human-robot interaction. Focusing on confirmatory HRI studies, a
key initial step is to develop clear hypotheses about the user(s) and the
robot(s).
<!--l. 659--><p class="indent" >   A hypothesis is a predictive statement on how a given condition will
affect a certain aspect of the outcome. The hypothesis drives the selection of
the type of the study and the method of evaluation [<a 
href="#Xbethel2010review">25</a>]. It should clearly
identify key constructs, be testable, and concise [<a 
href="#XRind_2011">163</a>]. As an example, in
an HRI study examining the effects of non-verbal communication on the
efficiency in human-robot teamwork [<a 
href="#Xbreazeal_effects_2005">31</a>], the experimenters hypothesized
that:
     <ul class="itemize1">
     <li class="itemize">"implicit  non-verbal  communication  positively  impacts  human  robot
     task  performance  with  respect  to  understandability  of  the  robot,
     efficiency of task performance, and robustness to errors that arise from
     miscommunication."</li></ul>
<!--l. 665--><p class="indent" >   The above is a statement relating the given condition of communication method
used, to the outcome of task performance. It names key constructs, including implicit
communication and robot understandability, making a concise prediction that, by
using implicit non-verbal communication, robot understandability, task efficiency,
and robustness to errors from miscommunication will be improved. As the aim of
user studies is to find support for the formulated hypotheses, a good hypothesis
should be testable in a scientific experiment. The condition stated in the hypothesis
should involve a factor, often referred to as independent variable, that the researcher
can vary and control independently in an experiment. The outcome stated in the
hypothesis, also known as the dependent variable, should be a parameter that is
measurable or observable in an experiment setup. Considering the above example, the
factor of robot communication method was controlled independently from other
factors by the researchers through employing either explicit communication
only, or both explicit communication and implicit non-verbal behaviors.
The outcome of task performance was measured through questionnaires
asking users to rate the effectiveness of the interaction, as well as video
                                                                  

                                                                  
analysis. While formulating a hypothesis has the benefits of focusing the
research, and guiding the design of the experiment, researchers should also be
careful not to be biased by the formulated hypotheses or over reliant on
theory, such that they become blinded to unexpected interesting findings
[<a 
href="#XRind_2011">163</a>].
<!--l. 668--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.2   </span> <a 
 id="x1-90003.2"></a>Building hypotheses based on HRI attributes </h4>
<!--l. 670--><p class="noindent" >When formulating hypotheses, it is helpful to think from a designer&#8217;s perspective as
suggested by Goodrich and Schultz [<a 
href="#Xgoodrich2007human">75</a>]. They defined HRI as "a field of study
dedicated to understanding, designing, and evaluating robotic systems for use by or
with humans". In their paper, they listed five attributes that the designer can change
to affect HRI: (1) level and behaviour of autonomy, (2) nature of information
exchange, (3) structure of team, (4) adaptation, learning, and training of people and
robot, (5) nature of the task.
<!--l. 672--><p class="indent" >   A common strategy for formulating hypotheses is then to consider which attribute
is the main concern of the study. Once the main attribute of concern has been
identified, the levels for testing this attribute can be determined. These attribute
settings would then naturally become the independent variables in the hypotheses.
The dependent variables stated in a hypothesis then often relate to what the designer
hopes to achieve.
<!--l. 674--><p class="indent" >   In the following paragraphs, we take a closer look at the five attributes mentioned
above.
<!--l. 676--><p class="indent" >   <span 
class="ecbx-1000">Level and Behaviour of Autonomy</span>. Autonomy is defined as the robot&#8217;s
ability to accommodate variations in its environment [<a 
href="#Xthrun2004toward">206</a>] by sensing, planning and
acting to reach a goal without external control [<a 
href="#XBeer_Fisk_Rogers_2014">18</a>]. Autonomy can range between
manual teleoperation to full autonomy. Beer et al. have proposed a 10-point
taxonomy to identify the level of robot autonomy along this spectrum [<a 
href="#XBeer_Fisk_Rogers_2014">18</a>]. This is
directly related to when and how the human or robot should take the initiative in
HRI. Seizing the initiative can happen in a reactive, deliberative or hybrid manner
[<a 
href="#Xjiang_mixed-initiative_2015">104</a>].
<!--l. 679--><p class="indent" >   In a study related to this attribute, a hypothesis may choose the robot&#8217;s level of
autonomy as the independent variable and explore its effect on a range of dependent
variables such as participant workload and robot perceived trustworthiness
[<a 
href="#XRau2013">158</a>]. The measurement of these dependent variables is further discussed in
Sec&#x00A0;<a 
href="#x1-160004.2">4.2<!--tex4ht:ref: subsec:constructs_n_metrics --></a>.
<!--l. 681--><p class="indent" >   <span 
class="ecbx-1000">Nature of Information Exchange</span>. The nature of information exchange is
primarily defined by the communication channel and communication format [<a 
href="#Xgoodrich2007human">75</a>].
These can be affected by human-robot interaction distance - e.g., whether they are
physically attached, in the same area, or in different areas [<a 
href="#Xagah_human_2000">3</a>]. Communication
channels used for human-robot interaction often mimic those in human-human
interaction. Such channels, and example formats, include visual, such as gaze [<a 
href="#Xadmoni2017gaze">2</a>] or
body gestures [<a 
href="#XLiu2018gesture">128</a>]; auditory, such as speech [<a 
href="#Xyongda2018speechAndGesture">229</a>] or audio-based expressions [<a 
href="#Xfrederiksen2019audio">67</a>];
and haptic, such as shared load [<a 
href="#Xchan2013handover">35</a>] or vibrations [<a 
href="#Xscheggi2014vibrotactile">178</a>]. A common strategy for
                                                                  

                                                                  
enabling intuitive/effective HRI is by programming robots to use communication
channels and formats familiar to humans [<a 
href="#Xhaddadi2013gestures">78</a>,&#x00A0;<a 
href="#Xgielniak2012exageration">74</a>,&#x00A0;<a 
href="#Xparker2010jstrips">147</a>].
<!--l. 684--><p class="indent" >   Considering the first example in this section [<a 
href="#Xbreazeal_effects_2005">31</a>], the attribute of main concern is
the nature of information exchange. The hypothesis considers the communication
method as the independent variable, comparing the combined use of implicit
non-verbal behaviors and explicit communication, or the latter only. The
dependent variables are robot understandability, task efficiency and robustness to
errors.
<!--l. 686--><p class="indent" >   <span 
class="ecbx-1000">Structure of Team</span>. Structure of team concerns the number (type) of humans
and robots, their roles, and the communication links among them. Directly relating
to these factors, Yanco and Drury&#8217;s proposed taxonomy includes the ratio of people
to robots, composition of robot teams, level of shared interaction among teams, and
human role [<a 
href="#Xyanco_classifying_2004">227</a>]. The human may take a range of roles from supervisor and operator,
to mechanic, bystander, or teammate [<a 
href="#Xscholtz2003theory">179</a>]. The role of the robot can be identified by
comparing HRI to human interaction with other agents, including other
humans, animals and objects and defining the role of the robot based on the
similarity of its role to those agents [<a 
href="#Xcollins_drawing_2019">41</a>]. This can also be affected by the
robot&#8217;s morphology [<a 
href="#Xyanco_classifying_2004">227</a>]. An anthropomorphic, zoomorphic or functional
robot is more likely to be deployed in roles matching its shape. In social
robots, the ability of the robot to deceive has been suggested as an additional
role capacity categorised based on the deception object, goal, and method
[<a 
href="#Xshim_taxonomy_2013">187</a>].
<!--l. 690--><p class="indent" >   Structure of the team and role of team members are among independent variables
that can potentially shape the basis of HRI hypotheses. Measurable dependent
variables e.g. likability, trustworthiness and team performance, should then be chosen
based on the study main research question(s). For example, [<a 
href="#Xrosenthal-vonderputten_experimental_2013">168</a>] examined humans&#8217;
emotional reaction to the positive/negative treatment a robot receives in videos and
based their hypotheses on existing literature in psychology. The independent
variables were either manipulated within the experiment (e.g. different videos) or
based on the user&#8217;s personality traits. The outcomes (e.g., participants&#8217; emotional
state and physiological arousal) were then evaluated via questionnaires or
physiological measures.
<!--l. 692--><p class="indent" >   <span 
class="ecbx-1000">Adaptation, Learning, and Training of People and Robot</span>. Adaptation,
learning, and training can happen for both human and robot in HRI. Most HRI
designs aim to develop intuitive interfaces for the human user. Therefore,
robotic systems for HRI are often designed to minimize the amount of user
training required [<a 
href="#Xgoodrich2007human">75</a>]. However, there is always some human learning and
adaptation taking place [<a 
href="#Xgreen2003learnability">76</a>]. Short-term HRI studies commonly treat learning in
humans as an undesirable carryover effect to be mitigated. In longitudinal
studies, adaptation and learning warrant careful study and may be the foci
of the studies [<a 
href="#Xlee2007habituation">121</a>]. Certain high risk application domains, such as bomb
disposal, can benefit from or may require careful training of the user or operator
[<a 
href="#Xrehfeld2006training">161</a>].
<!--l. 695--><p class="indent" >   Robot adaptation has been used to personalize interaction and promote longer
term engagement with users [<a 
href="#Xahmad2017adaptive">4</a>,&#x00A0;<a 
href="#Xtozadore2018adaptation">208</a>]. Robots can also learn tasks and skills from
humans through various approaches, such as Learning from Demonstration [<a 
href="#Xargall2009learning">11</a>]. In
                                                                  

                                                                  
the context of education, robots have taken the role of teachers or peer tutors for
teaching students [<a 
href="#Xedwards2008teacher">56</a>], while robotic pets have also been used as therapeutic animals
for treating anxiety by teaching patients to adapt their breathing to the robot&#8217;s
[<a 
href="#Xsefidgar2012tamer">182</a>].
<!--l. 698--><p class="indent" >   Training period and mode are examples of independent variables in a hypothesis
which might affect various outcomes such as user engagement and receptiveness. For
example, a study on personal space examined people&#8217;s behaviour (adaptation)
around robots depending on past experiences, among other factors [<a 
href="#Xtakayama_influences_2009">199</a>].
Their four hypotheses presented clearly how these independent variables
are expected to influence measurable outcomes (e.g. personal space around
robots).
<!--l. 700--><p class="indent" >   <span 
class="ecbx-1000">Nature of the Task</span>. Introduction of new technologies may change how
people fundamentally perform a task [<a 
href="#Xgoodrich2007human">75</a>]. Hence, it is important to consider
how people interact with the robot or perform the task when a new robot
technology is introduced. Task type and task criticality (measuring importance of
task outcome) [<a 
href="#Xyanco_classifying_2004">227</a>] as well as its cognitive/physical requirements [<a 
href="#Xrahimi_research_1990">155</a>] are
introduced as part of the HRI taxonomy. The robot&#8217;s level of autonomy is
an influential factor in determining the type of tasks it can perform [<a 
href="#Xthrun2004toward">206</a>].
The aspects of the task the robot should perform and the extent to which
the robot can perform those tasks affect the choice of the robot and the
study design [<a 
href="#XBeer_Fisk_Rogers_2014">18</a>]. A sound theoretical foundation ensures that the robot is
selected based on its suitability for the task rather the expected outcome
[<a 
href="#Xcollins_drawing_2019">41</a>].
<!--l. 704--><p class="indent" >   The introduction of new methods and technologies, such as kinesthetic teaching
[<a 
href="#XWrede2013">225</a>] and augmented reality [<a 
href="#Xchan2020AR">36</a>], has fundamentally changed how humans can carry
out tasks or interact with robots. For example, in a study examining methods for a
robot programming task, the hypotheses related the independent variable of task
shape (e.g., teleoperation or kinesthetic) to the outcomes of interaction ease and skill
performance [<a 
href="#XAkgun2011">6</a>].
<!--l. 706--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.3   </span> <a 
 id="x1-100003.3"></a>Pre-registration of hypotheses</h4>
<!--l. 707--><p class="noindent" >In certain fields, such as psychology, to improve rigour in studies, it is recommended
to register the hypotheses prior to conducting the study, to ensure that the
methodology of the study is justified based on its hypothesis, rather than the
expected outcome, to avoid bias. Researchers can register their hypotheses at
websites such as the Center for Open Science (<a 
href="https://osf.io/prereg" class="url" ><span 
class="ectt-1000">https://osf.io/prereg</span></a>),
AsPredicted (<a 
href="https://aspredicted.org" class="url" ><span 
class="ectt-1000">https://aspredicted.org</span></a>), or the U.S. National Library of Medicine
(<a 
href="https://clinicaltrials.gov" class="url" ><span 
class="ectt-1000">https://clinicaltrials.gov</span></a>). Results are then reported with reference to the
initial hypotheses to avoid a selective report based on the desired outcome. Some
journals (e.g., JMIR Research Protocols) may automatically generate an identifier for
submitted and published research protocols or proposals. The identifier is then linked
to all subsequent result papers. While registering hypotheses is not yet common
practice in the field of HRI, it may eventually be adopted for the reasons mentioned
                                                                  

                                                                  
above.
<!--l. 710--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">3.4   </span> <a 
 id="x1-110003.4"></a>Summary</h4>
<!--l. 712--><p class="noindent" >Identifying the research question and corresponding hypothesis marks the beginning
of the experimental design process. The hypothesis should be based on sound
background, and testable with an experiment. It should be deigned to shed light on
the research question. A clear hypothesis should concisely relate the independent
variables to the dependent variables. We suggest the five discussed HRI
attributes as the starting point for identifying the independent variables.
These variables are manipulated in the experiment to study their effects on
the outcome. Once the independent variables have been identified, the set
of values to be tested and thus the dependent variables can then be also
decided.
<!--l. 714--><p class="indent" >   Formulating a hypothesis is a critical initial step in any confirmatory HRI study.
The hypotheses sets the ground for the next steps of the study, from experimental
protocol and design of evaluation methods, to recruitment of participants, evaluation
metrics, statistical analysis and reporting the outcomes. Each of these steps are
discussed in detail in the following sections.
<!--l. 716--><p class="indent" >   <span 
class="ecbx-1000">Recommendations for forming HRI study hypothesis</span>
     <ul class="itemize1">
     <li class="itemize">A hypothesis is developed to formulate a testable claim to answer the
     research question.
     </li>
     <li class="itemize">A  hypothesis  is  a  predictive  statement  stating  how  an  experimental
     condition(s) is expected to affect a measured outcome(s).
     </li>
     <li class="itemize">A good hypothesis should be testable, concise, and name key constructs.
     </li>
     <li class="itemize">When constructing experiment hypotheses for HRI studies, it is useful to
     consider the various design factors that can be altered to affect HRI.
     </li>
     <li class="itemize">To  improve  rigour,  the  experimenter  may  consider  registering  the
     hypotheses prior to conducing the study.</li></ul>
<!--l. 917--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">4   </span> <a 
 id="x1-120004"></a>Experiment Design and Evaluation</h3>
<!--l. 919--><p class="noindent" >This section examines the design and implementation of an experimental study in
human-robot interaction. Once the hypothesis is formulated, the next step is to
                                                                  

                                                                  
determine the study design, parameters and select the appropriate evaluation
methods that are suited to explore the proposed hypothesis. A non-exhaustive list of
study components are presented below. Each of these components will be explored in
more detail, identifying advantages, disadvantages, and considerations in the
following section.
     <ul class="itemize1">
     <li class="itemize">Study design structure
     </li>
     <li class="itemize">Baseline and control
     </li>
     <li class="itemize">Choice of constructs and metrics
     </li>
     <li class="itemize">Choice of measurements
     </li>
     <li class="itemize">Trial location (laboratory, field, and online)
     </li>
     <li class="itemize">Session  frequency  and  duration  (single-session  or  multiple  follow-up
     sessions)
     </li>
     <li class="itemize">Type and number of robot(s) (humanoid, zoomorphic, nonbiomimetic)
     </li>
     <li class="itemize">Task behaviour of the robot (actions or behaviours)</li></ul>
<!--l. 932--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">4.1   </span> <a 
 id="x1-130004.1"></a>Study Model</h4>
<!--l. 933--><p class="noindent" >Researchers must select the type of study design and number of groups needed to
explain the hypothesis. Common study designs are the following: within-subjects,
between-subjects, or mixed-model factorial approach [<a 
href="#XChristensenLarryB.2007Em">40</a>]. This includes the
consideration of a control group in the study design and will be explained in more
detail below.
<!--l. 935--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">4.1.1   </span> <a 
 id="x1-140004.1.1"></a>Study Design Structure</h5>
<!--l. 935--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; <span 
class="ecbx-1000">Within-subjects Design </span>&#x00A0;<br 
class="newline" />&#x00A0; In a within-subjects design, there is one group of participants. Each participant is
                                                                  

                                                                  
exposed to all of the experimental conditions. An experimental condition can be
made up of a series of tasks, sessions or interaction patterns. An advantage of a
within-subjects design is that a smaller sample size can be used, which is helpful in
situations where participants are difficult to recruit [<a 
href="#Xlazar2017research">120</a>]. Other advantages include
the possibility of performing experiments in a single session sequentially (i.e., more
time efficient) and resulting in smaller statistical noise (e.g., the scores of
the same participant is compared between each experimental condition)
[<a 
href="#XChristensenLarryB.2007Em">40</a>].
<!--l. 939--><p class="indent" >   However, a longer experiment duration might increase the likelihood of
participants being affected by non-intended incidents, such as program glitches [<a 
href="#Xkrauth2000experimental">115</a>].
Long experimental duration and repeated task exposure can also lead to
participants anticipating the experiment outcome, habituation, practice
effect, and/or fatigue, resulting in careless task performance, or increased
performance due to experience. Within subject design is further prone to
cross-condition contamination, where exposure to one condition context affects the
participant&#8217;s response during the second condition, sometimes known as the Halo
effect (e.g. [<a 
href="#Xyamashita2016path">226</a>]). Sequence effects can also occur based on the sequence of
conditions presented [<a 
href="#Xsalkind2010encyclopedia">173</a>]. Researchers can consider using a Latin square
design and/or randomization to control the impact of habituation and other
confounding variables, and incorporate necessary breaks between experiments
[<a 
href="#Xlazar2017research">120</a>].
<!--l. 943--><p class="indent" >   <span 
class="ecbx-1000">Between-subjects Design </span>&#x00A0;<br 
class="newline" />&#x00A0; In between-subjects designs, participants are allocated to one group only from two
or more groups. Participants in each group are then exposed to one experimental
condition. This could be one task, session or interaction pattern. The number of
participant groups is determined based on the number of conditions. A strength of
between-subjects design is that it minimises confounding variables such as learning
effect, fatigue, and frustration that occur as the byproducts of running long
experimental sessions [<a 
href="#XChristensenLarryB.2007Em">40</a>,&#x00A0;<a 
href="#Xlazar2017research">120</a>].
<!--l. 946--><p class="indent" >   However, since participants are different between each experimental condition,
collected data will be affected by individual differences without randomization (i.e.,
random allocation of participant groups into experimental conditions). As a result, it
might be more difficult to detect significant differences and type II errors (i.e., false
negatives) are more likely to occur [<a 
href="#Xjohnston2010strategies">106</a>,&#x00A0;<a 
href="#Xbethel2020conducting">27</a>]. In a between-subjects design, the
experimental conditions should also be distinct from each other for between-group
differences to be perceived, otherwise the researcher runs the risk of detecting no
differences. Furthermore, larger sample size is required to mitigate the impact of
individual differences.
<!--l. 950--><p class="indent" >   <span 
class="ecbx-1000">Mixed-model Factorial Design </span>&#x00A0;<br 
class="newline" />&#x00A0; A mixed-model factorial design utilizes both between-subjects and within-subjects
designs, where the between-subjects aspect is used to investigate multiple
independent variables while other random effects are explored using within-subjects
designs. This study design can simultaneously explore the effect from individual
variables and the interaction effects Lazar et&#x00A0;al.&#x00A0;[<a 
href="#Xlazar2017research">120</a>]. The downsides of
each study design structure discussed above still apply and additionally,
mixed-model designs often require more subjects to properly randomize the
                                                                  

                                                                  
study.
<!--l. 955--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">4.1.2   </span> <a 
 id="x1-150004.1.2"></a>Baseline and Control</h5>
<!--l. 955--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; A control is used to demonstrate how another group or condition performs against
a neutral condition. This can be important for experiments that aim to evaluate the
impact of a new technology from different perspectives such as usability,
functionality, and engagement level. An example includes an immediate session
compared to a delayed session. Interested readers can learn more about control
conditions in experimental methodology books [<a 
href="#Xallen2017sage">7</a>,&#x00A0;<a 
href="#Xprice2015research">153</a>], and here we provide a brief
overview. Commonly used control conditions include waitlist/delayed (e.g.,
participants receiving no robot/therapy/treatment) and active control (e.g.,
participants interact with a monitor instead of a robot). The control scenarios can
take on many forms such as robot compared to human, animal, or conventional
treatments.
<!--l. 960--><p class="indent" >   <span 
class="ecbx-1000">Recommendations for study model</span>
<!--l. 962--><p class="indent" >   Study Design Structure.
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-15002x1"><span 
class="ecbx-1000">Within-group designs </span>are best used when researchers want to evaluate
     a  measure  across  multiple  time  points,  or  test  the  effects  of  different
     conditions.  It  is  best  used  when  a  limited  sample  of  participants  is
     available. Researchers must note the possibility of sequence effects, fatigue
     and practice effects.
     </li>
     <li 
  class="enumerate" id="x1-15004x2"><span 
class="ecbx-1000">Between-group  designs  </span>are  best  used  when  researchers  want  to
     test  the  difference  between  two  conditions,  behaviours  or  tasks,  and
     they  do  not  want  participants  to  be  influenced  by  similar  conditions.
     The  design  can  strongly  identify  effects  across-groups  compared  to
     within-group. Between-subjects design requires a larger sample size to
     produce statistically significant results.
     </li>
     <li 
  class="enumerate" id="x1-15006x3"><span 
class="ecbx-1000">Mixed-Model designs </span>are best used when researchers are seeking to
     assess both between and within-group effects. Mixed-model designs require
     a larger sample size and more research skill to implement correctly, but
     produce rigorous results.</li></ol>
<!--l. 968--><p class="noindent" ><span 
class="ecbx-1000">Control conditions </span>help to provide a comparison for the research condition of interest.
In HRI studies, there are many options to benchmark the robot performance.
A control may not be necessary in an initial study, but later studies that
are looking to substantiate the claims in more detail should consider using
one.
                                                                  

                                                                  
<!--l. 971--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">4.2   </span> <a 
 id="x1-160004.2"></a>HRI Evaluation Methodology: Constructs and Metrics</h4>
<!--l. 972--><p class="noindent" >Choice of constructs (what to measure) and metrics (how to measure it) are
important given that these will determine the research focus and the data to collect.
Constructs and metrics will help assess the validity of the research hypothesis and
the relevance of the variables controlled by the researchers, but also shed insights on
the implications for the other agents involved in the interaction process or the
interaction itself. We provide a summary of the common constructs and their related
metrics in Table <a 
href="#x1-18003r2">2<!--tex4ht:ref: table:metrics --></a>. Researchers should carefully evaluate the quality and soundness of
the chosen measurements as it affects the statistical analysis of the study
[<a 
href="#Xsuter2011introduction">195</a>].
<!--l. 974--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">4.2.1   </span> <a 
 id="x1-170004.2.1"></a>Common Constructs in HRI Studies</h5>
<!--l. 975--><p class="noindent" >HRI studies are often interested in validating theoretical concepts, known as
constructs. Most of these constructs are not directly observable but can be
linked to observable and/or measurable metrics and events [<a 
href="#Xhyland1981nature">101</a>]. Given the
highly diverse range of applications in human-robot interaction, more than
110 different metrics have been proposed or used in the HRI literature [<a 
href="#Xdamacharla2018common">45</a>].
These metrics can be grouped by the agents taking part or being measured
during the interaction process, which includes the robot, the human, and the
team (often referred to as the system) [<a 
href="#Xmurphy2013survey">142</a>]. Given the wide scope of HRI
studies, the metrics listed in this section might not apply to all types of user
studies.
<!--l. 977--><p class="indent" >   <span 
class="ecbx-1000">Robot-Related Constructs </span>&#x00A0;<br 
class="newline" />&#x00A0; Robot-related constructs can help evaluate the attributes in the hypothesis
relating to the level and behaviour of autonomy as defined in Sec&#x00A0;<a 
href="#x1-90003.2">3.2<!--tex4ht:ref: subsec:building_hypotheses --></a>. Neglect
tolerance [<a 
href="#Xolsen2003metrics">146</a>] (i.e., how the robot effectiveness declines when the human is
not attending the robot) and attention demand (percentage of time the
human must control the robot) are often used as overall measures of a robot&#8217;s
<span 
class="ecbx-1000">autonomy</span>. Beer et&#x00A0;al.&#x00A0;[<a 
href="#XBeer_Fisk_Rogers_2014">18</a>] suggested to also include a subjective rating of the
human intervention as a supplementary measurement of a robot&#8217;s level of
autonomy.
<!--l. 980--><p class="indent" >   <span 
class="ecbx-1000">Productivity </span>is another construct that is related to the robot&#8217;s performance at a
given task and/or the robot&#8217;s technical capabilities (e.g., accuracy at object
recognition). Time-based and error metrics such as task completion time and number
of unsuccessful actions are often used to assess the former [<a 
href="#Xolsen2003metrics">146</a>]. Finally, the third
robot-related construct is the <span 
class="ecbx-1000">social attributes</span>, which aim to capture how traits
(e.g., warmth and competence) and characteristics (e.g., anthropomorphic
appearance) associated with robots affect the social perception of people interacting
with them. Since these attributes mostly rely on people&#8217;s judgements, subjective
scales such as the Robotic Social Attributes Scale (RoSAS) [<a 
href="#Xcarpinella2017robotic">33</a>], Robot Incentives or
Robot Self-Efficacy Scale [<a 
href="#Xrobinson2018measures">164</a>,&#x00A0;<a 
href="#Xrobinson2020robot">165</a>], and Godspeed questionnaires [<a 
href="#Xbartneck2009measurement">15</a>] are often
employed as metrics.
                                                                  

                                                                  
<!--l. 982--><p class="indent" >   <span 
class="ecbx-1000">Human-Related Constructs </span>&#x00A0;<br 
class="newline" />&#x00A0; Human-related constructs are related to the attributes in the hypothesis involving
adaptation, learning, and training as well as the experimental task. For instance,
situation awareness and cognitive workload are two intertwined constructs that have
been identified as particularly relevant in both automation and robotics
[<a 
href="#Xsteinfeld_common_2006">192</a>,&#x00A0;<a 
href="#XBeer_Fisk_Rogers_2014">18</a>]. Situation awareness defines the awareness and understanding an
individual has of a situation [<a 
href="#Xsalmon2009measuring">174</a>]. Cognitive workload is a product of the mental
resources demanded by a task and the capacity of the person performing the
task. In addition, several other constructs have been used to investigate
people&#8217;s perception of robots, their responses to different robots under different
contexts and types of interaction [<a 
href="#Xbartneck2020human">16</a>], or even predicting future use of the
robot [<a 
href="#XBeer_Fisk_Rogers_2014">18</a>]. For instance, constructs such as willingness to interact with the
robot ([<a 
href="#Xheerink2009measuring">86</a>,&#x00A0;<a 
href="#Xrobinson2018measures">164</a>]), trust, perceived or physiological safety, engagement, and
affective states have been shown to be potential predictors of robot use [<a 
href="#XBeer_Fisk_Rogers_2014">18</a>].
The measurement or assessment of these constructs is frequently done using
questionnaires (e.g., RoSAS [<a 
href="#Xcarpinella2017robotic">33</a>], Discrete Emotions Questionnaire [<a 
href="#Xharmon2016discrete">81</a>]), physiological
measurements such as changes in heart rate and skin conductivity [<a 
href="#Xanzalone2015evaluating">9</a>] and behavioral
measurements.
<!--l. 985--><p class="indent" >   <span 
class="ecbx-1000">System-Related Constructs </span>&#x00A0;<br 
class="newline" />&#x00A0; System-related constructs are related to the team structure and task nature
attributes of the hypothesis. Most of the constructs associated with the system are
defined in the context of task-oriented applications, which aim to assess how well the
human(s) and robot(s) perform as a team. For instance, while productivity and
efficiency [<a 
href="#Xmurphy2013survey">142</a>] are associated with how well the task is completed and the time and
effort required to complete the task, fluency characterizes the coordination of joint
activities and actions between members of a well-synchronized team [<a 
href="#Xhoffman2019evaluating">90</a>,&#x00A0;<a 
href="#Xhoffman2007effects">91</a>]. Metrics
such the time elapsed between the end of an agent&#8217;s action and the beginning of the
other agent&#8217;s action (i.e., functional delay), number of unplanned human
interventions or interactions, and the time required to interact with the robot (i.e.,
interaction effort) are often used to measure how different experimental
conditions, human factors or new robot skills affect the system&#8217;s performance
[<a 
href="#Xdamacharla2018common">45</a>].
<!--l. 990--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">4.2.2   </span> <a 
 id="x1-180004.2.2"></a>Common Measurements in HRI Studies</h5>
<!--l. 990--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Multiple constructs can be of interest in an HRI study. However, existing studies
often attempt to capture the human-robot interaction experience with multiple
constructs using a single measurement, which is usually insufficient [<a 
href="#Xbethel2020conducting">27</a>]. Choosing the
right measurements often requires specific expertise about the phenomenon under
investigation to bridge the knowledge gap in an interdisciplinary HRI study. For
instance, facial expression alone might not be the best indicator for evaluating
emotions as expressions can be culture dependent or displayed out of context
(e.g., adults learn to smile to hide disappointment or awkwardness) [<a 
href="#Xbarrett2019emotional">13</a>].
                                                                  

                                                                  
This section highlights the five common evaluation methods listed in Bethel
et&#x00A0;al.&#x00A0;[<a 
href="#Xbethel2020conducting">27</a>] and discusses the advantages, disadvantages, and application of
each.
<!--l. 993--><p class="indent" >   <span 
class="ecbx-1000">Task Performance Metrics </span>&#x00A0;<br 
class="newline" />&#x00A0; Task performance metrics are objective measurements of how well the robot, the
participant, and the overall system accomplish the task under investigation.
The performance can be measured with respect to five different aspects
of the robot&#8217;s capability, which are navigation, perception, management,
manipulation, and sociability [<a 
href="#Xsteinfeld_common_2006">192</a>]. Examples of these metrics include task
completion time, error rate, and efficiency. More examples can be found in Table
<a 
href="#x1-18003r2">2<!--tex4ht:ref: table:metrics --></a>.
<!--l. 996--><p class="indent" >   Performance measurements are useful for comparing technologies as it gives a
numerical value indicating how much improvement can be achieved by adopting the
technology (e.g. introducing warehouse robots to improve the sorting efficiency of
packages). However, task performance metrics alone are insufficient to capture the
entire HRI experience and the result might be heavily influenced by the population.
For instance, people in engineering schools with more exposure to a range of
technology might have a shorter task completion time in a human-robot collaboration
task versus people who are less experienced with technology. The effect of
population can be reduced by selecting a large, diverse group of participants and
introducing a control/baseline to measure relative changes instead of the absolute
values.
<!--l. 1000--><p class="indent" >   <span 
class="ecbx-1000">Behavioural Measurements  </span>&#x00A0;<br 
class="newline" />&#x00A0; Behavioural data can be recorded under four different study contexts:
     <ul class="itemize1">
     <li class="itemize">Naturalistic observation involves observing people&#8217;s behaviour in a natural
     environment where it typically happens (quantitative or qualitative),
     </li>
     <li class="itemize">Participant observation where researchers are part of the experiment they
     are conducting (quantitative or qualitative),
     </li>
     <li class="itemize">Structured
     observation, where researchers only focus on gathering quantitative data
     of the behaviours under investigation in a particular experimental setup,
     are typically more efficient than naturalistic/participant observation as
     researchers will already know what to look for during the experiment,
     </li>
     <li class="itemize">and finally, case studies, which are detailed investigation of an individual,
     social unit, or events [<a 
href="#Xprice2015research">153</a>].</li></ul>
<!--l. 1009--><p class="indent" >   A metric in HRI studies might include recording how long participants look at the
robot. Behavioural measurements do not rely on the participant recollection or
self-interpretation of their behavior [<a 
href="#Xbethel2020conducting">27</a>]. However, behavioural measurements are
susceptible to the Hawthorne effect, where participants might behave differently from
                                                                  

                                                                  
how they normally would because they know they are being watched. Analysis of
behavioural data in the form of audio/videotape requires significant time and
expertise. The cost of data annotation is often high and many researchers might not
process the data. With annotation comes the problem of inter-rater and
intra-rater variability. Finally, behavioural measurements alone are insufficient in
explaining the participant behaviour [<a 
href="#Xbruke2016educational">105</a>] and should be used in conjunction
with other measurements, such as interviews, quantitative or qualitative
data.
<!--l. 1013--><p class="indent" >   <span 
class="ecbx-1000">Psychophysiological measures </span>&#x00A0;<br 
class="newline" />&#x00A0; Psychophysiological signals are physiological measurements that are affected by the
participant&#8217;s mental state. This includes heart rate, blood pressure, brain activity,
skin conductance, muscle activity, and more [<a 
href="#Xbethel2007survey">26</a>]. These measurements are good for
quantifying abstract concepts such as physiological arousal using non-invasive and
objective measurements. The time domain aspect of these measurements helps
pinpointing exactly when in the experiment a response was elicited [<a 
href="#Xdautenhahn2011new">48</a>]. In
addition, an advantage of using psychophysiological measures compared to
behaviour measures is that participants cannot easily manipulate an automatic
response.
<!--l. 1016--><p class="indent" >   Psychophysiological measurements are informative when applied properly. Since
the signal gives very specific information about the person&#8217;s state (e.g. heart rate and
respiration rate are indicators for arousal levels), it is important to understand what
the signal is measuring and not to infer meanings that may not be accurate. One
challenge of obtaining the measurement is that the signal is often influenced by
various confounding variables and noise (e.g. health status, room temperature).
Health conditions can prevent changes with respect to the baseline to be observed
due to the "Law of Initial Values". The law states that the measurement can increase
or decrease within a narrower range if the initial measurement is already high or low
[<a 
href="#Xbethel2007survey">26</a>]. Other confounds include orienting response, defensive response, startle response,
and habituation [<a 
href="#Xbethel2007survey">26</a>]. For example, a loud noise made by the robot during the
experiment could make the participant nervous, resulting in a temporary
increase in heart rate that is unrelated to the actual experiment. There may be
scenarios where the measurement does not provide any useful information, for
example, if the task might not provide a significant change from the baseline
or when different tasks might elicit the same psychophysiological readings
[<a 
href="#Xdautenhahn2011new">48</a>].
<!--l. 1020--><p class="indent" >   <span 
class="ecbx-1000">Interviews </span>&#x00A0;<br 
class="newline" />&#x00A0; Interviews can take on the form of informal conversational interviews, interview
guide approaches, standardized open-ended interviews, and closed quantitative
interviews [<a 
href="#Xbruke2016educational">105</a>,&#x00A0;<a 
href="#Xmills2014qualitative">136</a>]. An informal interview can be tailored towards the
individual and the questions would be more relevant, whereas a structured
interview makes the responses more comparable. A guided interview is more
comprehensive compared to an informal interview as all the topics are defined in
advance. Finally, data collected in closed quantitative interviews are the easiest
to analyze and compare Johnson&#x00A0;[<a 
href="#Xbruke2016educational">105</a>]. An interview provides information
that might not be collected in self-assessment and it does not force people
to rate the robot on scales that are not relevant to them (e.g., how much
                                                                  

                                                                  
did you trust this robot?). Qualitative responses can also be used to find
new research questions and correlations between response answers and new
theories.
<!--l. 1023--><p class="indent" >   Interviews provide a wealth of information about the interaction from the
participants&#8217; perspective. However, the quality of the data is influenced by the
participant&#8217;s response style. Volunteer participants might respond differently to
non-volunteers, as they might inherently be more interested in robots/new technology
[<a 
href="#Xpatten2017understanding">148</a>]. Data collected in informal interviews are more difficult to analyze, potentially
requiring training in particular qualitative methodologies to ensure rigor
(i.e., interpretive phenomonological analysis or content analysis) [<a 
href="#Xsmith2012interpretative">189</a>,&#x00A0;<a 
href="#Xelo2008qualitative">57</a>].
Consistency between unstructured interviews is difficult to maintain between
experiments. Qualitative data analysis can be conducted on programs such
as NVIVO, AtlasTI, and other natural language packages available online
[<a 
href="#Xcosta2017research">43</a>].
<!--l. 1027--><p class="indent" >   <span 
class="ecbx-1000">Self-assessment </span>&#x00A0;<br 
class="newline" />&#x00A0; Self-assessments in HRI studies can be both quantitative and qualitative, and often
come in the form of paper/computer based scales, questionnaires, or surveys. A list
of commonly used surveys can be found in Table <a 
href="#x1-18003r2">2<!--tex4ht:ref: table:metrics --></a>. The assessments are
easy to administer, but influenced by many factors and limited by human
performance. A factor that affects the accuracy of the measurement is the timing of
the survey. Since surveys are often administered at the end of the study, it
means the data can only be collected on a reflective level, researchers will
not be able to corroborate information from the participant directly, and
participants will need to remember the feelings [<a 
href="#Xdautenhahn2011new">48</a>,&#x00A0;<a 
href="#Xbethel2007survey">26</a>]. The assessment is also
influenced by societal or cultural norms where participants might answer
questions based on what they think is acceptable by the research team or
society [<a 
href="#Xbethel2007survey">26</a>]. Some self-assessment measures require substantial fees to be
paid for their use, increasing the cost of research. Others have copyright
that must be approved by the owner for its use, which might be difficult to
obtain.
<!--l. 1032--><p class="indent" >   <span 
class="ecbx-1000">Custom Questionnaires </span>&#x00A0;<br 
class="newline" />&#x00A0; Existing surveys might not always be sufficient to capture the variable at hand or
the research direction of interest. Researchers can consider designing their own
questionnaire, but it must be noted that proper validation requires extensive testing,
statistical measurement and sampling. Adopting existing questionnaires is
recommended if they have been validated. Questionnaires should not be altered once
validated, as they can become invalid when the order of the items or wording is
changed [<a 
href="#Xrust2014modern">171</a>].
<!--l. 1035--><p class="indent" >   To create a new questionnaire, the first step is to select the appropriate
measurement scale (i.e., nominal data, ordinal data, interval data, or ratio; see Table
<a 
href="#x1-18001r1">1<!--tex4ht:ref: tab:choiceOfMeasurement --></a> for more information). Once the scale is selected, types of options and the number
of options need to be considered.
   <div class="table">
                                                                  

                                                                  
<!--l. 1037--><p class="indent" >   <hr class="float"><div class="float" 
>
                                                                  

                                                                  
<div class="center" 
>
<!--l. 1038--><p class="noindent" >
<a 
 id="x1-18001r1"></a>
<a 
 id="x1-18002"></a>
<br />                                                                  <div class="caption" 
><span class="id">
Table&#x00A0;1: : </span><span  
class="content">Choice of measurement scales                                </span></div><!--tex4ht:label?: x1-18001r4 -->
<div class="tabular"> <table id="TBL-2" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"></colgroup><colgroup id="TBL-2-2g"><col 
id="TBL-2-2"></colgroup><colgroup id="TBL-2-3g"><col 
id="TBL-2-3"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:normal; text-align:left;" id="TBL-2-1-1"  
class="td11"> <!--l. 1043--><p class="noindent" ><span 
class="ecbx-1000">Type      of</span>
  <span 
class="ecbx-1000">value</span>          </td><td  style="white-space:normal; text-align:left;" id="TBL-2-1-2"  
class="td11"> <!--l. 1043--><p class="noindent" ><span 
class="ecbx-1000">Definition and example</span>                              </td><td  style="white-space:normal; text-align:left;" id="TBL-2-1-3"  
class="td11"> <!--l. 1043--><p class="noindent" ><span 
class="ecbx-1000">Stats.</span>
  <span 
class="ecbx-1000">average</span>       </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:normal; text-align:left;" id="TBL-2-2-1"  
class="td11"> <!--l. 1045--><p class="noindent" >Nominal
  data              </td><td  style="white-space:normal; text-align:left;" id="TBL-2-2-2"  
class="td11"> <!--l. 1045--><p class="noindent" >Categorized data e.g. male participants are assigned
  the value of 1 and females are assigned the value of
  2                                                                     </td><td  style="white-space:normal; text-align:left;" id="TBL-2-2-3"  
class="td11"> <!--l. 1045--><p class="noindent" >Mode            </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-3-"><td  style="white-space:normal; text-align:left;" id="TBL-2-3-1"  
class="td11"> <!--l. 1047--><p class="noindent" >Ordinal data   </td><td  style="white-space:normal; text-align:left;" id="TBL-2-3-2"  
class="td11"> <!--l. 1047--><p class="noindent" >Ranked      data      e.g.      participants      ranking
  their preferences between robots on a discrete scale
  from 1 to 10                                                      </td><td  style="white-space:normal; text-align:left;" id="TBL-2-3-3"  
class="td11"> <!--l. 1047--><p class="noindent" >Median         </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-4-"><td  style="white-space:normal; text-align:left;" id="TBL-2-4-1"  
class="td11"> <!--l. 1049--><p class="noindent" >Interval data   </td><td  style="white-space:normal; text-align:left;" id="TBL-2-4-2"  
class="td11"> <!--l. 1049--><p class="noindent" >Data measured along a scale e.g. participants rating
  their enjoyment level on a continuous scale from 0 to
  100                                                                  </td><td  style="white-space:normal; text-align:left;" id="TBL-2-4-3"  
class="td11"> <!--l. 1049--><p class="noindent" >Mean            </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-5-"><td  style="white-space:normal; text-align:left;" id="TBL-2-5-1"  
class="td11">              </td></tr></table></div></div>
                                                                  

                                                                  
   </div><hr class="endfloat" />
   </div>
<!--l. 1055--><p class="indent" >   As a general guideline for the design of question options as stated in Rust and
Golombok&#x00A0;[<a 
href="#Xrust2014modern">171</a>]:
     <ul class="itemize1">
     <li class="itemize">A personality/mood questionnaire often includes options such as &#8220;not at
     all&#8221;, &#8220;somewhat&#8221; and &#8220;very much&#8221;.
     </li>
     <li class="itemize">An  attitude  questionnaire  might  have  &#8220;strongly  disagree&#8221;,  &#8220;disagree&#8221;,
     &#8220;neutral&#8221;, &#8220;agree&#8221;, and &#8220;strongly agree&#8221; as the options.
     </li>
     <li class="itemize">A clinical symptom evaluation scale might include &#8220;always&#8221;, &#8220;sometimes&#8221;,
     &#8220;occasionally&#8221;, &#8220;hardly ever&#8221;, and &#8220;never&#8221; as the options.</li></ul>
<!--l. 1062--><p class="indent" >   The number of options for each questionnaire item depends on the nature of the
questionnaire. The important factor is to give sufficient options to the participants so
that they can express their opinions. A general rule is to include at least 4
options for rating scales and use consistent number and type of options across
the survey [<a 
href="#Xrust2014modern">171</a>]. When possible, test different survey wording and choice
of the variables using a pilot study to improve the quality of the survey.
Johnson&#x00A0;[<a 
href="#Xbruke2016educational">105</a>] listed a detailed procedure to construct a questionnaire in
educational research and the same principles can be adopted to HRI studies. In
general, researchers should note that customised questions have not yet
been tested outside the experiment, and when feasible, provide evaluation
metrics for validating the survey (see Sec&#x00A0;<a 
href="#x1-190004.2.3">4.2.3<!--tex4ht:ref: sec:reliability --></a> below). For more detailed
information on how to design a questionnaire, researchers should refer to Rust
and Golombok&#x00A0;[<a 
href="#Xrust2014modern">171</a>],&#x00A0;Litwin and Fink&#x00A0;[<a 
href="#Xlitwin2003assess">127</a>]. Examples where customised
questionnaires are designed and validated for HRI experiments can be found in
[<a 
href="#Xhoffman2007effects">91</a>,&#x00A0;<a 
href="#Xdragan2015effects">55</a>].
<a 
 id="x1-18003r2"></a>
   <!--l. 1067--><div class="longtable"> <table id="TBL-3" class="longtable" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-3-1g"><col 
id="TBL-3-1"></colgroup><colgroup id="TBL-3-2g"><col 
id="TBL-3-2"></colgroup><colgroup id="TBL-3-3g"><col 
id="TBL-3-3"></colgroup><colgroup id="TBL-3-4g"><col 
id="TBL-3-4"></colgroup>
<tr  
 style="vertical-align:baseline;" id="TBL-3-1-"><td colspan="4" style="white-space:nowrap; text-align:center;" id="TBL-3-1-1"  
class="td01">                <div class="multicolumn"  style="white-space:nowrap; text-align:center;"> <div class="caption" 
><span class="id">Table&#x00A0;2: </span><span  
class="content">Summary of Constructs and related Metrics</span></div><!--tex4ht:label?: x1-18003r2 -->                </div>
</td></tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-2-"><td  style="white-space:normal; text-align:left;" id="TBL-3-2-1"  
class="td01">
<!--l. 1069--><p class="noindent" ><span 
class="ecbx-1000">Agent</span> </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-2-2"  
class="td11"> <span 
class="ecbx-1000">Construct  </span></td><td  style="white-space:normal; text-align:left;" id="TBL-3-2-3"  
class="td11">
 <!--l. 1069--><p class="noindent" ><span 
class="ecbx-1000">Measurement Type</span>      </td><td  style="white-space:normal; text-align:left;" id="TBL-3-2-4"  
class="td10">
 <!--l. 1069--><p class="noindent" ><span 
class="ecbx-1000">Metrics</span>                                          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-3-"><td  style="white-space:normal; text-align:left;" id="TBL-3-3-1"  
class="td01">
<!--l. 1071--><p class="noindent" >       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-4-"><td  style="white-space:normal; text-align:left;" id="TBL-3-4-1"  
class="td01">       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-4-2"  
class="td11">            </td><td  style="white-space:normal; text-align:left;" id="TBL-3-4-3"  
class="td11">                        </td><td  style="white-space:normal; text-align:left;" id="TBL-3-4-4"  
class="td10"></td></tr>
<tr  
 style="vertical-align:baseline;" id="TBL-3-5-"><td  style="white-space:normal; text-align:left;" id="TBL-3-5-1"  
class="td01">
<!--l. 1072--><p class="noindent" ><div class="multirow"><!-- rows=86422285 -->
<span 
class="ecbx-1000">Robot</span></div> </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-5-2"  
class="td11"> Autonomy [<a 
href="#Xsteinfeld_common_2006">192</a>]              </td><td  style="white-space:normal; text-align:left;" id="TBL-3-5-3"  
class="td11">
 <!--l. 1072--><p class="noindent" >Task-performance metrics  </td><td  style="white-space:normal; text-align:left;" id="TBL-3-5-4"  
class="td10">
 <!--l. 1072--><p class="noindent" >Neglect tolerance and attention demand [<a 
href="#Xolsen2003metrics">146</a>]</td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-6-"><td  style="white-space:normal; text-align:left;" id="TBL-3-6-1"  
class="td01">
<!--l. 1073--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-6-2"  
class="td11"> Social Attributes [<a 
href="#Xcarpinella2017robotic">33</a>]       </td><td  style="white-space:normal; text-align:left;" id="TBL-3-6-3"  
class="td11">
 <!--l. 1073--><p class="noindent" >Self-reporting
  questionnaires                 </td><td  style="white-space:normal; text-align:left;" id="TBL-3-6-4"  
class="td10">
 <!--l. 1073--><p class="noindent" >RoSAS [<a 
href="#Xcarpinella2017robotic">33</a>]                                             </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-7-"><td  style="white-space:normal; text-align:left;" id="TBL-3-7-1"  
class="td01">
<!--l. 1074--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-7-2"  
class="td11"> Productivity [<a 
href="#Xsteinfeld_common_2006">192</a>]           </td><td  style="white-space:normal; text-align:left;" id="TBL-3-7-3"  
class="td11">
 <!--l. 1074--><p class="noindent" >Task-performance metrics  </td><td  style="white-space:normal; text-align:left;" id="TBL-3-7-4"  
class="td10">
 <!--l. 1074--><p class="noindent" >Task completion time, percentage of successful
  actions [<a 
href="#Xsteinfeld_common_2006">192</a>]                                           </td>
                                                                  

                                                                  
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-8-"><td  style="white-space:normal; text-align:left;" id="TBL-3-8-1"  
class="td01">
</td></tr><tr 
class="cline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-9-"><td  style="white-space:normal; text-align:left;" id="TBL-3-9-1"  
class="td01">
<!--l. 1076--><p class="noindent" ><div class="multirow"><!-- rows=86422285 -->
<span 
class="ecbx-1000">Human</span></div> </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-9-2"  
class="td11"> Situation awareness [<a 
href="#Xscholtz2003theory">179</a>]  </td><td  style="white-space:normal; text-align:left;" id="TBL-3-9-3"  
class="td11">
 <!--l. 1076--><p class="noindent" >Task-performance
  metrics,
  self-report questionnaires,
  physiological
  measurements                 </td><td  style="white-space:normal; text-align:left;" id="TBL-3-9-4"  
class="td10">
 <!--l. 1076--><p class="noindent" >Situation             Awareness             Global
  Assessment  Technique,  Situation  Awareness
  Rating  Technique  [<a 
href="#Xendsley1998comparative">58</a>],  gaze  analysis  [<a 
href="#Xdini2017measurement">51</a>],
  secondary task performance [<a 
href="#Xsalmon2009measuring">174</a>]                </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-10-"><td  style="white-space:normal; text-align:left;" id="TBL-3-10-1"  
class="td01">
<!--l. 1077--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-10-2"  
class="td11"> Cognitive workload [<a 
href="#Xsteinfeld_common_2006">192</a>]  </td><td  style="white-space:normal; text-align:left;" id="TBL-3-10-3"  
class="td11">
 <!--l. 1077--><p class="noindent" >Self-reporting
  questionnaires,
  task-performance metrics,
  physiological
  measurements                 </td><td  style="white-space:normal; text-align:left;" id="TBL-3-10-4"  
class="td10">
 <!--l. 1077--><p class="noindent" >NASA-Task                                      Load
  Index (TLX), Social Avoidance and Distress
  Scale (SADS) [<a 
href="#Xmarvel2020towards">130</a>], task errors and reaction
  time  [<a 
href="#Xprewett2010managing">152</a>],  respiratory  rate,  heart  rate  and
  skin conductance and temperature [<a 
href="#Xnovak2011psychophysiological">145</a>]       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-11-"><td  style="white-space:normal; text-align:left;" id="TBL-3-11-1"  
class="td01">
<!--l. 1078--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-11-2"  
class="td11"> Acceptance [<a 
href="#Xheerink2009measuring">86</a>]              </td><td  style="white-space:normal; text-align:left;" id="TBL-3-11-3"  
class="td11">
 <!--l. 1078--><p class="noindent" >Self-reporting
  questionnaires                 </td><td  style="white-space:normal; text-align:left;" id="TBL-3-11-4"  
class="td10">
 <!--l. 1078--><p class="noindent" >Godspeed   questionnaires   [<a 
href="#Xbartneck2009measurement">15</a>],   Technology
  Acceptance Model based questionnaires [<a 
href="#Xheerink2009measuring">86</a>],
  Almere Model [<a 
href="#Xmarvel2020towards">130</a>]                                  </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-12-"><td  style="white-space:normal; text-align:left;" id="TBL-3-12-1"  
class="td01">
<!--l. 1079--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-12-2"  
class="td11"> Trust [<a 
href="#Xsteinfeld_common_2006">192</a>]                     </td><td  style="white-space:normal; text-align:left;" id="TBL-3-12-3"  
class="td11">
 <!--l. 1079--><p class="noindent" >Self-report questionanires,
  task-performance metrics   </td><td  style="white-space:normal; text-align:left;" id="TBL-3-12-4"  
class="td10">
 <!--l. 1079--><p class="noindent" >Human-Robot
  trust  scale  [<a 
href="#Xschaefer2013perception">177</a>],  trust  in  automation  scale
  [<a 
href="#Xjian2000foundations">103</a>],  task  allocation  [<a 
href="#Xroncone2017transparent">167</a>],  Almere  Model,
  BEHAVE-II,  Multi-Dimensional  Measure  of
  Trust, Negative Attitude toward Robots Scale
  (NARS) [<a 
href="#Xmarvel2020towards">130</a>]                                          </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-13-"><td  style="white-space:normal; text-align:left;" id="TBL-3-13-1"  
class="td01">
<!--l. 1080--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-13-2"  
class="td11"> Affective state [<a 
href="#Xkulic2007affective">117</a>]         </td><td  style="white-space:normal; text-align:left;" id="TBL-3-13-3"  
class="td11">
 <!--l. 1080--><p class="noindent" >Self-report questionnaires,
  physiological
  measurements                 </td><td  style="white-space:normal; text-align:left;" id="TBL-3-13-4"  
class="td10">
 <!--l. 1080--><p class="noindent" >Discrete    Emotions    Questionnaire    [<a 
href="#Xharmon2016discrete">81</a>],
  Pleasure-Arousal-Dominance    ratings    [<a 
href="#Xbetella2016affective">23</a>],
  cardiovascular  and  electromyogram  (EMG)
  activities [<a 
href="#Xkulic2007affective">117</a>]                                         </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-14-"><td  style="white-space:normal; text-align:left;" id="TBL-3-14-1"  
class="td01">
<!--l. 1081--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-14-2"  
class="td11"> Stress [<a 
href="#Xlasota_survey_2017">119</a>]                    </td><td  style="white-space:normal; text-align:left;" id="TBL-3-14-3"  
class="td11">
 <!--l. 1081--><p class="noindent" >Self-report questionnaires,
  physiological
  measurements                 </td><td  style="white-space:normal; text-align:left;" id="TBL-3-14-4"  
class="td10">
 <!--l. 1081--><p class="noindent" >Skin potential response, semantic differential
  (SD) questionnaire [<a 
href="#Xarai2010assessment">10</a>], NASA-TLX, NARS,
  SADS [<a 
href="#Xmarvel2020towards">130</a>]                                             </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-15-"><td  style="white-space:normal; text-align:left;" id="TBL-3-15-1"  
class="td01">
<!--l. 1082--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-15-2"  
class="td11"> Perceived safety [<a 
href="#Xlasota_survey_2017">119</a>]       </td><td  style="white-space:normal; text-align:left;" id="TBL-3-15-3"  
class="td11">
 <!--l. 1082--><p class="noindent" >Self-report questionnaires,
  physiological             and
  behavioral measurement    </td><td  style="white-space:normal; text-align:left;" id="TBL-3-15-4"  
class="td10">
 <!--l. 1082--><p class="noindent" >Godspeed (safety questionnaire) [<a 
href="#Xbartneck2009measurement">15</a>], NARS
  [<a 
href="#Xnomura2006measurement">144</a>],    distance    between    human    and
  robot [<a 
href="#Xmumm2011human">141</a>], cardio-vascular and electrodermal
  activity [<a 
href="#Xlasota_survey_2017">119</a>]                                           </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-16-"><td  style="white-space:normal; text-align:left;" id="TBL-3-16-1"  
class="td01">
<!--l. 1083--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-16-2"  
class="td11"> Engagement [<a 
href="#Xsteinfeld_common_2006">192</a>]            </td><td  style="white-space:normal; text-align:left;" id="TBL-3-16-3"  
class="td11">
 <!--l. 1083--><p class="noindent" >Physiological            and
  behavioural
  measurements,
  self-assessment                 </td><td  style="white-space:normal; text-align:left;" id="TBL-3-16-4"  
class="td10">
 <!--l. 1083--><p class="noindent" >Changes     in     heart     rate     and     skin
  conductivity changes, non-verbal gestures [<a 
href="#Xanzalone2015evaluating">9</a>],
  Self Assessment Manikin Instrument [<a 
href="#Xmarvel2020towards">130</a>]     </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-17-"><td  style="white-space:normal; text-align:left;" id="TBL-3-17-1"  
class="td01">
                                                                  

                                                                  
</td></tr><tr 
class="cline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-18-"><td  style="white-space:normal; text-align:left;" id="TBL-3-18-1"  
class="td01">
<!--l. 1085--><p class="noindent" ><div class="multirow"><!-- rows=86422285 -->
<span 
class="ecbx-1000">System</span></div> </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-18-2"  
class="td11"> Productivity [<a 
href="#Xsteinfeld_common_2006">192</a>]           </td><td  style="white-space:normal; text-align:left;" id="TBL-3-18-3"  
class="td11">
 <!--l. 1085--><p class="noindent" >Task-performance
  metrics,          self-report
  questionnaires                 </td><td  style="white-space:normal; text-align:left;" id="TBL-3-18-4"  
class="td10">
 <!--l. 1085--><p class="noindent" >Fan                       out                       and
  interaction effort [<a 
href="#Xolsen2003metrics">146</a>], System Usability Scale
  (SUS),   Multidimensional   Robot   Attitude
  Scale (MRAS) [<a 
href="#Xmarvel2020towards">130</a>], productivity time [<a 
href="#Xdamacharla2018common">45</a>]   </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-19-"><td  style="white-space:normal; text-align:left;" id="TBL-3-19-1"  
class="td01">
<!--l. 1086--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-19-2"  
class="td11"> Fluidity[<a 
href="#Xsteinfeld_common_2006">192</a>]                  </td><td  style="white-space:normal; text-align:left;" id="TBL-3-19-3"  
class="td11">
 <!--l. 1086--><p class="noindent" >Task-performance metrics  </td><td  style="white-space:normal; text-align:left;" id="TBL-3-19-4"  
class="td10">
 <!--l. 1086--><p class="noindent" >Agent    idle    time,    concurrent    activity,
  functional delay [<a 
href="#Xhoffman2019evaluating">90</a>]                                 </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-20-"><td  style="white-space:normal; text-align:left;" id="TBL-3-20-1"  
class="td01">
<!--l. 1087--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-20-2"  
class="td11"> Efficiency[<a 
href="#Xmurphy2013survey">142</a>]                </td><td  style="white-space:normal; text-align:left;" id="TBL-3-20-3"  
class="td11">
 <!--l. 1087--><p class="noindent" >Task-performance
  metrics,      self-reporting
  questionnaires                 </td><td  style="white-space:normal; text-align:left;" id="TBL-3-20-4"  
class="td10">
 <!--l. 1087--><p class="noindent" >Human-robot  action  or  time  ratio,  time  to
  complete the task [<a 
href="#Xsteinfeld_common_2006">192</a>], perceived quality of
  interaction [<a 
href="#Xbaraglia2017efficient">12</a>], MRAS, SUS [<a 
href="#Xmarvel2020towards">130</a>]               </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-21-"><td  style="white-space:normal; text-align:left;" id="TBL-3-21-1"  
class="td01">
<!--l. 1088--><p class="noindent" >       </td><td  style="white-space:nowrap; text-align:left;" id="TBL-3-21-2"  
class="td11"> Reliability[<a 
href="#Xsteinfeld_common_2006">192</a>]               </td><td  style="white-space:normal; text-align:left;" id="TBL-3-21-3"  
class="td11">
 <!--l. 1088--><p class="noindent" >Task-performance metrics  </td><td  style="white-space:normal; text-align:left;" id="TBL-3-21-4"  
class="td10">
 <!--l. 1088--><p class="noindent" >False alarms, number of interventions [<a 
href="#Xmurphy2013survey">142</a>]   </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-3-22-"><td  style="white-space:normal; text-align:left;" id="TBL-3-22-1"  
class="td01">       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-23-"><td  style="white-space:normal; text-align:left;" id="TBL-3-23-1"  
class="td01">       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-24-"><td  style="white-space:normal; text-align:left;" id="TBL-3-24-1"  
class="td01">
<!--l. 1091--><p class="noindent" >       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-25-"><td  style="white-space:normal; text-align:left;" id="TBL-3-25-1"  
class="td01">
<!--l. 1091--><p class="noindent" >       </td>
</tr><tr  
 style="vertical-align:baseline;" id="TBL-3-26-"><td  style="white-space:normal; text-align:left;" id="TBL-3-26-1"  
class="td01">
<!--l. 1091--><p class="noindent" >       </td>
   </tr>
   </table></div>
   <h5 class="subsubsectionHead"><span class="titlemark">4.2.3   </span> <a 
 id="x1-190004.2.3"></a>Other Considerations</h5>
<!--l. 1093--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; It is important to consider the quality of a metric, which is the extent
to which it can accurately (i.e., validity) and consistently (i.e., reliability)
capture the construct of interest. Donmez et&#x00A0;al.&#x00A0;[<a 
href="#Xdonmez2009evaluation">54</a>] identified several other
factors that should be evaluated when selecting the type of measurements
and the specific metrics to be included in a HRI user study. These factors
are:
     <ul class="itemize1">
     <li class="itemize">a)  Experimental  constraints:  such  as  the  temporal  and  monetary
     resources associated to collecting and analyzing a specific metric, and the
     characteristics of the testing environment (e.g. gaze-based metrics make
     sense in controlled, in-the-lab settings but are impossible to obtain in a
     field context)
     </li>
     <li class="itemize">b) Comprehensive understanding: how much each selected metric explains
     the hypothesis of interest and the amount of additional insight that can
     be obtained from the casual relationship between metrics (e.g. a decrease
                                                                  

                                                                  
     in task performance can be explained by an increase in the human-partner
     mental workload)
     </li>
     <li class="itemize">c) Statistical efficiency: the selected metric should provide the researcher
     with  a  good  number  of  measurements  such  that  requirements  on  the
     statistical power needed to detect potential effects are met; and
     </li>
     <li class="itemize">d)  Measurement  efficiency:  unless  otherwise  required,  the  type  of
     measurement used to collect a specific metric should not be intrusive or
     distracting to the participants and to the nature of the study, task and
     interaction.</li></ul>
<!--l. 1102--><p class="indent" >   <span 
class="ecbx-1000">Reliability </span>&#x00A0;<br 
class="newline" />&#x00A0; Reliability is concerned with the degree to which metrics are free from
measurement error. Formally, it is defined as the proportion of variance of a metric
that is attributed to the true score variance (i.e., the metric that would be obtained if
there were no measurement errors) [<a 
href="#Xsuter2011introduction">195</a>]. Since true score variance cannot be
calculated directly [<a 
href="#Xheale2015validity">85</a>], in practice, reliability is often described in terms of
consistency over four potential sources of error (time, forms, items and raters) and
involves some type of correlation computation.
<!--l. 1105--><p class="indent" >   Consistency over time is often referred to as <span 
class="ecti-1000">test-retest reliability </span>and can be
measured as the correlation coefficient between the metric values obtained under
similar circumstances at two different moments in time (stability). Consistency
between forms is known as <span 
class="ecti-1000">parallel form reliability </span>and can be obtained by measuring
the correlation between the metrics collected using a different form of the original
instrument in an immediate retest procedure. Consistency across items, also known
as <span 
class="ecti-1000">internal consistency reliability</span>, is the most commonly reported type of
reliability and corresponds to the correlation between people&#8217;s responses
across the items on a multiple-item instrument. Cronbach&#8217;s alpha is the most
commonly used test to determine the internal consistency of an instrument
[<a 
href="#Xheale2015validity">85</a>].
<!--l. 1107--><p class="indent" >   When raters are used to generate information that serves as the metric of interest,
<span 
class="ecti-1000">inter-rater reliability </span>can be applied to determine the extent to which different
observers are consistent in their judgements. Inter-rater reliability coefficients are
sensitive to consistency in terms of relative agreement, but they do not determine
absolute agreement (e.g., two observers agree on the relative rank ordering of their
scores/metrics but attribute different values) [<a 
href="#Xsuter2011introduction">195</a>]. Kappa is frequently used to test
inter-rater reliability and detailed instructions around its use can be found in [<a 
href="#Xmchugh2012interrater">135</a>].
In addition to the inter-rater reliability, <span 
class="ecti-1000">intra-rater reliability </span>measures the
consistency of one observer&#8217;s ratings over time and is also calculated using a
correlation coefficient. Researchers can capture this measurement by asking
a rater to complete the same rating scale at two different time points. A
correlation coefficient greater than 0.7 is considered to indicate good agreement
[<a 
href="#Xlitwin2003assess">127</a>].
<!--l. 1109--><p class="indent" >   Independent of reliability type, it is important to note that reliability describes
                                                                  

                                                                  
the quality of a set of metrics produced by a testing instrument and not the quality
of the instrument itself [<a 
href="#Xsuter2011introduction">195</a>]. Similarly, reliability scores are dependent on the
participant sample being measured and hence researchers should always include it
when reporting their studies&#8217; results.
<!--l. 1113--><p class="indent" >   <span 
class="ecbx-1000">Validity </span>&#x00A0;<br 
class="newline" />&#x00A0; Validity is considered the most important quality of a measured dependent
variable. It is defined as the degree to which empirical evidence, i.e., the scales,
metrics and instruments employed in a study, actually measure and support the
theoretical properties and constructs they are supposed to measure [<a 
href="#Xgergle2014experimental">73</a>]. The general
recommendation regarding construct validity is to select metrics that have been the
target of extended validation (e.g. NASA TLX questionnaire) [<a 
href="#Xdonmez2009evaluation">54</a>]. In cases where
non-validated metrics are chosen, construct validity and the degree to which valid
inferences can be made based on the proposed metrics are assessed by considering the
following four aspects [<a 
href="#Xprice2015research">153</a>,&#x00A0;<a 
href="#Xralph2018construct">156</a>]:
     <ul class="itemize1">
     <li class="itemize"><span 
class="ecti-1000">Face validity  </span>is the extent to which a metric or measurement method
     appears  to  be  intuitively  reasonable  and  represent  that  which  the
     researcher is attempting to measure. Since face validity is based on people&#8217;s
     intuitions  about  human  behavior,  it  is  considered  to  be  a  very  weak
     evidence of construct validity [<a 
href="#Xgergle2014experimental">73</a>].
     </li>
     <li class="itemize"><span 
class="ecti-1000">Content validity </span>relates to the extent to which a measurement reflects and
     covers all the content that it presumably samples. In other words, does
     the selected measurement encompass all aspects of the construct it was
     designed to measure? [<a 
href="#Xheale2015validity">85</a>]. Content validity is often assessed using expert
     judgements.
     </li>
     <li class="itemize"><span 
class="ecti-1000">Criterion validity </span>relates to the extent to which different measurements
     (also referred to as criteria) reflect the same construct and is measured in
     three ways [<a 
href="#Xheale2015validity">85</a>]: <span 
class="ecti-1000">convergent validity</span>, which shows that the metric of interest
     is highly correlated with other criteria measuring the same variable or
     construct; <span 
class="ecti-1000">concurrent validity</span>, which indicates the extent to which the
     measurement or metric being evaluated is related to other criteria or some
     other construct measured at the same moment in time; and <span 
class="ecti-1000">predictive</span>
     <span 
class="ecti-1000">validity</span>, which assess the degree to which the metric of interest predicts
     things they theoretically ought to predict.
     </li>
     <li class="itemize"><span 
class="ecti-1000">Discriminant validity </span>is evaluated by the degree to which the metric being
     evaluated is not correlated with measures of variables that are conceptually
     distinct.</li></ul>
<!--l. 1125--><p class="indent" >   <span 
class="ecbx-1000">Recommendations for constructs and metrics for HRI evaluation</span>
                                                                  

                                                                  
<!--l. 1127--><p class="indent" >   Table <a 
href="#x1-18003r2">2<!--tex4ht:ref: table:metrics --></a> provides a quick overview of the common constructs and measurements
and can be used to select what is best for each trial.
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-19002x1"><span 
class="ecbx-1000">Task performance metrics </span>should not be used alone to substantiate
     claims about the entire HRI experience.
     </li>
     <li 
  class="enumerate" id="x1-19004x2"><span 
class="ecbx-1000">Behavioural measurements </span>quantify human-related constructs. When
     using  these  measurements,  psychology  effects  should  be  taken  into
     account and sufficient training should be given to the raters. Behavioural
     measurements can be used in conjunction with other metrics.
     </li>
     <li 
  class="enumerate" id="x1-19006x3"><span 
class="ecbx-1000">Psychophysiological measurements </span>give detailed information about
     the  participant&#8217;s  internal  state.  They  require  significant  expertise
     to   ensure   the   correct   measurements   are   taken   and   the   right
     interpretations/conclusions are drawn.
     </li>
     <li 
  class="enumerate" id="x1-19008x4"><span 
class="ecbx-1000">Interviews </span>are useful when researchers are interested in developing a
     deeper understanding of participants&#8217; responses during the experiment.
     Best practice is to use a research team member not invested in the final
     outcome of the study to conduct the interview to avoid unconscious bias
     towards reporting of experiences that are more favourable towards the
     intended outcome. If possible, interviews should be used alongside other
     measurements to better quantify the experience.
     </li>
     <li 
  class="enumerate" id="x1-19010x5"><span 
class="ecbx-1000">Self-assessments data </span>are often collected through the use of surveys
     and  questionnaires.  Similar  to  interviews,  self-assessments  can  also  be
     influenced by various human factors. Whenever possible, it is better to use
     existing questionnaires as they have been validated.
     </li>
     <li 
  class="enumerate" id="x1-19012x6"><span 
class="ecbx-1000">The  design  of  a  questionnaire </span>involves  selecting  what  to  measure
     and how to measure it. Psychometric validation is lengthy, complex and
     time-consuming. Researchers should carefully consider the need to create
     a new scale, and investigate other literature for suitability. If the scale is
     used for the first time, this should be reported as part of the measures
     section. Reports on survey validity and reliability should be included when
     reporting the scale in a research trial.</li></ol>
<!--l. 1137--><p class="indent" >   Other considerations to be reported when describing measurements:
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-19014x1"><span 
class="ecbx-1000">Metric  reliability </span>captures  the  quality  of  the  data  with  respect  to
     measurement errors. Test-retest reliability, internal consistency reliability,
     inter-rater reliability, and intra-rater reliability are the common measures
     that are applicable to HRI studies.
                                                                  

                                                                  
     </li>
     <li 
  class="enumerate" id="x1-19016x2"><span 
class="ecbx-1000">Validity </span>is  the  extent  to  which  the  data-gathering  tool  measures  the
     intended  measurement.  Face  validity,  content  validity,  and  criterion
     validity are three important metrics to consider when exploring metric
     validity in the HRI setting.</li></ol>
<!--l. 1143--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">4.3   </span> <a 
 id="x1-200004.3"></a>Context of HRI study</h4>
<!--l. 1144--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">4.3.1   </span> <a 
 id="x1-210004.3.1"></a>Trial location</h5>
<!--l. 1144--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Researchers will need to select an appropriate trial location. A variety of different
trial locations can be considered when conducting an experiment in human-robot
interaction.
<!--l. 1147--><p class="indent" >   <span 
class="ecbx-1000">Laboratory Testing </span>&#x00A0;<br 
class="newline" />&#x00A0; Laboratory testing involves conducting the trial in a research laboratory
or research team office space. Most studies are conducted in a laboratory
setting due to factors such as ease of testing, convenient access to robotic
equipment, and rigorous control and observation capabilities. Laboratory
testing involves requesting participants to attend a testing session and to
use a set-up that has been arranged for the participant to interact with.
It is generally easier to control for unpredictable variables in laboratory
studies, but observed interactions might be less natural and lack real-world
relevance.
<!--l. 1150--><p class="indent" >   <span 
class="ecbx-1000">Field Testing </span>&#x00A0;<br 
class="newline" />&#x00A0; Field testing involves deploying the robot in a setting outside of a laboratory, such
as a public setting. There is an increased trend to deploy robots into more realistic
settings that better represent their final deployment use case. Field testing sites can
include public spaces such as museums [<a 
href="#Xpitsch2009first">150</a>], shopping malls [<a 
href="#Xkanda2009affective">107</a>], and schools [<a 
href="#Xlemaignan2016learning">123</a>]),
or involve non-public environments such as industrial spaces [<a 
href="#Xpuljiz2018implementation">154</a>]. Field trials can
involve many uncontrolled variables and while experiments conducted in the field
better replicate real engagement patterns, subtle evaluations can be difficult in such a
noisy environment.
<!--l. 1153--><p class="indent" >   <span 
class="ecbx-1000">Online Environment </span>&#x00A0;<br 
class="newline" />&#x00A0; Online testing involves conducting the experiment or interaction through a digital
medium, such as using tele-presence, online or computer-based mediums. There is
growing interest to use robots within virtual environments given the ease of
deployment and capacity to engage large samples [<a 
href="#Xbethel2020conducting">27</a>]. This method is described
further in section <a 
href="#x1-330005.3">5.3<!--tex4ht:ref: sec:recruitment --></a>.
                                                                  

                                                                  
<!--l. 1159--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">4.3.2   </span> <a 
 id="x1-220004.3.2"></a>Session Frequency and Duration</h5>
<!--l. 1159--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; To determine duration and frequency of testing session, it is important to consider
the following parameters:
     <ul class="itemize1">
     <li class="itemize">How many conditions?
     </li>
     <li class="itemize">How many sessions?
     </li>
     <li class="itemize">How long is each session?
     </li>
     <li class="itemize">How long is the duration of the whole experiment?
     </li>
     <li class="itemize">How frequently to run the sessions?
     </li>
     <li class="itemize">What timing is best for the follow-up studies?</li></ul>
<!--l. 1170--><p class="indent" >   All parameters related to duration and frequency can be determined given the
hypothesis and independent variables of the experiment. In addition, researchers
should run a pilot proof of concept trial with a limited number of participants
to best determine the frequency and duration of the session before larger
sample recruitment. An increase in the number of sessions and duration of the
study can help reduce the impact of the novelty effect (i.e., perception of a
new technology spikes on first encounter). Specifically, it is recommended
that a study should persist for at least two consecutive months to minimize
the impact of novelty effect [<a 
href="#Xsung2009robots">194</a>,&#x00A0;<a 
href="#Xde2016long">50</a>]. On the other hand, trials that run
for a long time can see challenges with participant retention (i.e., Fernaeus
et&#x00A0;al.&#x00A0;[<a 
href="#Xfernaeus2010you">66</a>]).
<!--l. 1172--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">4.3.3   </span> <a 
 id="x1-230004.3.3"></a>Type of Robot</h5>
<!--l. 1172--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Researchers should consider the choice of robot for their experiment by taking into
consideration different capabilities and requirements. Examples include selecting a
robot based on its appearance, mobility, processing power, and level of functionality.
There is a limited number of robotic systems that are commercially available and not
all the systems have the features that one might be interested to explore (e.g., facial
features). Selection of robots should take into consideration how the target
sample will identify the appropriate social behaviour from the robot. Robot
appearance can affect how people perceive the robot&#8217;s sense of intelligence,
                                                                  

                                                                  
sociability, likability, credibility, and submissiveness, along with other attributes,
[<a 
href="#Xphillips2018human">149</a>]. A humanoid robot can be more intuitive but physical resemblance to
humans might set a higher expectation in robot capabilities [<a 
href="#Xpatten2017understanding">148</a>]. Zoomorphic
robots that look like animals can be less intuitive, but free from the user&#8217;s
preconceived expectations. Decisions on what robot model to use must be made
based on the intended use case. Patten and Newhart&#x00A0;[<a 
href="#Xpatten2017understanding">148</a>] provides a list of
design principles if researchers desire to build a customized robot for the
study.
<!--l. 1177--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">4.3.4   </span> <a 
 id="x1-240004.3.4"></a>Task Behaviour of the Robot</h5>
<!--l. 1177--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Researchers must decide what will be the task behaviour of the robot in the
experimental design. There are a variety of different options, and the role of the
operator can vary depending on the experimental tasks and the capability of the
robot. Robots can act autonomously, or operators can have full control over the
interaction, but in some cases, they might only intervene when the robot is unable to
make a decision (e.g. [<a 
href="#Xkanda2009affective">107</a>]). The operator in a WoZ study is also known as the wizard
who remotely controls the robot. WoZ is commonly used when when current robotic
systems are insufficient to handle a fully autonomous interaction in a safe or socially
acceptable matter. The wizard can replace natural language processing or other
sensing requirements, generate non-verbal behaviour, navigate, localise, and
perform manipulation or classification tasks as defined by the experiment
[<a 
href="#Xriek2012wizard">162</a>].
<!--l. 1180--><p class="indent" >   Researchers should specify the details of operator training and the duration of the
training during experiment preparation, regardless of the task. In addition, the
capability of the operator/wizard, and interaction scope need to be fully defined and
stay consistent across experiments. For example, if a participant asks a question to
the wizard that is outside of the scope of the interaction, the wizard should answer
by mimicking how a machine would react (e.g., I don&#8217;t know), instead of inserting
additional information. Any operator error should be noted as it can affect
experiment results. More information about this technique can be found in [<a 
href="#Xriek2012wizard">162</a>].
Special considerations should be given when WoZ is used, as it does not reflect how a
robot can behave on its own in that instance. Therefore, the experiment
answers a research question about a proposed robot system that is not yet
available or possible. Other ethical considerations regarding this technique
both from the operator and participant&#8217;s perspectives will be explored in
<a 
href="#x1-270005">5<!--tex4ht:ref: sec:par --></a>.
<!--l. 1185--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">4.3.5   </span> <a 
 id="x1-250004.3.5"></a>Planning for Failure and Contingency</h5>
<!--l. 1185--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Experimental planning must also take into consideration a plan for the possibility
                                                                  

                                                                  
of failure and contingency. Failures can happen at any time during the experiment
and good trial design can help to minimise the impact of these failures. Prior to the
experiment, researchers may face failure in participant recruitment, which is
discussed in Sec&#x00A0;<a 
href="#x1-330005.3">5.3<!--tex4ht:ref: sec:recruitment --></a>. Common failures that occur during the experimental conduct,
such as equipment failure or the researcher failed to follow the experiment protocol,
can be mitigated through the use of pilot studies (see Sec&#x00A0;<a 
href="#x1-430006.1">6.1<!--tex4ht:ref: sec:pilot_studies --></a>) and having
detailed protocols on what to do should a failure or anomaly occur. Finally,
comprehensive data management plans should be in place to avoid any challenges
related to trial evaluation, including backup equipment and data capture
whenever possible to avoid data loss. Steps to mitigate failures in trial evaluation
through data cleaning and analysis recommendations are also discussed in
Sec&#x00A0;<a 
href="#x1-420006">6<!--tex4ht:ref: sec:eva --></a>.
<!--l. 1190--><p class="indent" >   <span 
class="ecbx-1000">Recommendations for HRI study context</span>
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-25002x1">HRI  studies  can  take  place  in  the  laboratory,  in  public,  industrial  or
     home settings, or online. There is always a trade-off between a controlled
     laboratory environment and the real-world and there will be differences
     between online versus real-life interactions. Researchers should select the
     location that is appropriate to the hypothesis under investigation. They
     can also work incrementally, by first conducting studies in the laboratory
     then moving towards a more realistic setting. Trials conducted in one
     setting should not be generalised in the abstract, introduction or discussion
     section across all settings (i.e., laboratory, public, and online).
     </li>
     <li 
  class="enumerate" id="x1-25004x2">When  possible  and  feasible,  it  is  best  to  use  robot  techniques  and
     behaviours in trial that a robot can deliver on its own to avoid deception
     or assessment of effects that are not yet achievable. Wizard of Oz is best
     used if the researcher must evaluate a technique that cannot be delivered
     on its own for the purpose of theoretical exploration, or the researcher is
     using the robot as an intentional avatar for experiments.
     </li>
     <li 
  class="enumerate" id="x1-25006x3">Planning for failure is an important step in experimental design due to
     the  dynamic  nature  of  experiments  involving  human  participants.  We
     have identified specific failure modes that could happen during participant
     recruitment,  experiment,  and  data  analysis.  The  best  approach  is  to
     perform multiple test runs prior to the start of the experiment to minimise
     the risk of potential challenges and failures, by being able to identify them
     before participant recruitment. Researchers may choose to use a small sub
     sample of participants that are discarded from the final analysis (e.g., the
     first 3 test runs with live participants) or request other staff to complete
     the session without collecting their data formally in the experiment.</li></ol>
                                                                  

                                                                  
<!--l. 1198--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">4.4   </span> <a 
 id="x1-260004.4"></a>Examples</h4>
<!--l. 1199--><p class="noindent" >An example of a HRI study that focuses on a robot-related construct (i.e.,
social intelligence) can be found in [<a 
href="#Xshao2020user">185</a>]. The authors aimed to build an
adaptive system allowing the robot to adjust its social behaviour based on the
user&#8217;s affect. To evaluate the participant&#8217;s affect, the authors utilized both
physiological measurements (EEG) and the participant&#8217;s self-assessment
data, demonstrating the use of multiple evaluation metrics to quantify the
experience.
<!--l. 1201--><p class="indent" >   Towards system-related constructs, Hoffman and Breazeal&#x00A0;[<a 
href="#Xhoffman2007effects">91</a>] pioneered the first
fluency evaluation in HRI with a physical robot. The authors hypothesized that the
human-robot collaboration fluency can be improved by replicating the anticipatory
behaviour in human-human collaborations. The authors implemented a cognitive
framework in a non-anthropomophic robot and designed an experiment to
evaluate the framework. As part of the experiment, the authors developed
and evaluated a customised survey using proper techniques as described in
Section&#x00A0;<a 
href="#x1-190004.2.3">4.2.3<!--tex4ht:ref: sec:reliability --></a>.
<!--l. 1203--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">5   </span> <a 
 id="x1-270005"></a>Participants</h3>
<!--l. 1206--><p class="noindent" >HRI research envisions robots in a variety of settings, interacting with users. A major
requirement to validate proposed approaches is the participation of users during
experiments. Therefore, recruiting the appropriate participant population is crucial.
In this section, we first identify the types of participants involved in HRI studies and
summarize the important steps in participant selection, including sampling,
randomization, and blinding. We then discuss the procedure for participant
recruitment, retention, and highlight various ethical considerations pertaining to the
people involved in HRI studies.
<!--l. 1211--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.1   </span> <a 
 id="x1-280005.1"></a>Types of Participants</h4>
<!--l. 1212--><p class="noindent" >Participants of a HRI study can be classified into four categories: observers, WoZ
operators, teleoperators, confederates, and interactors. The role of the observer might
involve viewing videos of HRI interactions and interpreting the content based on
HRI evaluation metrics (refer to Section <a 
href="#x1-180004.2.2">4.2.2<!--tex4ht:ref: sec:eva-behav-measure --></a>). Wizards, or operators, are
participants who operate the robot and may require special training. Confederates
are actors or researchers who pretend to be experiment participants, but
in reality work for the research team (see [<a 
href="#Xallen2017sage">7</a>] for more information). The
use of wizards of confederates sometimes allow researchers to create edge
scenarios and measure effects that would only rarely arise in naturalistic
interaction. Finally, the interactors are the participants who interact with the
robot.
                                                                  

                                                                  
<!--l. 1214--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.2   </span> <a 
 id="x1-290005.2"></a>Participant Selection</h4>
<!--l. 1215--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">5.2.1   </span> <a 
 id="x1-300005.2.1"></a>Sampling</h5>
<!--l. 1215--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Sampling is the process of identifying a representative group of participants
suitable for the research study. An unbiased sample where participants are randomly
drawn from a population is usually difficult to achieve; hence, biased (i.e.,
non-random) sampling techniques are often used, such as convenience sampling and
purposive sampling [<a 
href="#Xetikan2016comparison">60</a>]. Convenience sampling is often used when participant
recruitment is limited by practical constraints, such as participant accessibility,
proximity, and availability [<a 
href="#Xetikan2016comparison">60</a>]. The majority of existing HRI studies use
convenience samples involving university students [<a 
href="#Xbartneck2020human">16</a>,&#x00A0;<a 
href="#Xbaxter2016characterising">17</a>]. The use of convenience
samples generally reduces the external validity of the research findings as the
participant population is not representative of the user population in terms
of social traits, cognitive performance, and attitudes towards technology
[<a 
href="#Xbartneck2020human">16</a>,&#x00A0;<a 
href="#Xbaxter2016characterising">17</a>]. Geographic proximity is often another reason for using a convenience
sample, which comes with the trade-off of limiting the cultural diversity of
the participants. In general, convenient samples introduce sampling bias
(e.g., excluding certain groups) or sampling error (e.g., including a higher
proportion of certain groups) [<a 
href="#Xpatten2017understanding">148</a>]. When using convenience sampling (e.g.,
university students), researchers might consider balancing potential confounding
variables, such as gender or educational background, during participant selection
[<a 
href="#Xbartneck2020human">16</a>].
<!--l. 1218--><p class="indent" >   In purposive sampling, participants are deliberately chosen for certain
characteristics that they possess, such as knowledge or experience that would enable
them to assist with the research. There are different types of purposive sampling
methods including maximum variation sampling, homogeneous sampling, typical case
sampling, extreme/deviant case sampling, total population sampling and expert
sampling. Specific examples can be found in Etikan et&#x00A0;al.&#x00A0;[<a 
href="#Xetikan2016comparison">60</a>]. Sampling
affects the statistical power of the study, which is determined based on the
combination of the number of trials, the number of participants, and the number
of random or uncontrolled factors. A lack of statistical power can lead to
both type I errors (false positives) or type II errors (false negatives) and
cause the wrong conclusions to be made. Sampling more participants from a
diverse population is the standard and most straightforward way to increase
statistical power [<a 
href="#Xdix2020statistics">52</a>]. More discussion on the sample size can be found in Section
<a 
href="#x1-480006.3.3">6.3.3<!--tex4ht:ref: sec:samplesize --></a>.
<!--l. 1222--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">5.2.2   </span> <a 
 id="x1-310005.2.2"></a>Randomization</h5>
<!--l. 1222--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Randomization and stratification can help assign participants to groups, conditions,
and observers while minimising biases. Randomization involves assigning participants
with an equal chance to experimental groups to explore the effects of the condition
and prevents researchers from predicting the results of the conditions. Simple
randomization is a basic form of condition assignment, which can involve methods
such as a coin toss or simple computer-generated function [<a 
href="#XBellerElaineM2002Rict">20</a>]. Simple randomization
can be used for sample sizes above &#x003E;200 participants, given their statistical
likelihood to even out in terms of allocation, but should not be used for &#x003C;100
participants as there might be an unequal number of participants in each group
[<a 
href="#Xrandomisation200">109</a>,&#x00A0;<a 
href="#Xbeller2002randomisation">21</a>,&#x00A0;<a 
href="#Xkang2008issues">108</a>].
<!--l. 1225--><p class="indent" >   Stratified randomization can be used when the characteristics of the participants
need to be equalized across the experimental groups. For instance, if age is a factor
that can affect the HRI experiment, researchers can stratify the participants to
ensure that similar age groups are present in each participant group. However, factors
relevant to the experiment need to be identified and normalised, which might
not always be possible. There are other stratification techniques such as
block randomization [<a 
href="#XDoigGordonS2005Raac">53</a>] or response adaptive randomization [<a 
href="#XHigginsTrevor2003RiCT">87</a>], which is
a technique to adjust the condition assignment favoring conditions with
better performance based on ongoing data collection. While these techniques
are more common in the medicine field and less common in current HRI
studies, they can be useful in longitudinal studies with socially assistive
robots.
<!--l. 1227--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">5.2.3   </span> <a 
 id="x1-320005.2.3"></a>Blinding and Allocation Concealment</h5>
<!--l. 1227--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Blinding and allocation concealment are techniques that are used to further remove
bias effects in experimental condition assignment, and must be decided on in the
study design [<a 
href="#XWoodLesley2008Eeob">224</a>,&#x00A0;<a 
href="#XRuxtonG.2017Acaa">172</a>,&#x00A0;<a 
href="#Xschulz2002allocation">181</a>]. Blinding is the process of preventing the participants,
researchers or both parties from knowing what intervention or experimental
content participants are allocated to. Conventional blinding strategies include
single-blinded or double-blinded studies. Blinding is often not reported Hrbjartsson
et&#x00A0;al.&#x00A0;[<a 
href="#Xhrobjartsson2009reporting">99</a>] and researchers should include this information as part of the study
design.
<!--l. 1230--><p class="indent" >   Allocation concealment is when the researcher does not know the upcoming
experimental condition until the moment of assignment. Blinding reduces the effect of
observer bias (i.e., the observer&#8217;s judgement is influenced by the knowledge of the
group), whereas allocation concealment reduces allocation bias (i.e., how participants
are assigned to each group) Ruxton&#x00A0;[<a 
href="#XRuxtonG.2017Acaa">172</a>]. Both blinding and concealment can be
executed cheaply. For instance, researchers studying the effect of different
robot appearances can ask a researcher unrelated to the project to randomly
assign letters to each robot. The mapping between the robot and the coded
letter is concealed from the researchers who are analysing the data until the
data has been processed. Allocation concealment can be implemented using
                                                                  

                                                                  
paper-based techniques Doig and Simpson&#x00A0;[<a 
href="#XDoigGordonS2005Raac">53</a>] or using online tools such as
SEPTRE.
<!--l. 1234--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.3   </span> <a 
 id="x1-330005.3"></a>Participant Recruitment and Incentivization</h4>
<!--l. 1234--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; During participant recruitment, the required sample size, the best recruitment
venue, and ways of motivating participants to join the study are among the key
considerations. The right sample size will depend on the study design structure and
the desired statistical power as discussed in Section <a 
href="#x1-480006.3.3">6.3.3<!--tex4ht:ref: sec:samplesize --></a>.
<!--l. 1237--><p class="indent" >   Researchers may face challenges in recruiting sufficient participants to take part
in the study. Significant recruitment difficulties may indicate that the study design
needs to be re-evaluated (e.g., due to the choice of task difficulty, lack of marketing
material, accessibility of the trial location). While study design may be an hidden
factor, some participants are inherently more difficult to recruit, e.g., participants for
long-term studies or studies that require special participant groups (e.g., patients,
children). For HRI studies, participants, such as patients, are often recruited through
external collaborators. For instance, in [<a 
href="#Xlemaignan2016learning">123</a>], the authors recruited children with
handwriting difficulties through the collaborating therapist. Once the participants
who are more difficult to recruit have enrolled, researchers might inquire about the
participant&#8217;s future availability and willingness to partake in other relevant studies.
However, researchers might also need to consider when not to reuse participants.
Scenarios where participants should not be reused include when the study
requires first exposure to the system. Participants who have prior experience in
other studies should be excluded to prevent cross-condition contamination
[<a 
href="#Xnoauthor_when_2018">139</a>].
<!--l. 1239--><p class="indent" >   Participants can be recruited through contacts (phone contacts, email, social
media, other web-based techniques), community groups, mailing lists, volunteer
banks, schools, external collaborators [<a 
href="#Xlazar2017research">120</a>] and crowd-sourcing platforms such as
Amazon Mechanical Turk [<a 
href="#Xchandler2014nonnaivete">37</a>,&#x00A0;<a 
href="#Xchandler_non-naive_2015">38</a>]. The techniques and the corresponding
recommendations are summarized in Table&#x00A0;<a 
href="#x1-33001r3">3<!--tex4ht:ref: tab:participantrecruitment --></a>.
   <div class="table">
                                                                  

                                                                  
<!--l. 1241--><p class="indent" >   <hr class="float"><div class="float" 
>
                                                                  

                                                                  
<div class="center" 
>
<!--l. 1242--><p class="noindent" >
<a 
 id="x1-33001r3"></a>
<a 
 id="x1-33002"></a>
<br />                                                                  <div class="caption" 
><span class="id">
Table&#x00A0;3: : </span><span  
class="content">Participant recruitment methods                             </span></div><!--tex4ht:label?: x1-33001r5 -->
                                                                  

                                                                  
<div class="tabular"> <table id="TBL-4" class="tabular" 
cellspacing="0" cellpadding="0"  
><colgroup id="TBL-4-1g"><col 
id="TBL-4-1"></colgroup><colgroup id="TBL-4-2g"><col 
id="TBL-4-2"></colgroup><colgroup id="TBL-4-3g"><col 
id="TBL-4-3"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-1-"><td  style="white-space:normal; text-align:left;" id="TBL-4-1-1"  
class="td11"> <!--l. 1247--><p class="noindent" ><span 
class="ecbx-1000">Method</span>      </td><td  style="white-space:normal; text-align:left;" id="TBL-4-1-2"  
class="td11"> <!--l. 1247--><p class="noindent" ><span 
class="ecbx-1000">Advantages</span>                               </td><td  style="white-space:normal; text-align:left;" id="TBL-4-1-3"  
class="td11"> <!--l. 1247--><p class="noindent" ><span 
class="ecbx-1000">Disadvantages</span>                            </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-2-"><td  style="white-space:normal; text-align:left;" id="TBL-4-2-1"  
class="td11"> <!--l. 1249--><p class="noindent" >Contacts        </td><td  style="white-space:normal; text-align:left;" id="TBL-4-2-2"  
class="td11"> <!--l. 1249--><p class="noindent" >Participants   can   be   recruited   via
  the researcher&#8217;s external collaborators,
  such       as       recruiting       patients
  through  collaborating  therapists  [<a 
href="#Xlemaignan2016learning">123</a>]
  or organizations [<a 
href="#Xtapus2009use">201</a>].
  <!--l. 1251--><p class="noindent" >Researcher contacts are more likely to
  agree to participate.
  <!--l. 1253--><p class="noindent" >                                                     </td><td  style="white-space:normal; text-align:left;" id="TBL-4-2-3"  
class="td11"> <!--l. 1253--><p class="noindent" >This technique might be susceptible to
  snowball
  sampling,  which  is  a  biased  sampling
  technique where one participant refers
  other potential participants [<a 
href="#Xpatten2017understanding">148</a>].
  <!--l. 1255--><p class="noindent" >Participants  who  are  contacts  of  the
  researchers  might  be  less  likely  to
  provide negative feedback.                  </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-3-"><td  style="white-space:normal; text-align:left;" id="TBL-4-3-1"  
class="td11"> <!--l. 1257--><p class="noindent" >Community
  groups,
  professional
  organizations   </td><td  style="white-space:normal; text-align:left;" id="TBL-4-3-2"  
class="td11"> <!--l. 1257--><p class="noindent" >This method makes it easier to contact
  a large group of specialized participants
  at once (good for purposive sampling).  </td><td  style="white-space:normal; text-align:left;" id="TBL-4-3-3"  
class="td11"> <!--l. 1257--><p class="noindent" >Response rates might be low and lead
  to  a  biased  sample.  Researchers  can
  consider running a short presentation at
  the corresponding organization to raise
  awareness and gauge interest.              </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-4-"><td  style="white-space:normal; text-align:left;" id="TBL-4-4-1"  
class="td11"> <!--l. 1259--><p class="noindent" >Mailing list     </td><td  style="white-space:normal; text-align:left;" id="TBL-4-4-2"  
class="td11"> <!--l. 1259--><p class="noindent" >This recruitment method is cheap, fast,
  and easy to execute.                          </td><td  style="white-space:normal; text-align:left;" id="TBL-4-4-3"  
class="td11"> <!--l. 1259--><p class="noindent" >Response rates might be low and lead to
  a biased sample. Cold-emailing might
  solicit negative perception towards the
  study  and  hence  it  is  preferred  to
  recruit through existing mailing lists.    </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-5-"><td  style="white-space:normal; text-align:left;" id="TBL-4-5-1"  
class="td11"> <!--l. 1261--><p class="noindent" >Volunteer
  banks            </td><td  style="white-space:normal; text-align:left;" id="TBL-4-5-2"  
class="td11"> <!--l. 1261--><p class="noindent" >Volunteer banks enable access to a large
  number of participants.                      </td><td  style="white-space:normal; text-align:left;" id="TBL-4-5-3"  
class="td11"> <!--l. 1261--><p class="noindent" >Volunteers
  might be fundamentally different from
  non-volunteers leading to sample bias.
  For instance, volunteers might be more
  excited  about  technology,  have  prior
  experience  with  robots,  or  be  more
  willing  to  adopt  a  new  technology
  compared to non-volunteers [<a 
href="#Xpatten2017understanding">148</a>].         </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-6-"><td  style="white-space:normal; text-align:left;" id="TBL-4-6-1"  
class="td11"> <!--l. 1263--><p class="noindent" >Crowd-sourcing
  platforms       </td><td  style="white-space:normal; text-align:left;" id="TBL-4-6-2"  
class="td11"> <!--l. 1263--><p class="noindent" >This  method  is  also  cheap,  fast,  and
  easy  to  conduct.  Different  filters  on
  the   platforms   or   sites   also   enable
  researchers  to  include  a  more  diverse
  population  or  find  participants  with
  specific  skills  [<a 
href="#Xchandler2014nonnaivete">37</a>].  The  experimental
  tasks typically involve viewing pictures
  or  videos  online  to  replace  in-person
  HRI [<a 
href="#Xdautenhahn2018some">47</a>].                                         </td><td  style="white-space:normal; text-align:left;" id="TBL-4-6-3"  
class="td11"> <!--l. 1263--><p class="noindent" >Crowd-sourcing workers can have prior
  experience  with  similar  studies  (i.e.,
  they   are   non-native),   bring   their
  pre-assumed knowledge into the study,
  which   in   turns   reduces   the   effect
  size of the study [<a 
href="#Xchandler_non-naive_2015">38</a>]. Workers might
  also  discuss  the  experiment  in  the
  worker  discussion  boards  where  they
  might obtain foreknowledge about the
  experiment  and  be  influenced  by  the
  opinions of other people [<a 
href="#Xchandler2014nonnaivete">37</a>]. Another
  HRI  specific  concern  is  that  online
  surveys  and  passive  observation  may
  not reflect participants&#8217; perception and
  interaction with the robot in real life.
  Guidelines on crowd-sourcing platforms
  can be found in [<a 
href="#Xthomas2017validity">203</a>].                         </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-4-7-"><td  style="white-space:normal; text-align:left;" id="TBL-4-7-1"  
class="td11">              </td></tr></table></div></div>
                                                                  

                                                                  
   </div><hr class="endfloat" />
   </div>
<!--l. 1269--><p class="indent" >   Regardless of the recruitment method, researchers should always pay special
attention when recruiting vulnerable people. Vulnerability is not limited to
disabilities but also includes instances where people might be exploited and abused
due to an existing unequal or dependent relationship [<a 
href="#Xsmith2008ethical">190</a>]. For instance, students
participating in their supervisor&#8217;s study or clinicians recruiting patients for trials.
This also includes recruiting close friends or family, who may comply with
the experimental manipulation to please the researcher. A simple way to
minimise the effect of potential unequal relationship is through information
disclosure (i.e., who is involved with the project, how privacy will be protected,
how patients&#8217; right to medical care will be reserved regardless of the study
results).
<!--l. 1271--><p class="indent" >   Motivating participants to enroll in the study might be another challenge.
Different incentives might be provided, such as food for students, gifts for children,
raffles, and participant honoraria. Participant compensation should be proportional
to the amount of time required and the type of participants involved. However, biases
might be introduced when compensation is offered, as financial incentives
might cause data from a subset of participants to be over-represented in the
entire sample [<a 
href="#Xswanson2015research">196</a>], and interfere with the participants&#8217; ability to weigh the
risks and benefits of the study [<a 
href="#Xfry2006ethics">69</a>]. When compensation is involved, the
rule of thumb is that the payment should not be provided as a reward for
the risk or harm [<a 
href="#Xfry2006ethics">69</a>]. Researchers should follow the standard procedure
defined by the host research institute and maintain transparency. Ethics
committees might only allow small amount of incidental expenses to be
reimbursed. In addition, researchers can motivate participants by providing
accommodations depending on the context of the study, such as offering
to conduct the experiment at the participant&#8217;s home to accommodate for
those who might not be able to travel. Finally, incentives must be suitable
and not cause additional risk for participation or unfairly encourage people
to participate who are then not voluntarily providing consent, e.g., they
are financially challenged and participate in the trial only to receive the
incentive.
   <h4 class="subsectionHead"><span class="titlemark">5.4   </span> <a 
 id="x1-340005.4"></a>Participant Retention</h4>
<!--l. 1277--><p class="noindent" >Longitudinal studies are particularly susceptible to the problem of participant
retention. Participants dropping out over time can result in a type of sampling bias
known as mortality. In clinical research, participants can drop out of the
experiment if they do not feel the benefit of the therapy, causing the remaining
sample to be biased towards the groups with more effective treatment [<a 
href="#Xpatten2017understanding">148</a>]. In
the HRI context, mortality is observed when participants stop using the
technology over time, especially in longitudinal, in-home studies [<a 
href="#Xfernaeus2010you">66</a>,&#x00A0;<a 
href="#Xde2016long">50</a>].
The reasons for the participant&#8217;s departure may not be random and it is
important to record mortality in longitudinal studies and its occurrence in each
                                                                  

                                                                  
group.
<!--l. 1281--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.5   </span> <a 
 id="x1-350005.5"></a>Participant Preparation and Training</h4>
<!--l. 1282--><p class="noindent" >Once the appropriate participants are recruited for the study, researchers should
consider the need for participant preparation and training especially in experiments
that involve observers or operators. In the HRI literature, commonly the details of
the training program including its length and approach are not reported [<a 
href="#Xriek2012wizard">162</a>].
Experimenters should consider the following questions when planning for participant
preparation and training. A full description of participant debriefing is available in
[<a 
href="#Xallen2017sage">7</a>].
     <ul class="itemize1">
     <li class="itemize">What  information  does  the  participant  need  prior  to  the  start  of  the
     experiment in order to give informed consent? Is deception involved?
     </li>
     <li class="itemize">What training does the participant need to undergo?
     </li>
     <li class="itemize">How will the observers be trained (e.g. what scale is being used to evaluate
     the HRI session and what are the evaluation standards)?
     </li>
     <li class="itemize">Do the initial results present good inter-rater/intra-rater agreement? For
     instance, some experiments might require observers to undergo training, so
     that they can label affects from people&#8217;s facial expressions. Researchers can
     administer a test prior to the start of the actual experiment to determine
     whether the participants had been trained sufficiently. If the initial results
     show  poor  rater  agreement,  researchers  might  consider  extending  the
     training or removing some observers.</li></ul>
<!--l. 1292--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.6   </span> <a 
 id="x1-360005.6"></a>Privacy and Ethical Considerations</h4>
<!--l. 1293--><p class="noindent" >This section discusses other considerations about participants from the procedural
and ethical aspects of the experiment including privacy protection, informed consent,
and special considerations for different experiment setups.
<!--l. 1295--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">5.6.1   </span> <a 
 id="x1-370005.6.1"></a>Privacy protection</h5>
<!--l. 1295--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; HRI studies follow guidelines similar to other research that deals with human
participants. Maintaining the privacy of the participants is important as it
                                                                  

                                                                  
protects them from potential harms, including embarrassment or fear of
being judged by others. The basic guidelines noted by Lazar et&#x00A0;al.&#x00A0;[<a 
href="#Xlazar2017research">120</a>]
include:
     <ul class="itemize1">
     <li class="itemize">collect the minimum amount of information,
     </li>
     <li class="itemize">limit the number of times data is used,
     </li>
     <li class="itemize">aim for full information disclosure whenever possible,
     </li>
     <li class="itemize">provide a secure data storage medium (especially when cloud-services are
     used),
     </li>
     <li class="itemize">remove personal identifiers whenever possible,
     </li>
     <li class="itemize">follow standard data disposal procedures recommended by the institution,
     </li>
     <li class="itemize">and provide channels to address concerns and enforce accountability.</li></ul>
<!--l. 1309--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">5.6.2   </span> <a 
 id="x1-380005.6.2"></a>Informed consent</h5>
<!--l. 1309--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Obtaining informed consent in HRI studies is similar to any other research
involving human participants. Participants must understand the purpose, procedure
and risks involved with their participation and their rights to data and information.
For experiment setups that require deception (e.g., WoZ studies), informed consent
cannot be obtained and researchers have to purposefully withhold information at the
start of the study. While the use of deception raises ethical concerns, it is sometimes
needed especially when the phenomenon under investigation is prone to reactivity,
which is when the measurement changes the participant&#8217;s behaviour Price
et&#x00A0;al.&#x00A0;[<a 
href="#Xprice2015research">153</a>]. It is important to devise a post-study debriefing procedure
for this type of HRI studies to fulfill the researchers&#8217; duty in information
disclosure.
<!--l. 1314--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">5.6.3   </span> <a 
 id="x1-390005.6.3"></a>Special considerations</h5>
                                                                  

                                                                  
<!--l. 1314--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; In addition to being aware of the participants who might require special care,
accommodation, or the region specific standards when preparing the experiment
protocol, this section is dedicated to highlight potential concerns relating to other
special participant groups and research practices.
<!--l. 1317--><p class="indent" >   As HRI is a relatively new field, many phenomena are not well understood and
researchers should be aware of the potential long term impacts on the participants
across all age groups. For instance, Lemaignan et&#x00A0;al.&#x00A0;[<a 
href="#Xlemaignan2016learning">123</a>] conducted a study
with a social robot in a classroom setting. They pointed out the potential
impact of the robot, changing the dynamics between the teachers and the
students in the classroom. In addition, de&#x00A0;Graaf&#x00A0;[<a 
href="#Xde2016ethical">49</a>] noted that long term
human-robot interactions can potentially foster human-robot relationships, where
humans start forming emotional bonds towards the technology. Considerations
should be given beyond risk assessment from the safety perspective. The
role and utility of the technology in the user&#8217;s life should also be taken into
account, especially in settings such as home, long term care, workplaces, and
schools. Researchers should be aware of the ethical implications involved when
developing robots with human-like qualities or high level of autonomy. These
functions might fundamentally be a form of deception, as the users might
wrongly believe that the robots have human characteristics when they do
not.
<!--l. 1319--><p class="indent" >   Researchers should also be aware of the potential for participants, such as Wizard
of Oz operators, to have a negative experience during the study. For instance, in Rea
et&#x00A0;al.&#x00A0;[<a 
href="#Xrea2017wizard">159</a>], the authors reviewed social HRI experiments that put the WoZ
operators in stressful situations, such as having to cheat [<a 
href="#Xshort2010no">188</a>], enduring
verbal/physical abuse, or simulating other behaviours that would otherwise be
socially awkward (e.g. long periods of silence). Researchers should minimise the
duration of the necessary social conflicts in the interaction script, allow the operators
to have positive interactions with the participants in post experiment debriefing, and
have means for the operators to be trained and express their concerns Rea
et&#x00A0;al.&#x00A0;[<a 
href="#Xrea2017wizard">159</a>].
<!--l. 1321--><p class="indent" >   COVID-19 has highlighted the importance of hygiene in every procedure
including experimental studies to protect both the participant and the researcher.
Experimental procedures should take into account the appropriate institution and
government guideline relating to social distance and minimize contact whenever
possible. As some robots cannot be sanitized using alcohol-based methods,
procedures should be established between the host institution and the lab to ensure
proper hygiene (e.g., have people who need to work with the robots sanitize their
hands before and after working with the robot or using near UV light for equipment
sterilization [<a 
href="#Xkitagawa2020effectiveness">112</a>]).
<!--l. 1325--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.7   </span> <a 
 id="x1-400005.7"></a>Ethics application</h4>
<!--l. 1326--><p class="noindent" >Most academic research projects operate under guidelines established by their
                                                                  

                                                                  
governments or institutions. For instance, the most recent guidelines for EU projects
are defined in the Horizon 2020 framework and the original principals are derived
from the Charter of Fundamental Rights of the European Union, and the European
Convention on Human Rights. Projects in the United States are fundamentally based
on the Belmont report and Hippocratic Oath. In Canada, the Tri-Agency Policies
and Guidelines govern all ethical conduct for human research. In Australia, the
standards are defined under the National Statement on Ethical Conduct
in Human Research. While most research projects in collaboration with
industry often go through the university ethics application system, some may
choose to go with commercial ethics review boards. This practice is not
recommended as these commercial review boards might be subjected to conflict of
interests [<a 
href="#Xlemmens2000ethics">124</a>]. While pilot studies involving only internal researchers may
often begin during development stage, any formal user studies involving
recruited participants should only commence after ethics approval has been
granted.
<!--l. 1331--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">5.8   </span> <a 
 id="x1-410005.8"></a>Examples</h4>
<!--l. 1332--><p class="noindent" >The study conducted by Moyle et&#x00A0;al.&#x00A0;[<a 
href="#Xmoyle2017use">140</a>] demonstrates sample randomization and
stratification techniques. The authors investigated the use of PARO robots for
treating the behavioural and psychological symptoms of dementia. The experiment
took place in 28 different long term care facilities where the facilities were first
stratified based on the organization type (private vs. nonprofit), then the
experimental conditions were randomly assigned in blocks. A cluster randomized
control trial was employed to minimize the effect of between-group contamination, as
patients in different care facilities are inevitably exposed to different activities and
treatment environments.
<!--l. 1334--><p class="indent" >   Lemaignan et&#x00A0;al.&#x00A0;[<a 
href="#Xlemaignan2016learning">123</a>] highlights the procedure for participant recruitment and
study design in a study aiming to integrate robots in childhood education for both
normal children and children with disabilities. This study demonstrated how an
iterative study design can be used to address the challenges of validation with a
vulnerable population group. The system was first validated with a more accessible
population and then with the clinical group. Finally, the authors conducted two
in-depth case studies at the lab with 2 children over the period of a month. Since the
study took place under multiple experimental settings (i.e.: schools, the clinic, and
the lab), the authors were able to identify differences in the children&#8217;s behaviour in
the lab compared to other settings.
<!--l. 1336--><p class="indent" >   The work described in [<a 
href="#Xkanda2009affective">107</a>] showcases participant recruitment in a target field
environment. The authors released a semi-autonomous robot in a shopping mall,
where it interacted with visitors every weekday for four hours over five weeks. During
the first three weeks of the study, 332 participants were recruited by flyers that were
distributed around the mall or approached by the experimenters who were onsite. At
the end of the study, a questionnaire was mailed to each participant and 70% of the
participants responded.
                                                                  

                                                                  
<!--l. 1338--><p class="indent" >   Wang et&#x00A0;al.&#x00A0;[<a 
href="#Xwang2010rome">215</a>] illustrates the importance of considering cultural context
during recruitment. The study explored how the cultural background of the
participants (Chinese vs. US) affects the human-robot collaboration process when
using two different communication styles (implicit vs. explicit). The research team
was able to find representative samples by recruiting participants directly from two
comparable universities in China and the US and conducting the experiments in the
respective countries.
<!--l. 1340--><p class="indent" >   Yanco et&#x00A0;al.&#x00A0;[<a 
href="#XYancoJFR2015">228</a>] provides an example showing recruitment of specialised
participant groups. The study evaluated HRI interfaces in-situ, developed
for expert users for use during the DARPA trials. The authors used the
unique event to collect data with expert users. They observed the teams
participating during the trials and analyzed the HRI methodologies used for robot
teleoperation.
<!--l. 1342--><p class="indent" >   <span 
class="ecbx-1000">Recommendations for participants of HRI studies</span>
<!--l. 1344--><p class="indent" >   <span 
class="ecbx-1000">Participant selection</span>:
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-41002x1">Balancing potential confounding variables can help with noise introduced
     by the participant selection process, especially when non-random sampling
     techniques are used.
     </li>
     <li 
  class="enumerate" id="x1-41004x2">Simple randomization such as a coin toss or random number generator can
     be used when there are a large number of participants, but should not be
     used for a small participant group. Stratified randomization is best used
     for equalising the number of participants across multiple factors, such as
     age. Block randomization and response adaptive are more sophisticated
     randomization techniques.
     </li>
     <li 
  class="enumerate" id="x1-41006x3">Blinding  should  be  reported  in  the  final  report  as  it  represents  an
     important methodological step. Allocation concealment is an important
     step  for  demonstrating  that  the  experiment  is  not  influenced  by  the
     researcher&#8217;s bias.</li></ol>
<!--l. 1350--><p class="noindent" ><span 
class="ecbx-1000">Recruitment and incentivization</span>: Each recruitment platform comes with different
advantages and trade-offs (see Table&#x00A0;<a 
href="#x1-33001r3">3<!--tex4ht:ref: tab:participantrecruitment --></a>). In addition, suitable incentivization can be
considered, but care should be taken that it does not affect the participant&#8217;s
consent.
<!--l. 1352--><p class="indent" >   <span 
class="ecbx-1000">Retention</span>: Participant retention is important especially in longitudinal studies
and participant drop-out should be noted in the final study report.
<!--l. 1354--><p class="indent" >   <span 
class="ecbx-1000">Training</span>: Researchers should ensure that the participants are sufficiently trained.
Participant preparation, such as training for WoZ operators/teleoperators, should be
described in the study.
<!--l. 1356--><p class="indent" >   <span 
class="ecbx-1000">Privacy and ethics</span>:
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-41008x1">Participant privacy should be maintained through data anonymization
     and data management.
                                                                  

                                                                  
     </li>
     <li 
  class="enumerate" id="x1-41010x2">Informed  consent  may  not  be  possible  for  WoZ  studies.  Appropriate
     procedures need to be put in place to debrief the participants after the
     study.
     </li>
     <li 
  class="enumerate" id="x1-41012x3">Given HRI is an emerging field, there may be long term consequences of
     the experience that are unknown. Considerations need to be given to all
     possible participants, from interactors such as children vulnerable groups,
     to the researchers participating in the study.</li></ol>
<!--l. 1363--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">6   </span> <a 
 id="x1-420006"></a>Data Collection and Data Analysis</h3>
<!--l. 1365--><p class="noindent" >Once researchers have decided on the experimental design of their user study, the
next step is to collect and analyze the data to test the research hypothesis. To ensure
that the conclusions drawn from the analysis are valid and generalizable, researchers
must carefully identify any threat to the validity and generalizability of their results
(e.g., extreme metric values or insufficient statistical power), take the appropriate
actions (e.g., reduce error variance), and choose an adequate statistical analysis for
their study design and data. Before diving into the data analysis and statistical
methods that can be used for this purpose, we first cover general recommendations
on how to:
     <ul class="itemize1">
     <li class="itemize">validate the data collection process before the main study is conducted;
     </li>
     <li class="itemize">identify common threats to the quality, validity and reliability of the data;
     and
     </li>
     <li class="itemize">clean, better understand, and post-treat the data prior to any statistical
     analysis.</li></ul>
<!--l. 1373--><p class="indent" >   The recommendations listed in this section are common practices in fields such as
psychology (e.g., [<a 
href="#Xleong2006psychology">125</a>]) and human-computer interaction (e.g., [<a 
href="#Xlazar2017research">120</a>,&#x00A0;<a 
href="#Xhornbaek2013some">97</a>]). The section
next provides an overview of the main statistical methods found in the literature as
well as some recommendations on how to best use these methods. We end this section
with some examples that showcase how statistical analyses are commonly done in
HRI studies.
                                                                  

                                                                  
<!--l. 1378--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.1   </span> <a 
 id="x1-430006.1"></a>Pilot Studies</h4>
<!--l. 1381--><p class="noindent" >Pilot studies are small-scale versions of a full study often conducted with one of two
objectives: evaluate the feasibility of a major study or pre-test a particular research
instrument or procedure. Besides helping researchers identify and correct
potential deficiencies with their research design prior to the implementation of
the full study, pilot studies can also help all the members of the research
team to familiarize themselves with the study protocol as well as assess
the appropriateness of the planned data collection and analysis techniques
[<a 
href="#Xvan2002importance">211</a>].
<!--l. 1383--><p class="indent" >   In the case of HRI studies, pilots are undoubtedly helpful in identifying potential
problems with the primary data collection methodology. For example, Bethel and
Murphy&#x00A0;[<a 
href="#Xbethel2010review">25</a>] were able to identify that participants often misunderstood questions
relating to the dominance dimension when using the Self-Assessment Manikin (SAM)
[<a 
href="#Xbradley1994measuring">30</a>] to report their affective state. Based on this observation, the authors later
decided to exclude participants&#8217; dominance ratings from the analysis. Pilot
studies can also sometimes help identify serious omissions or mistakes in data
collection [<a 
href="#Xhornbaek2013some">97</a>]. This is of particular importance when psychophysiological
measurements are included as part of a study since the quality and reliability of
these measurements are dependent on factors such as noise, lighting, sensor
placement, usage of appropriate amounts of conducting gel or paste, etc. [<a 
href="#Xbethel2007survey">26</a>,&#x00A0;<a 
href="#Xtiberio2013psychophysiological">207</a>].
Similarly, when a new questionnaire is developed or modifications are applied to
existing validated questionnaires, it is often recommended to pilot test all
questionnaires. In this context, pilot studies will help researchers to test the
duration of the survey as well as identify ambiguous or difficult questions
[<a 
href="#Xrueben2020introduction">169</a>].
<!--l. 1386--><p class="indent" >   Pilot studies can also help determine whether the independent variable or
conditions being manipulated by the researchers work and are perceived as intended.
For instance, if the aim of a study is to investigate how robots can convey an
affective state through their movement, researchers should make sure that
participants can clearly recognize the robot&#8217;s gestures and postures [<a 
href="#Xhomburg2018include">95</a>].
Pilot studies can assist researchers in the detection of confounds, that is,
factors that adversely affect the relationship between the independent and
dependent factors. For example, in HRI studies, researchers should consider
confound factors such as novelty, physical embodiment or the environment itself
[<a 
href="#Xbaxter2016characterising">17</a>,&#x00A0;<a 
href="#Xfeil2009human">64</a>].
<!--l. 1388--><p class="indent" >   Pilot studies are smaller versions of the main study. Thus, they should closely
follow the research design and protocol of the main study. For instance, the
participants involved in the pilot study should be representative of the target study
population and their selection should also be based on the same inclusion/exclusion
criteria as the main study [<a 
href="#Xthabane2010tutorial">202</a>]. Similarly, pilot studies should be executed using the
same administration and measurement procedures that would be used to
carry out the real study [<a 
href="#Xsalkind2010encyclopedia">173</a>]. Finally, the success of the main study is not
guaranteed by having a pilot study. It is still possible that problems not
previously encountered during the pilot might arise during the actual study
[<a 
href="#Xvan2002importance">211</a>].
                                                                  

                                                                  
<!--l. 1392--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.2   </span> <a 
 id="x1-440006.2"></a>Common Sources of Error and Bias in User Studies</h4>
<!--l. 1395--><p class="noindent" >As in the case of HCI, HRI studies are challenging because measurements of human
behavior and social interactions can potentially suffer from higher fluctuations [<a 
href="#Xlazar2017research">120</a>].
These fluctuations, often referred to as measurement errors, can arise from the
testing conditions (e.g., robot&#8217;s speech recognition kept failing during the interaction,
which negatively impacted acceptance ratings [<a 
href="#Xwerner2020survey">221</a>]), the type of measurements
and instruments (e.g., questions were ambiguous and difficult to answer),
the participants themselves (e.g., participants with higher technical affinity
often have more realistic expectations about robot capabilities and thus
feel less anxious when interacting with robots [<a 
href="#Xhorstmann2019great">98</a>]), among other factors
[<a 
href="#Xsalkind2010encyclopedia">173</a>].
<!--l. 1397--><p class="indent" >   Measurement errors can be random or systematic. While random errors are due to
chance and cause measurement fluctuations in either direction, systematic errors are
often due to external, predicable factors and result in measurements being biased. For
example, participants might consistently under-perform during all trials of a task
because of tiredness or nervousness. The impact of random errors can be reduced by
increasing the observed sample size or averaging multiple measurements. When
dealing with systematic errors, [<a 
href="#Xlazar2017research">120</a>] recommend that researchers should: <span 
class="ecbx-1000">1.) </span>aim to
eliminate or control all possible sources of bias during the study and data
collection, and <span 
class="ecbx-1000">2.) </span>isolate their impact from the main effect of interest when
analyzing the data. Possible sources of systematic errors or biases in HRI studies
are:
     <ul class="itemize1">
     <li class="itemize">Inappropriate,   inaccurate,   or   incorrectly   configured   measurement
     instruments. For example, physiological measurement instruments such as
     EMG, EGG or IMU sensors require an accurate placement on the body
     [<a 
href="#Xtiberio2013psychophysiological">207</a>].
     </li>
     <li class="itemize">Inappropriate or unclear experimental procedures. For example, when a
     within-subject design is employed, the randomization of all conditions is
     critical. Otherwise, conditions tested later may be consistently better or
     worse than conditions tested earlier due to learning or fatigue effects. In the
     case of WoZ studies, if poorly designed, it is very likely that participants&#8217;
     ratings on the perceived intelligence of the robot are reflections of the
     intelligence of the human operator rather than the robot[<a 
href="#Xweiss2015meta">217</a>].
     </li>
     <li class="itemize">Characteristics of the participants may also introduce systematic errors
     into  the  results  of  a  HRI  study.  As  previously  mentioned  in  Sec.&#x00A0;<a 
href="#x1-270005">5<!--tex4ht:ref: sec:par --></a>,
     since many HRI studies employ university students as their main pool of
                                                                  

                                                                  
     participants, often from a computer science or engineering background, it
     is likely that the measurements will be biased by the high levels of robotics
     and technological familiarity of the students [<a 
href="#Xbartneck2020human">16</a>].
     </li>
     <li class="itemize">Non-intended  robot  errors  due  to  lack  of  technical  robustness  and
     functionality. Multiple studies have shown that participants&#8217; evaluation of
     their interaction experience with a robot can be strongly influenced by
     technical issues. For instance, [<a 
href="#Xhancock2011meta">80</a>] observed that robot reliability plays
     a substantial role in how much trust participants attribute to a robot.
     Similarly,  [<a 
href="#Xwerner2020survey">221</a>]  report  on  a  study  in  which  a  poorly  designed  robot
     frustrated participants and hence biased their acceptance ratings.</li></ul>
<!--l. 1406--><p class="indent" >   In addition to these sources of systematic bias, there are also interaction errors
and failures that are often observed when a human and a robot interact with each
other. From the human perspective, Reason&#x00A0;[<a 
href="#Xreason1990human">160</a>] divides human errors into slips,
lapses, mistakes and violations. Slips represent the situations when the action is not
what was intended (e.g., accidentally pressing the wrong button). Lapses occur as a
result of lapses of user&#8217;s memory and/or attention (e.g., forgetting to turn the robot
off). Mistakes represent the situation when the intention is not appropriate and
consequently the performed action is wrong. In other words, slips and lapses are
execution failures while mistakes represent planning failures. Violations represent
intentional illegitimate actions (e.g., directing the robot to run into a wall). From the
robot perspective, errors can be classified into two classes&#x00A0;[<a 
href="#Xhonig2018understanding">96</a>]: 1) <span 
class="ecti-1000">technical failures</span>
and 2) <span 
class="ecti-1000">interaction failures</span>. Technical failures are caused by problems in
either the robot&#8217;s hardware or software. Interaction failures are caused by the
uncertainties in the robot interaction with the environment, other agents, and
humans&#x00A0;[<a 
href="#Xsteinbauer2012survey">191</a>]. These errors, either from human or robot, can negatively
influence the quality of the collected data. Hence, early detection of the possible
sources of interaction errors that might happen during a HRI study is of high
importance.
<!--l. 1411--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.3   </span> <a 
 id="x1-450006.3"></a>Considerations Prior to Data Analysis</h4>
<!--l. 1412--><p class="noindent" >No matter how well a user study is designed and implemented, researchers frequently
have to deal with errors and their effects on study results. In this subsection, we
describe the strategies for data cleaning and outlier detection, appropriate
visualization techniques to better understand and examine data, and the selection of
appropriate statistical analyses.
<!--l. 1414--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.3.1   </span> <a 
 id="x1-460006.3.1"></a>Data Cleaning</h5>
<!--l. 1414--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Data cleaning can be described as an iterative and repeated three-stage process in
                                                                  

                                                                  
which data errors and abnormalities are screened, diagnosed and edited. In
most studies, data cleaning is mainly done as a post-data collection process.
However, careful data monitoring and visualization during the collection
process can also help to improve the quality of the data prior to the data
cleaning.
<!--l. 1417--><p class="indent" >   Researchers must keep in mind that it is not always immediately clear
whether a suspected data point is erroneous and why it is so. Similarly, missing
values could be due to interruptions during data collection or unavailable
information [<a 
href="#Xvan2005data">210</a>]. Thus, researchers should utilise their knowledge about
potential technical errors (e.g., measurement errors) or previously identified
systematic biases (.e.g., novelty or learning effects) and expected ranges of
normal values to define the rules to follow during data cleaning. For more
information about detecting and handling failures, we refer the reader to
[<a 
href="#Xhonig2018understanding">96</a>,&#x00A0;<a 
href="#Xwogalter2006handbook">223</a>].
<!--l. 1421--><p class="indent" >   <span 
class="ecbx-1000">Screening: </span>A good starting point for detecting invalid data is to do a
visual scan of each participant&#8217;s responses and measurements as they are
collected. Patterned responses in questionnaires, or responses completed in less
time than what was observed across all participants are often employed
as indicators of invalid data [<a 
href="#Xleong2006psychology">125</a>]. If instructional manipulation checks or
attention check questions, that is, questions with obvious answers, were
included among the measurements collected during the study, they can also be
employed to identify potentially erroneous data points. This screening method is
frequently used in studies where participants were recruited through crowd
sourcing services [<a 
href="#Xhauser2016attentive">84</a>]. During screening, researchers should also look for lack or
excess of data (e.g., missing values), outliers, and strange patterns in the data
distributions.
<!--l. 1423--><p class="indent" >   Most of the the methods used to evaluate possible outliers are based on the idea
that a particular proportion of valid data points will exist within a given <span 
class="cmmi-10">k </span>number of
standard deviations from the population mean. By setting a threshold on the number
of standard deviations considered as valid, data points above and below
that threshold are considered to be outliers. Statistics such as a the first
(<span 
class="cmmi-10">Q</span><span 
class="cmr-10">1</span>) and third (<span 
class="cmmi-10">Q</span><span 
class="cmr-10">3</span>) quartiles of the data as well as the interquartile range
(<span 
class="cmmi-10">IQR </span><span 
class="cmr-10">= </span><span 
class="cmmi-10">Q</span><span 
class="cmr-10">3 </span><span 
class="cmsy-10">- </span><span 
class="cmmi-10">Q</span><span 
class="cmr-10">1</span>) are also used to define outlier detection bounds. Data
points outside these bounds are considered to be outliers and are thus eligible
to be excluded from any posterior data analysis. In the case of a sample
size that is relatively small, alternative methods such as the Grubb&#8217;s test
(see [<a 
href="#Xleong2006psychology">125</a>] for more information) are recommended instead. When looking
for outliers, it is important to keep in mind that although all outliers are
characterized as extreme data points, not all extreme data points fall into the outlier
category. Similarly, researchers should be aware of erroneous inliers, that is,
data points generated by error but that fall within the expected range of
values.
<!--l. 1425--><p class="indent" >   <span 
class="ecbx-1000">Diagnosis:  </span>In this phase, researchers clarify the nature of potentially
problematic data points identified during data screening. Data points should be
categorized as: erroneous, true extreme, true normal (i.e., prior expectations about
normal data ranges were incorrect), or idiopathic (i.e., there is no explanation,
                                                                  

                                                                  
but the data point is still suspected of error). For the suspected points for
which diagnosis is less straightforward, i.e., they do not fall into the expected
ranges of true extreme or outlier values, the application of a combination of
diagnostic procedures is recommended [<a 
href="#Xvan2005data">210</a>]. Examples of these procedures are:
determine whether data points were consistently the same throughout the whole
data collection procedure or collect additional information, e.g., question
the experimenter in charge of data collection, and if possible, repeat the
measurement.
<!--l. 1427--><p class="indent" >   <span 
class="ecbx-1000">Editing:  </span>After the identification of errors, missing values and true values, the
next step is to decide what to do with these problematic data points. Overall,
researchers should decide whether problematic data points should be corrected,
excluded or left unchanged. In the case of impossible values, the general
recommendation is to correct them if a correct value can be found, or exclude them
otherwise [<a 
href="#Xleong2006psychology">125</a>]. This editing rule also applies when missing values are due to
hardware or software failures. In the case of missing data, the researcher should
decide on the amount of missing data that is acceptable before leaving a
participant&#8217;s data out of the analysis as well as on what acceptable substitute
values should be used instead (e.g., samplewide median values are often
employed to substitute a missing variable or metric) [<a 
href="#Xleong2006psychology">125</a>]. Additionally,
researchers should declare excluded data and the reasons for excluding this
data.
<!--l. 1429--><p class="indent" >   Whatever the data cleaning strategy researchers employ prior to data analysis,
they should provide detailed documentation of the data-cleaning methods, error
types and rates, error deletion and correction rates in their study report. Similarly,
researchers should report on the differences in the outcomes of their studies with and
without outliers [<a 
href="#Xvan2005data">210</a>].
<!--l. 1431--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.3.2   </span> <a 
 id="x1-470006.3.2"></a>Data Visualization</h5>
<!--l. 1431--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Simple visualizations can be made for both individual participants and the
aggregated responses with the goal of seeing the trend (the direction of data
progression over time), level (changes in relative value of the data over the dependent
variable), and stability/variability of the data between experiment conditions and
participants [<a 
href="#Xlane2014visual">118</a>]. Four common techniques are highlighted in this section:
cumulative records, semi-logarithmic charts, bar graphs and line graphs.
Cumulative records are generated by summing the participants&#8217; responses
across sessions. The method is useful when there is a progression to the
experimental conditions (e.g. in each experiment, the robot learns to do a new skill
in addition to what it can do previously) and a survey is administered at
the end of each condition. This type of graph is a simple way to illustrate
trends (increasing, decreasing, no change) in the total response within a
study.
<!--l. 1435--><p class="indent" >   Semi-logarithmic charts are commonly used to display the rate of change or
                                                                  

                                                                  
proportional changes in performance. These graphs are common in machine
learning to track the accuracy of the algorithm over the training iterations.
Both cumulative records and log charts are for continuous data, whereas bar
graphs are common for discrete data or continuous data for studies with small
sample sizes [<a 
href="#Xlane2014visual">118</a>,&#x00A0;<a 
href="#Xweissgerber2017data">220</a>]. Bar graphs are typically used for presenting summary
statistics between experimental conditions (e.g. user ratings between the control
condition versus the robot condition). However, summary statistics can be
problematic as many distributions can lead to the same graph while other
important features of the dataset are hidden [<a 
href="#Xweissgerber2017data">220</a>,&#x00A0;<a 
href="#Xgeorge2017updating">72</a>]. In the case of a small
sample size, researchers should plot the data distributions or use scatter plots
instead of the traditional bar graphs [<a 
href="#Xweissgerber2015beyond">219</a>]. Weissgeber et al. Weissgerber
et&#x00A0;al.&#x00A0;[<a 
href="#Xweissgerber2017data">220</a>] implemented a free, online tool for data visualization, which can
serve as a starting point for researchers when deciding between different
visualizations.
<!--l. 1437--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.3.3   </span> <a 
 id="x1-480006.3.3"></a>Small Sample Size Studies</h5>
<!--l. 1437--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; A study&#8217;s statistical power is defined as the probability of detecting a significant
effect of the factor being manipulated on the dependent variables being measured if it
exists. Statistical power is directly tied to the number of samples (or measurements)
being analyzed. Thus, the likelihood of detecting small or even moderate effects
vanishes as the sample size decreases [<a 
href="#Xmorgan2017use">138</a>]. Similarly, the likelihood of detecting an
effect when there is none (Type I error) or failing to detect an effect (Type
II error) can be reduced with greater sample size. As a result, statistical
analyses are more reliable when the sample size is large and the subsequent
statistical power is greater [<a 
href="#Xmaxwell2008sample">132</a>]. The general recommendation is to perform a
pre-study power analysis to estimate the appropriate number of participants and
measurements required in order to achieve adequate statistical power or accurate
parameter estimates [<a 
href="#Xpatten2017understanding">148</a>,&#x00A0;<a 
href="#Xbethel2020conducting">27</a>]. Tools like G*Power 3 can be used [<a 
href="#Xfaul2007g">63</a>] for this
purpose.
<!--l. 1443--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.4   </span> <a 
 id="x1-490006.4"></a>Data Analysis</h4>
<!--l. 1444--><p class="noindent" >This section provides an overview of the the most common statistical analyses found
in the literature: hypothesis testing methods, confidence intervals and Bayesian
inference methods.
<!--l. 1446--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.1   </span> <a 
 id="x1-500006.4.1"></a>Hypothesis Testing</h5>
<!--l. 1446--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Hypothesis testing is the most common approach used in HRI studies. We usually
                                                                  

                                                                  
see in the papers or reports that a <span 
class="ecti-1000">null hypothesis </span>is rejected or retained with a
<span 
class="ecti-1000">p-value </span>&#x003C; 5% or <span 
class="ecti-1000">p-value </span>&#x003C; 1%. This hypothesis test is a formal approach for deciding
between two interpretations of a statistical relationship in a sample. One
interpretation, called the <span 
class="ecti-1000">null hypothesis </span>(often symbolized <span 
class="cmmi-10">H</span><sub><span 
class="cmr-7">0</span></sub>), refers to no
relationship in the target population. The other interpretation is called the
<span 
class="ecti-1000">alternative hypothesis </span>(often symbolized <span 
class="cmmi-10">H</span><sub><span 
class="cmr-7">1</span></sub>) which refers to the existence of
relationship in the population which is reflected in the population sample as
well.
<!--l. 1449--><p class="indent" >   Significance level is the threshold at which it is decided whether the null
hypothesis should be rejected or retained. This significance value is also known as
<span 
class="ecti-1000">p-value </span>and it is related to the probability statement made about the observed
sample in the context of a hypothesis, not about the hypotheses being tested [<a 
href="#Xaltman2017points">8</a>]. The
smallest significance level that is normally considered as a reasonable evidence is 5%.
In practice, if the probability of observing the <span 
class="ecti-1000">null hypothesis </span>statement <span 
class="cmmi-10">H</span><sub><span 
class="cmr-7">0</span></sub> is less
than 5%, the <span 
class="ecti-1000">null hypothesis </span>is rejected in favor of the <span 
class="ecti-1000">alternative hypothesis </span><span 
class="cmmi-10">H</span><sub><span 
class="cmr-7">1</span></sub>. The
<span 
class="ecti-1000">p-value </span>represents the probability of observing a similar statistical relationship if the
data being analyzed was generated from random samples&#x00A0;[<a 
href="#Xkaptein2012rethinking">111</a>]. P-values do not
give the probability of a hypothesis being true or false for this particular
experiment, they only provide a description of the long term Type I error rate
for a class of hypothetical experiments. Similarly, p-values do not indicate
whether the means of the samples being analyzed are either equal or not
equal&#x00A0;[<a 
href="#Xaltman2017points">8</a>].
<!--l. 1453--><p class="indent" >   Hypothesis testing methods can be classified into a) parametric and b)
non-parametric tests. These two classes are discussed in detail in the following
sections.
<!--l. 1455--><p class="noindent" ><span 
class="ecbx-1000">Standard Parametric Methods</span>&#x00A0;<br 
class="newline" />&#x00A0; Parametric tests are widely used in analyzing HRI data. These tests include
t-test, analysis of variance (ANOVA), and ordinary least squares regression.
Here, we focus on the ANOVA test as it is the most known and (mis)used
test. The ANOVA test assesses the potential difference between two or more
groups on a <span 
class="ecti-1000">continuous </span>measurement. Before using ANOVA, there are some
assumptions that need to be met in the tested data&#x00A0;[<a 
href="#Xcardinal2013anova">32</a>]. These assumptions are as
follows:
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-50002x1">Normality of data samples
     </li>
     <li 
  class="enumerate" id="x1-50004x2">Homogeneity of variance (sphericity)
     </li>
     <li 
  class="enumerate" id="x1-50006x3">Independence of samples</li></ol>
<!--l. 1463--><p class="indent" >   There are different tests for normality such as Chi-square, Kolmogorov-Smironov,
Shapiro-Wilk, Jarque-Barre, and D&#8217;Agostino-Pearson. Some researchers argue that if
the sample size of all groups are equal (balanced model) and sufficiently large, the
normality assumption can be relaxed provided the samples are symmetrical &#x00A0;[<a 
href="#Xblanca2017non">28</a>,&#x00A0;<a 
href="#Xfeir1974empirical">65</a>].
                                                                  

                                                                  
The homogeneity assumption can be tested by comparing variance values or through
specific statistical tests such as Levene&#8217;s, Fligner Killeen, and Bartlett&#8217;s tests.
ANOVA is not robust against unequal variance&#x00A0;[<a 
href="#Xgrissom2000heterogeneity">77</a>], thus if this assumption is
violated, it may alter both the type I error, i.e., detecting an effect when there is
none&#x00A0;[<a 
href="#Xharwell1992summarizing">83</a>] and the statistical power of the test&#x00A0;[<a 
href="#Xnimon2012statistical">143</a>]. For the independence
assumption, there is no specific test that proves its validity, but it should be
accounted for in the design of the study. For instance, the observation data
may be dependent if repeated measurements are collected from the same
subject.
<!--l. 1465--><p class="indent" >   From the study design perspective (Section&#x00A0;<a 
href="#x1-120004">4<!--tex4ht:ref: sec:hri --></a>), ANOVA can be categorized
based on the study model as well as the number of independent variables
considered in the study&#x00A0;[<a 
href="#Xtabachnick2007using">198</a>]. In the following, we list the different types of
ANOVA:
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-50008x1"><span 
class="ecbx-1000">Between-subjects ANOVA </span>is used when examining the differences
     between two or more independent groups. Within this type of ANOVA
     there are one-way ANOVA and factorial-ANOVA. The main difference
     between these two branches is the number of independent variables to be
     tested.
         <ol  class="enumerate2" >
         <li 
  class="enumerate" id="x1-50010x1"><span 
class="ecbx-1000">One-way ANOVA </span>is used when assessing the difference between
         groups with respect to one independent variable. In practice, one-way
         ANOVA is used when studying the difference between at least three
         groups as in the case of two groups, t-test is usually used.
         </li>
         <li 
  class="enumerate" id="x1-50012x2"><span 
class="ecbx-1000">Factorial-ANOVA </span>is used when examining multiple independent
         variables.</li></ol>
     </li>
     <li 
  class="enumerate" id="x1-50014x2"><span 
class="ecbx-1000">Within-subjects ANOVA</span>, also called repeated measures ANOVA, is used
     when the same subjects were tested for each experiment condition. It is
     frequently used with pre- and post-test experiment design. However, it is not
     limited to only two time periods, it can be also used when examining the
     differences over two or more measurements in time. When using this type
     of ANOVA, it is also important to test the sphericity assumption,
     that is, the variance in the differences between all pairs of groups are
     equal, is valid&#x00A0;[<a 
href="#Xmauchly1940significance">131</a>]. This assumption can be be tested using Mauchly&#8217;s
     test.
     </li>
     <li 
  class="enumerate" id="x1-50016x3"><span 
class="ecbx-1000">Mixed-model ANOVA</span>, also called within-between ANOVA, is used when
     studying the differences by group and time. That is, when the study follows
     both a between-subjects and within-subjects design.
     </li>
                                                                  

                                                                  
     <li 
  class="enumerate" id="x1-50018x4"><span 
class="ecbx-1000">Multivariate analysis of variance (MANOVA) </span>is used when studying the
     differences between groups on multiple dependent variables. We can not replace
     it with simply performing multiple ANOVA&#8217;s for each dependent variable as
     ANOVA would not account for the correlations between the dependent
     variables.</li></ol>
<!--l. 1477--><p class="noindent" ><span 
class="ecbx-1000">Post-hoc Tests and Corrections</span>&#x00A0;<br 
class="newline" />&#x00A0; While the ANOVA test determines whether the differences observed among groups
are due to chance, it does not identify which particular differences between
pairs of groups are significant/not-significant. This is what <span 
class="ecti-1000">post-hoc </span>tests do,
they are used after the ANOVA test to assess the differences between all
possible group pairs (multiple comparisons)&#x00A0;[<a 
href="#Xhsu1996multiple">100</a>]. The more groups in an
ANOVA test, the higher the Type I error rate. Type I error rate (i.e., false
positive) is defined by the significant level in the case of one comparison (two
groups). The error rate inflates with the increase in the number of groups
which in turn increases the number of comparisons&#x00A0;[<a 
href="#Xthompson1994common">205</a>]. A post-hoc test
constrains the experiment-wise error rate to the significant level with an
adjusted p-value. A variety of methods exist in the literature for conducting
post-hoc tests. Here, we focus on the most common tests available in statistical
software.
     <ul class="itemize1">
     <li class="itemize"><span 
class="ecbx-1000">Bonferroni test </span>is simply a series of t-tests performed on all possible
     pairs  of  the  tested  groups.  In  order  to  limit  the  Type  I  error  rate  to
     the  significant  level,  Benferroni  test  sets  the  significance  cutoff  at  the
     significant  level  divided  by  the  number  of  comparisons.  This  is  called
     <span 
class="ecti-1000">Benferroni correction</span>. Thus, Benferroni test tends to be more conservative.
     Tanevska et&#x00A0;al.&#x00A0;[<a 
href="#Xtanevska2020socially">200</a>] utilizes Bonferroni test after the significance was
     found in mixed-model ANOVA analysis. They compared people perception
     of iCub robot when it uses a personalized behaviour versus when it uses
     adaptive one in three different settings.
     </li>
     <li class="itemize"><span 
class="ecbx-1000">Tukey&#8217;s Honest Significant Difference (HSD) </span>is used to measure the
     the pairwise differences in groups. It gives an estimate of the difference
     between the groups and a confidence interval for the estimate. This test
     is used with balanced data, i.e., when the sample size is equal between
     groups. Akalin et&#x00A0;al.&#x00A0;[<a 
href="#Xakalin2019influence">5</a>] employed the Tukey HSD test, after significance
     was found in one-way ANOVA analysis, for comparing different feedback
     styles given by a social robot to older people performing arm exercises.
     </li>
     <li class="itemize"><span 
class="ecbx-1000">Tukey-Kramer HSD </span>is the same as Tukey HSD except it can be used
     with unbalanced groups.
     </li>
     <li class="itemize"><span 
class="ecbx-1000">Fisher&#8217;s  Least  Significant  Difference  (LSD) </span>is  the  most  &#8217;liberal&#8217;
                                                                  

                                                                  
     method as it has a high probability of Type I error rate. This test is
     computationally identical to the Bonferroni test except it does not employ
     any adjustment in Type I error&#x00A0;[<a 
href="#Xsheskin2020handbook">186</a>]. Bethel&#x00A0;[<a 
href="#Xbethel2009robots">24</a>] employed Fisher&#8217;s LSD
     after significance was found in a two-way ANOVA analysis for comparing
     people perception of different robot&#8217;s operating modes.
     </li>
     <li class="itemize"><span 
class="ecbx-1000">Dunnett&#8217;s  test </span>is  used  when  we  care  about  the  difference  between
     the control group and the other groups; not considering all the pairwise
     comparisons. Also, it can be used with unbalanced groups as well as if
     the homogeneity assumption is violated. Szafir&#x00A0;[<a 
href="#Xszafir2019mediating">197</a>] employed Dunnett&#8217;s
     test after significance was found in one-way ANOVA analysis comparing
     human interaction quality with a drone using different interfaces.
     </li></ul>
<!--l. 1501--><p class="indent" >   For more details about the post-hoc test and types, we refer the reader to&#x00A0;[<a 
href="#Xhomack2001understanding">94</a>],
[<a 
href="#Xhilton2006statnote">88</a>], and [<a 
href="#Xcardinal2013anova">32</a>].
<!--l. 1503--><p class="noindent" ><span 
class="ecbx-1000">Non-Parametric Methods </span>&#x00A0;<br 
class="newline" />&#x00A0; When deviation from parametric assumptions is a concern, sample size is small or
there is uncertainty about the type of distribution, non-parametric methods and
modern robust statistical tests offer viable alternatives [<a 
href="#Xlittle2013nonparametric">126</a>].
<!--l. 1506--><p class="indent" >   Non-parametric methods require minimal assumptions about the underlying
distribution generating the observed data. This is achieved by ranking the original
data instead of using the data itself, and the assessment of test statistical significance
through randomization tests. Ranks have the advantage of not being affected by the
presence of outliers or skewed distributions and allow for test statistics distributions
that do not depend on assumptions such as normality. Similarly, randomization
tests, in which all ways to permute the data are considered, allow to assess a
test statistic significance without making explicit assumptions regarding
distributions [<a 
href="#Xfreund2010statistical">68</a>]. Some of the most commonly used non-parametric tests
are:
     <ul class="itemize1">
     <li class="itemize"><span 
class="ecbx-1000">Wilcoxon Mann Whitney Test</span>, also known as the non-parametric
     analog of the <span 
class="cmmi-10">t</span>-test. This method is a rank-based test for comparing two
     populations on a continuous outcome using independent samples, e.g.,
     compare the weight of males to that of females. Abd et&#x00A0;al.&#x00A0;[<a 
href="#Xabd2017trust">1</a>] employed
     the wilcoxon mann whitney test for comparing the users&#8217; trust towards a
     Baxter robot operated in different modes.
     </li>
     <li class="itemize"><span 
class="ecbx-1000">Wilcoxon Signed Rank Test </span>is the non-parametric equivalent of the
     paired samples <span 
class="cmmi-10">t</span>-test. This method examines whether two samples were
     drawn  from  the  same  population  and  can  be  used  when  comparing
     two related samples, matched samples, or repeated measurements on a
     single sample. Mirnig et&#x00A0;al.&#x00A0;[<a 
href="#Xmirnig2017err">137</a>] utilized wilcoxon signed rank test for
                                                                  

                                                                  
     comparing the frequencies of different social cues during human interaction
     with a faulty robot.
     </li>
     <li class="itemize"><span 
class="ecbx-1000">Kruskal-Wallis  Test  </span>is  the  non-parametric  analog  to  a  one-way
     between-subjects ANOVA. The method analyzes the population medians
     of the ranked form data. Harriott et&#x00A0;al.&#x00A0;[<a 
href="#Xharriott2015measuring">82</a>] employed Kruskal-Wallis test
     for comparing the mental workload in human-human interaction versus in
     human-robot interaction scenarios.
     </li>
     <li class="itemize"><span 
class="ecbx-1000">Friedman&#8217;s       Test      </span>is       the       non-parametric       equivalent
     of the repeated-measure ANOVA. This test examines the data based on
     its rank properties. Hypothesis testing for the Friedman&#8217;s test may be
     expressed by the medians or the average ranks. Walters et&#x00A0;al.&#x00A0;[<a 
href="#Xwalters2007robotic">214</a>] used
     the friedman&#8217;s test for comparing the user&#8217;s comfort rating for different
     approaching techniques of a robot in a fetch and carry task.</li></ul>
<!--l. 1518--><p class="indent" >   In the context of HRI studies, [<a 
href="#Xschrum2020four">180</a>] advocate for the use of non-parametric tests
when analyzing categorical data suck as Likert scales. Similarly, recent studies
seem to indicate an increase in the use of non-parametric tests. For instance,
in a user study comparing the difference in the effectiveness of different
learning aids (e.g., robots, tablets and traditional teachers) in the teaching
of the Latin script [<a 
href="#Xsandygulova2020cowriting">175</a>], the authors opted for using non-paramteric tests
after confirming that their collected data violated the normality assumption
required for an ANOVA analysis. In [<a 
href="#Xgaudiello2016trust">71</a>] and [<a 
href="#Xwang2020arespace">216</a>], where all collected data
consisted of Likert-type self-assessment data, the authors decided to use
non-parametric tests such as the Kruskal-Wallis and Wilcoxon Mann Whitney
tests.
<!--l. 1520--><p class="noindent" ><span 
class="ecbx-1000">Robust Statistical Methods </span>&#x00A0;<br 
class="newline" />&#x00A0; Robust methods, also known as modern methods, are defined as statistical methods
particularly designed to provide adequate control of Type I error rates, increase the
likelihood on discovering relevant differences between groups [<a 
href="#Xerceg2008modern">59</a>] and deal with issues
such as non-symmetric distributions, outliers, differences in group variances, among
others [<a 
href="#Xwilcox2018guide">222</a>].
<!--l. 1523--><p class="indent" >   Robust methods replace traditional regression methods (i.e, ordinary least
square), measures of location (i.e., the mean) and measures of association (i.e.,
Pearson correlation coefficients) with robust alternatives such as the sample median,
trimmed means or <span 
class="ecti-1000">Kendall </span>and <span 
class="ecti-1000">Spearman </span>coefficients combined with bootstrap
methods. These alternative measures can be later used to perform hypothesis
testing.
<!--l. 1525--><p class="indent" >   Bootstrapping methods provide precise estimates of population distributions by
iteratively resampling cases from a set of observed data. They can be used to obtain
accurate confidence interval estimates in the presence of outliers or strongly
skewed data [<a 
href="#Xrussell2000log">170</a>]. Rank-based methods such as rank transform ANOVA-type
statistic and Wilcoxon analysis offer extensions to classic non-parametric
methods and are known to produce valid results when analyzing data that is
                                                                  

                                                                  
non-normally distributed and/or with different variance between groups
[<a 
href="#Xerceg2008modern">59</a>].
<!--l. 1527--><p class="indent" >   There are still some limitations associated to use of these robust alternatives. For
instance, there is a reduction in the number of degrees of freedom available for
statistical tests and the estimation of sample size required for sufficient statistical
power is more complex [<a 
href="#Xkitchenham2017robust">113</a>]. We refer the reader to [<a 
href="#Xerceg2008modern">59</a>], [<a 
href="#Xwilcox2018guide">222</a>], and [<a 
href="#Xkitchenham2017robust">113</a>] for a more
detailed introduction to these robust alternative methods.
<!--l. 1529--><p class="indent" >   As an example of the application of robust statistical methods, in their study of
how attributions and perceptions of trust are affected by the presence of intelligent
artificial mediation during human-computer communication, Hohenstein et al.
observed that most of their participants ratings followed a skewed distribution [<a 
href="#Xhohenstein2020ai">93</a>].
Instead of a standard mean as a measure of central tendency, the authors computed
and reported trimmed mean estimates. Bootstrap methods were also used to compute
confidence intervals.
<!--l. 1531--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.2   </span> <a 
 id="x1-510006.4.2"></a>Confidence Interval</h5>
<!--l. 1531--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Confidence interval (CI) use the same underlying mathematical methods as the
hypothesis testing, but instead of giving a probability of a single value, a range of
values is indicated&#x00A0;[<a 
href="#Xdix2020statistics">52</a>]. It is powerful for showcasing the level of uncertainty around
an estimate or prediction. The confidence interval is defined as a range of values,
calculated by statistical methods, that includes the desired true parameter with a
probability defined in advance; called the <span 
class="ecti-1000">confidence level</span>. The size of the confidence
interval depends on the sample size, the standard deviation, and the selected level of
confidence&#x00A0;[<a 
href="#Xgardner1986confidence">70</a>]. For instance, if the sample size is large, the confidence interval
will be narrow. On the other hand, the larger confidence level is, the wider
the confidence interval. The most commonly used value for the confidence
level is 95% - similar to a 5% level of statistical significance in hypothesis
test.
<!--l. 1534--><p class="indent" >   CIs can be one or two-sided. A two-sided CI defines the population parameter
from both lower and upper bounds. A one-sided CI provides either an upper or a
lower limit to the population parameter. Calculation of the CI of a sample parameter
takes the general form of CI = Point estimate <span 
class="cmsy-10"> </span>Margin of error, where the
margin of error is given by the product of the critical value, selected as per
the required confidence limits, and the standard error of point estimate.
In descriptive statistics, the CI is reported with the point estimate of the
concerned parameter, indicating the reliability of the estimate. Hancock
et&#x00A0;al.&#x00A0;[<a 
href="#Xhancock2011meta">80</a>] utilized the CI for analyzing the effects of human, robot, and
environmental factors on the perceived trust in HRI. They used a two-sided CI for
each factor and they found that most of the factors comparison did not
include zero. This suggested that the identified relationship is consistent and
substantive.
                                                                  

                                                                  
<!--l. 1536--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.3   </span> <a 
 id="x1-520006.4.3"></a>Bayesian Inference Methods</h5>
<!--l. 1536--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; In recent years, cautionary recommendations against the use of classical statistical
approaches based on null-hypothesis testing have been frequently issued
[<a 
href="#Xvandekerckhove2018bayesian">212</a>], [<a 
href="#Xkaptein2012rethinking">111</a>], [<a 
href="#Xcumming2014new">44</a>]. In particular, there has been a wide concern regarding the
fact that <span 
class="cmmi-10">p</span><span 
class="cmsy-10">-</span>values alone do not convey information about the size of the
effect being analyzed or the precision with which this effect is estimated.
Alongside these warnings, there have been recurrent calls to shift from classic
frequentist methods such as null-hypothesis testing to Bayesian inference
methods. While hypothesis testing provides a <span 
class="cmmi-10">p</span><span 
class="cmsy-10">-</span>value that indicates the
probability of a given observation (i.e., an estimate or the difference between
groups) is due to chance, Bayesian approaches provide a relative comparison of
how well a null hypothesis and an alternative hypothesis account for the
actual data [<a 
href="#Xkruschke2018bayesian">116</a>]. That is, Bayesian methods allow researchers to make strong
observations about the probability of a given phenomenon based on the collected
evidence.
<!--l. 1539--><p class="indent" >   Bayesian methods offer multiple advantages such as robustness in low-power
situations (e.g., small sample size) and quantification of uncertainty. Moreover, they
allow for the inclusion of relevant context or domain information through
the choice of informative priors, and they offer a powerful framework to
build and test complex models [<a 
href="#Xkruschke2018bayesian">116</a>]. It is important to notice however, that
Bayesian methods require expertise to be used correctly since they are highly
dependent on a good model specification [<a 
href="#Xschad2019toward">176</a>]. A general recommendation is
to consult with an expert especially when working with continuous data
[<a 
href="#Xdix2020statistics">52</a>].
<!--l. 1541--><p class="indent" >   Although still new, we have started seeing the application of Bayesian Methods in
the analysis of HRI studies. For instance, while analysing the relation between
participants&#8217; extroversion and their tendency to view a robot as anthropomorphic,
Kaplan et al.&#x00A0;[<a 
href="#Xkaplan2019relationship">110</a>] performed a Bayesian regression analysis to determine the model
that best predicted the participant&#8217;s ratings of the robots, based on their
personality variables. In [<a 
href="#Xbodala2020modeling">29</a>], one-way Bayesian ANOVA was used to confirm
whether the workload manipulation proposed by the authors had an effect
on the participants&#8217; perceived workload as measured by the NASA-TLX
questionnaire. Hamilton et al. [<a 
href="#Xhamilton2020tradeoffs">79</a>] employed a Bayesian repeated measures
analysis of variance to determine whether different categories of mixed-reality
deictic gestures used to augment the gestural capabilities of non-humanoid
robots had an effect of task-related metrics such as accuracy and reaction
time.
<!--l. 1543--><p class="indent" >   We refer the reader to [<a 
href="#Xkruschke2018bayesian">116</a>], [<a 
href="#Xetz2018become">62</a>], [<a 
href="#Xly2018bayesian">129</a>], and [<a 
href="#Xetz2018introduction">61</a>] for a detailed introduction to
Bayesian statistics. A concrete and detailed example on how to use such methods can
be found in [<a 
href="#Xschad2019toward">176</a>].
<!--l. 1545--><p class="noindent" >
                                                                  

                                                                  
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.4   </span> <a 
 id="x1-530006.4.4"></a>Regression Analysis</h5>
<!--l. 1545--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Sometimes the aim of a user study is not determine the validity of a hypothesis,
but rather to analyze the relationship among a dependent variable of interest (e.g., a
task performance metrics or the final score of a subjective questionnaire) and a
number of independent variables or <span 
class="ecti-1000">predictors </span>(e.g., the factors being manipulated
during a user study). Regression analysis is a statistical method that, under the
assumption of the existence of a linear relationship between the dependent variable
and the predictors, allows to: <span 
class="ecti-1000">i.) </span>quantify this relation, i.e., how much of the variance
observed for the dependent variable is explained by the predictors, and <span 
class="ecti-1000">ii.) </span>make
predictions of the values of the dependent variable based on the observed values of
the predictors [<a 
href="#Xlazar2017research">120</a>].
<!--l. 1548--><p class="indent" >   Linear regression models are constructed differently depending on the purpose of
the statistical analysis [<a 
href="#Xlazar2017research">120</a>]. If the purpose of the linear regression analysis is to find
the relationship between the dependent variable and all predictors, the linear
model is built by simultaneously considering all predictors. However, if the
purpose of the analysis is to build a model that explains the relationship
between the dependent variable and each individual predictor, a hierarchical
procedure in which predictors are added to the model one at the time is more
appropriate.
<!--l. 1550--><p class="indent" >   Linear regression models can be also used to measure the strength of the
relationship between the dependent variable and the predictors, make inferences
about the significance and predictive power of the predictors, and compute prediction
and confidence intervals for a given a set of values of the independent predictors [<a 
href="#Xchatterjee2012handbook">39</a>].
As with other statistical methods, for a linear regression model to be appropriate and
valid, in addition to the linearity assumption previously mentioned, the
following assumptions about the data being modelled must also be respected
[<a 
href="#Xchatterjee2012handbook">39</a>,&#x00A0;<a 
href="#Xramachandran2020mathematical">157</a>]:
     <ul class="itemize1">
     <li class="itemize">The expected difference between the observed and the fitted values of the
     dependent variable must be equal to zero. This difference is also known as
     error.
     </li>
     <li class="itemize">The   variance   of   the   errors   remains   constant   across   all   values
     (homoscedasticity).
     </li>
     <li class="itemize">None  of  the  independent  variables  must  be  a  constant  or  a  linear
     combination of other predictors (colinearity).
     </li>
     <li class="itemize">The errors associated with one observation should not be correlated with
     errors of any other observations (auto-correlation)
     </li>
     <li class="itemize">The errors must follow a normal distribution (normality). This assumption
                                                                  

                                                                  
     is critical when sample size is small.</li></ul>
<!--l. 1559--><p class="indent" >   Violations of the first assumption can serve as indications that the current linear
model has been incorrectly specified and relevant predictors are missing or irrelevant
predictors should be removed. Deviations from the other assumptions (i.e.,
homoscedasticity, colinearity, auto-correlation, and normality) can potentially result
in biased errors and regression coefficients and thus lead to incorrect inferences and
misleading significance tests or confidence intervals [<a 
href="#Xbest2014sage">22</a>].
<!--l. 1561--><p class="indent" >   Although often underutilized in the HRI literature compared to other behavioural
research communities such as human-computer interaction and affective computing,
we have started seeing the application of linear regression models and analysis
in HRI studies. Traeger et al. [<a 
href="#Xtraeger2020vulnerable">209</a>] used a multi-level regression analysis
to determine how a robot&#8217;s social behaviours influences the conversation
dynamics between the human members of a human-robot group. Ceha et al. [<a 
href="#Xceha2019expression">34</a>]
used regression analysis to understand how robot curiosity is perceived by
children, whether a question-asking behavior can lead to curiosity contagion
between a robot and a group of children and how these two factors impact
learning.
<!--l. 1564--><p class="indent" >   We refer the reader to [<a 
href="#Xbest2014sage">22</a>], [<a 
href="#Xchatterjee2012handbook">39</a>], and [<a 
href="#Xramachandran2020mathematical">157</a>], for a detailed introduction to
Regression Analysis.
<!--l. 1566--><p class="noindent" >
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.5   </span> <a 
 id="x1-540006.4.5"></a>Statistical Analysis for Small Sample Sizes</h5>
<!--l. 1566--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; When data from small sample size studies still needs to be analyzed, [<a 
href="#Xmorgan2017use">138</a>] provide
the following recommendations. First, for categorical data, alternative exact
tests such as the Fisher&#8217;s test [<a 
href="#Xmcdonald2009handbook">134</a>] should be preferred since more common
tests (e.g., the <span 
class="cmmi-10">&#x03C7;</span><sup><span 
class="cmr-7">2</span></sup><span 
class="cmsy-10">-</span>test) are known to lack accuracy with a small sample
size. Second, for continuous, interval data, it is often advised to verify key
assumptions such as normality and sphericity before employing classic parametric
methods such as <span 
class="cmmi-10">t</span><span 
class="cmsy-10">-</span>tests or Analysis of Variance (ANOVA). However, in
small sample sized data, the tests employed to verify these assumptions are
likely to be under-powered and may lead to incorrect conclusions about
the validity of these assumptions. In these cases, Welch&#8217;s extensions to the
t-test and ANOVA or non-parametric options such as the Mann-Whitney or
Kruskal-Wallis tests are recommended. It is important to notice however that
non-parametric tests are known to have less statistical power than their
parametric counterparts. Third and last, when visualizing small sample size data,
the use of common visualization methods such as histograms and box plots
can be misleading and hard to interpret. Scatterplots are suggested as the
best choice for showing the distribution and trends of the data in this case
[<a 
href="#Xweissgerber2015beyond">219</a>].
<!--l. 1569--><p class="noindent" >
                                                                  

                                                                  
   <h5 class="subsubsectionHead"><span class="titlemark">6.4.6   </span> <a 
 id="x1-550006.4.6"></a>Statistical Analysis Based on the Type of Measurement Scales</h5>
<!--l. 1569--><p class="noindent" >&#x00A0;<br 
class="newline" />&#x00A0; Putting interviews aside, most of the measurements obtained in HRI studies can be
categorized into continuous, interval values (e.g., behavioral measurements such as
the distance between a human and a robot or physiological data such as heart rate)
or ordinal data (e.g., Likert items and scales often used on self-reporting
questionnaires). While the choice of statistical methods to analyze the former is a
more or less straightforward process (the literature suggests that classic parametric
tests (e.g., ANOVA) are preferred if assumptions of normality and sphericity are
valid, otherwise non-parametric tests (e.g., Kruskal-Wallis) are used instead
[<a 
href="#Xvenero2020experimental">213</a>]), there is an ongoing debate on what is the correct way of analyzing the
latter.
<!--l. 1572--><p class="indent" >   On the one hand, some researchers argue that ordinal data obtained using Likert
scales can be treated as interval data and thus standard parametric statistical
methods can be used. On the other hand, more conservative researchers
choose to employ non-parametric tests instead even though it is known that
these methods are less powerful and lack sensitivity to detect smaller effects
[<a 
href="#Xsullivan2013analyzing">193</a>].
<!--l. 1574--><p class="indent" >   Recently, Schrum et al. [<a 
href="#Xschrum2020four">180</a>] provided a set of recommendations on the use and
analysis of Likert scales in HRI studies. These recommendations are: i) employ
summary statistics such as mode, median, range and skewness when reporting
individual Likert items; ii) although there is compelling evidence suggesting that
parametric tests such as ANOVA can be used on Likert scale data [<a 
href="#Xsullivan2013analyzing">193</a>] as long as the
appropriate assumptions have been tested and validated, a conservative
approach in which non-parametric tests are used instead, is recommended; iii)
analysis should be performed on a multi-item scale instead of on single Likert
items.
<!--l. 1598--><p class="noindent" >
   <h4 class="subsectionHead"><span class="titlemark">6.5   </span> <a 
 id="x1-560006.5"></a>Examples</h4>
<!--l. 1599--><p class="noindent" >Sena et al. [<a 
href="#Xsena2020quantifying">184</a>] propose a visual feedback framework in robot learning from
demonstration that allows the novice user to observe robot learning progress and
provide better distribution for the demonstrations over the task space. They compare
between three conditions for the visual feedback methodology using a within-subjects
study. They conduct power analysis prior to the data collections that indicated that
for a medium effect size (Cohen&#8217;s <span 
class="cmmi-10">f </span><span 
class="cmr-10">= 0</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">3</span>) and a type I error rate = 0.05, the
required sample size is 30. Then, prior to performing a repeated measures ANOVA
analysis, they checked and verified that the collected data are normally distributed
dependent variable, continuous dependent variable, absence of outliers and sphericity
of the data (Homogeneity of variance). They also have another study that compare
between four conditions and they used mixed-factor ANOVA analysis. The power
analysis indicated the required sample size is 36 or 9 per condition. However, when
they check the ANOVA assumptions, they found that three out of four conditions
data are not normally distributed using Anderson-Darling test. The excess
                                                                  

                                                                  
Kurtosis for the four groups was 1.37, 1.60, -0.64 and 0.98, where a normal
distribution would have an excess kurtosis value of zero. They chose to go
with the argument about the robustness of the ANOVA for the violation
of the normality assumption especially with a <span 
class="ecti-1000">minor </span>violation. They also
tested for the homogeneity of the variance using Mauchly&#8217;s test and they got
<span 
class="cmmi-10">&#x03C7;</span><sup><span 
class="cmr-7">2</span></sup><span 
class="cmr-10">(2) = 1</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">24</span><span 
class="cmmi-10">,p </span><span 
class="cmr-10">= 0</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">54 </span>which indicate the validity of this assumption and no need for
data correction.
<!--l. 1601--><p class="indent" >   On the other hand, Correia et al.&#x00A0;[<a 
href="#Xcorreia2018exploring">42</a>] used non-parametric methods for analyzing
their data after they found that both the normality and the homogeneity
assumptions are violated. They used the Shapiro-Wilk test for checking the
normality assumption and the Levene&#8217;s test for checking the homogeneity
assumption. They compare between two condition using the Mann-Whitney-U
test. This test is similar to t-test with which a comparison between two
groups can be conducted. For pairwise comparison for three conditions and
more, the Kruskal-Wallis-H test can be used which is analogous to ANOVA
test.
<!--l. 1603--><p class="indent" >   Kaplan et al.&#x00A0;[<a 
href="#Xkaplan2019relationship">110</a>] used both correlation coefficient and Bayesian regression
analysis for comparison between eight groups. They first conduct a pairwise
correlation analysis to find how much each pair of the eight groups are correlated.
Then, a Bayesian regression analysis was conducted to determine the model
which best predicts the data. They used the Jeffrey&#8217;s Awesome Statistical
Program (JASP) for Bayesian analysis. The regression model only includes the
significant predictors of the dependent variable. Any predictor variable that was
not significant at the <span 
class="cmmi-10">p &#x003C; </span><span 
class="cmr-10">0</span><span 
class="cmmi-10">.</span><span 
class="cmr-10">05 </span>level was excluded from the final regression
model.
<!--l. 1641--><p class="indent" >   <span 
class="ecbx-1000">Recommendations for data collection and analysis of HRI studies</span>
     <ol  class="enumerate1" >
     <li 
  class="enumerate" id="x1-56002x1"><span 
class="ecbx-1000">Piloting</span>: Researchers should aim to conduct a a pilot student that covers
     the proposed scope of the experiment prior to the main study. This will
     help them identify critical problems and deficiencies in the study protocol
     as well as plan and take all the corrective actions.
     </li>
     <li 
  class="enumerate" id="x1-56004x2"><span 
class="ecbx-1000">Biases</span>: Although all sources of systematic errors cannot be eliminated,
     it is import to properly identify and control them because they represent
     potential threats to the validity and reliability of the data collected during
     a user study. Knowledge about potential sources of bias can be also very
     helpful during data cleaning and analysis. Pilot studies are a good manner
     in which potential biases can be identified.
     </li>
     <li 
  class="enumerate" id="x1-56006x3"><span 
class="ecbx-1000">Quality  of  data</span>:  Data  cleaning  and  post-processing  is  an  important
     step prior to data analysis, since the validity and generalizability of the
     observations and conclusions made in a study strongly depend on the
     quality of the data being analyzed. Researchers should formulate a set
     of predefined rules for dealing with data errors, and missing or extreme
                                                                  

                                                                  
     values beforehand. It is also important to provide detailed documentation
     of all the data-cleaning methods employed, the types and sources of error
     identified in the data as well as the excluded data and the reasons for
     exclusion.
     </li>
     <li 
  class="enumerate" id="x1-56008x4"><span 
class="ecbx-1000">Statistical analysis</span>:
         <ul class="itemize1">
         <li class="itemize">Look  at  the  data  before  jumping  to  the  statistical  testing  stage.
         Visualization techniques such as scatter plots allow us to get an initial
         idea about spread of the data, anomalies or extreme values, and the
         match between the data and the model (i.e., is the data linear?)
         </li>
         <li class="itemize">Researchers  should  not  blindly  apply  statistical  hypotheses  tests,
         such  as  ANOVA,  seeking  for  statistical  significance.  If  a  study  is
         well-designed, even if the final p-value is not significant, it conveys
         meaningful results that suggest there is no strong evidence of the
         casual relation being investigated. For a well-designed experiment,
         researchers should pay more attention to statistical power analysis
         that includes the relationship between the sample size and effect size.
         </li>
         <li class="itemize">Proper experiments should ensure that the power will be reasonably
         high to detect the significance with sample size calculation. Also, it
         is important to report p-values as well as confidence intervals and
         effect sizes when possible.
         </li>
         <li class="itemize">Despite the potential misuse, there is a better general understanding
         of classic parametric methods. Researchers are more likely to use
         them correctly and readers are more likely to understand the results
         better.  It  is  important  to  report  p-values  as  well  as  confidence
         intervals and effect sizes when possible.</li></ul>
     </li>
     <li 
  class="enumerate" id="x1-56010x5"><span 
class="ecbx-1000">Reproducibility</span>: It is important to include all the data necessary to
     reproduce your results (e.g., means, variances, number of participants,
     demographics, number of conditions, excluded data points, etc.).</li></ol>
<!--l. 1661--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">7   </span> <a 
 id="x1-570007"></a>Limitations and Future Directions</h3>
<!--l. 1663--><p class="noindent" >This manuscript describes a systematic approach for designing and executing an
HRI-study over its lifecycle. The main steps and applicable guidelines at each stage
are provided, together with an interactive tool to serve as a guide and checklist, and
                                                                  

                                                                  
facilitate study development and monitoring. The interactive tool provides a
framework for systematically considering the study design, documenting the design
choices, and sharing the developed designs between researchers. For each step
in the life-cycle we also provide illustrative examples drawn from the HRI
literature.
<!--l. 1666--><p class="indent" >   While the paper provides a comprehensive overview of current best practices, it is
not exhaustive. Furthermore, while we have focused on the HRI (and to some
extend the HCI) literature, appropriate methodologies continue to be adapted
from complementary fields, e.g., psychology, neuroscience, and biomechanics.
The HRI field is young and rapidly evolving, and adaptation from other
fileds, where beneficial and highly relevant for HRI, should be considered in
experimental design. No doubt, new methodology and technology will emerge that
demonstrates that previous theories or ways of measuring phenomena are no
longer accepted or conventional. Researchers should continue to monitor
the HRI and related literature to remain up to date with advancements in
methodology.
<!--l. 1668--><p class="indent" >   One example where best practice is likely to change in the future is study
registration. While the importance of identification and registration of hypotheses has
been acknowledged by the HRI community, it is not yet a common practice in
the literature. In the near future, this might become a requirement with
more publication venues recognising the value of this approach for rigour in
HRI studies. Another area is the use of online studies and interaction tools,
particularly given the large shift to online interaction during the Covid-19
pandemic.
<!--l. 1671--><p class="indent" >   Considering the majority of HRI studies to date, and as noted by previous
reviews [<a 
href="#Xbethel2020conducting">27</a>,&#x00A0;<a 
href="#Xbartneck2020human">16</a>,&#x00A0;<a 
href="#Xhoffman2020primer">92</a>], the following guidelines are highlighted to improve future
studies:
<!--l. 1673--><p class="indent" >   <span 
class="ecbx-1000">Increase awareness and use of approaches in related fields: </span>The HRI
community should closely follow and incorporate findings in related fields to improve
HRI study methodology. This includes adapting theories from psychology and social
sciences to develop hypotheses, constructs and measures, and incorporating
innovations for measurement, data processing and data analysis from engineering,
statistics and machine learning.
<!--l. 1675--><p class="indent" >   <span 
class="ecbx-1000">Increase transparency: </span>A detailed description of the study process, including
any methodology or data processing errors, should be reported in the paper
to avoid misleading researchers on experiment results, which can further
contribute to the replication crisis as seen in other fields. Authors should consider
pre-registering their hypotheses, and open-source the code and data used in their
work.
<!--l. 1677--><p class="indent" >   <span 
class="ecbx-1000">Increased rigour: </span>Authors should strive towards improved practices in
recruitment, aiming for larger cohorts with appropriate demographics, and systematic
sampling, blinding and allocation methodology. Rigorous statistical methods for data
analysis should be employed.
<!--l. 1679--><p class="indent" >   Adoption of best practices and increased rigour in the design and deployment of
HRI studies will inevitably lead to better reproducability of findings and ensure that
findings are relevant in application deployments, avoiding costly failures [<a 
href="#Xhoffman2019anki">89</a>] and
                                                                  

                                                                  
increasing the impact of the field.
<!--l. 1--><p class="noindent" >
   <h3 class="likesectionHead"><a 
 id="x1-580007"></a>References</h3>
<!--l. 1--><p class="noindent" >
     <div class="thebibliography">
     <p class="bibitem" ><span class="biblabel">
  <a 
 id="Xabd2017trust"></a>[1] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Moaed&#x00A0;A  Abd,  Iker  Gonzalez,  Mehrdad  Nojoumian,  and  Erik&#x00A0;D
     Engeberg.    Trust,  satisfaction  and  frustration  measurements  during
     human-robot interaction. In <span 
class="ecti-1000">30th Florida Conference on Recent Advances</span>
     <span 
class="ecti-1000">in RoboticsMay</span>, pages 11&#8211;12, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
  <a 
 id="Xadmoni2017gaze"></a>[2] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Henny Admoni and Brian Scassellati. Social eye gaze in human-robot
     interaction: A review. <span 
class="ecti-1000">Journal of Human-Robot Interaction</span>, 6(1):25, Mar
     2017. ISSN 2163-0364. doi: 10.5898/JHRI.6.1.Admoni.
     </p>
     <p class="bibitem" ><span class="biblabel">
  <a 
 id="Xagah_human_2000"></a>[3] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Arvin     Agah.             Human     interactions     with     intelligent
     systems:    research    taxonomy.         27(1):71&#8211;107,    2000.         ISSN
     0045-7906.            doi:     10.1016/S0045-7906(00)00009-4.            URL
     <a 
href="http://www.sciencedirect.com/science/article/pii/S0045790600000094" class="url" ><span 
class="ectt-1000">http://www.sciencedirect.com/science/article/pii/S0045790600000094</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
  <a 
 id="Xahmad2017adaptive"></a>[4] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Muneeb&#x00A0;Imtiaz Ahmad, Omar Mubin, and Joanne Orlando. Adaptive
     social   robot   for   sustaining   social   engagement   during   long-term
     children&#8211;robot interaction.   <span 
class="ecti-1000">International Journal of Human&#8211;Computer</span>
     <span 
class="ecti-1000">Interaction</span>, 33(12):943&#8211;962, Dec 2017.  ISSN 1044-7318, 1532-7590.  doi:
     10.1080/10447318.2017.1300750.
     </p>
     <p class="bibitem" ><span class="biblabel">
  <a 
 id="Xakalin2019influence"></a>[5] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Neziha Akalin, Annica Kristoffersson, and Amy Loutfi. The influence
     of feedback type in robot-assisted training. <span 
class="ecti-1000">Multimodal Technologies and</span>
     <span 
class="ecti-1000">Interaction</span>, 3(4):67, 2019.
     </p>
     <p class="bibitem" ><span class="biblabel">
  <a 
 id="XAkgun2011"></a>[6] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Baris  Akgun  and  Kaushik  Subramanian.     Robot  learning  from
     demonstration: Kinesthetic teaching vs. teleoperation. page&#x00A0;7, 2011.
     </p>
                                                                  

                                                                  
     <p class="bibitem" ><span class="biblabel">
  <a 
 id="Xallen2017sage"></a>[7] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mike  Allen.    <span 
class="ecti-1000">The  SAGE  encyclopedia  of  communication  research</span>
     <span 
class="ecti-1000">methods</span>. Sage Publications, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
  <a 
 id="Xaltman2017points"></a>[8] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Naomi  Altman  and  Martin  Krzywinski.    Points  of  significance:
     interpreting p values, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
  <a 
 id="Xanzalone2015evaluating"></a>[9] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Salvatore&#x00A0;M   Anzalone,   Sofiane   Boucenna,   Serena   Ivaldi,   and
     Mohamed  Chetouani.   Evaluating  the  engagement  with  social  robots.
     <span 
class="ecti-1000">International Journal of Social Robotics</span>, 7(4):465&#8211;478, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xarai2010assessment"></a>[10] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tamio Arai, Ryu Kato, and Marina Fujita.  Assessment of operator
     stress induced by robot collaboration in assembly.  <span 
class="ecti-1000">CIRP annals</span>, 59(1):
     5&#8211;8, 2010.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xargall2009learning"></a>[11] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Brenna&#x00A0;D.  Argall,  Sonia  Chernova,  Manuela  Veloso,  and  Brett
     Browning. A survey of robot learning from demonstration. <span 
class="ecti-1000">Robotics and</span>
     <span 
class="ecti-1000">Autonomous Systems</span>, 57(5):469&#8211;483, May 2009.   ISSN 09218890.   doi:
     10.1016/j.robot.2008.10.024.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbaraglia2017efficient"></a>[12] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jimmy  Baraglia,  Maya  Cakmak,  Yukie  Nagai,  Rajesh&#x00A0;PN  Rao,
     and Minoru Asada.  Efficient human-robot collaboration: When should
     a   robot   take   initiative?      <span 
class="ecti-1000">The  International  Journal  of  Robotics</span>
     <span 
class="ecti-1000">Research</span>, 36(5-7):563&#8211;579, 2017.  doi: 10.1177/0278364916688253.  URL
     <a 
href="https://doi.org/10.1177/0278364916688253" class="url" ><span 
class="ectt-1000">https://doi.org/10.1177/0278364916688253</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbarrett2019emotional"></a>[13] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lisa&#x00A0;Feldman  Barrett,  Ralph  Adolphs,  Stacy  Marsella,  Aleix&#x00A0;M
     Martinez,  and  Seth&#x00A0;D  Pollak.    Emotional  expressions  reconsidered:
     Challenges   to   inferring   emotion   from   human   facial   movements.
     <span 
class="ecti-1000">Psychological science in the public interest</span>, 20(1):1&#8211;68, 2019.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XHRItextbook"></a>[14] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C.&#x00A0;Bartneck, T.&#x00A0;Belpaeme, F.&#x00A0;Eyssel, T.&#x00A0;Kanda, M.&#x00A0;Keijsers, and
     S.&#x00A0;Sabanovic.  <span 
class="ecti-1000">Human-Robot Interaction &#8211; An Introduction</span>.  Cambridge:
     Cambridge University Press, 2020.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbartneck2009measurement"></a>[15] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Christoph Bartneck, Dana Kuli&#263;, Elizabeth Croft, and Susana Zoghbi.
     Measurement instruments for the anthropomorphism, animacy, likeability,
     perceived  intelligence,  and  perceived  safety  of  robots.    <span 
class="ecti-1000">International</span>
     <span 
class="ecti-1000">journal of social robotics</span>, 1(1):71&#8211;81, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbartneck2020human"></a>[16] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Christoph  Bartneck,  Tony  Belpaeme,  Friederike  Eyssel,  Takayuki
     Kanda, Merel Keijsers, and Selma &#352;abanovi&#263;. <span 
class="ecti-1000">Human-Robot Interaction:</span>
     <span 
class="ecti-1000">An Introduction</span>. Cambridge University Press, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbaxter2016characterising"></a>[17] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Paul Baxter, James Kennedy, Emmanuel Senft, Severin Lemaignan,
     and Tony Belpaeme. From characterising three years of hri to methodology
     and reporting recommendations. In <span 
class="ecti-1000">2016 11th ACM/IEEE International</span>
     <span 
class="ecti-1000">Conference on Human-Robot Interaction (HRI)</span>, pages 391&#8211;398. IEEE,
     2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XBeer_Fisk_Rogers_2014"></a>[18] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jenay&#x00A0;M  Beer,  Arthur&#x00A0;D  Fisk,  and  Wendy&#x00A0;A  Rogers.   Toward  a
     framework  for  levels  of  robot  autonomy  in  human-robot  interaction.
     <span 
class="ecti-1000">Journal of Human-Robot Interaction</span>, 3(2):74, Jun 2014.   doi: 10.5898/
     JHRI.3.2.Beer. URL <a 
href="http://dl.acm.org/citation.cfm?id=3109833" class="url" ><span 
class="ectt-1000">http://dl.acm.org/citation.cfm?id=3109833</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbelhassein2019towards"></a>[19] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kathleen  Belhassein,  Guilhem  Buisan,  Aurlie  Clodic,  and  Rachid
     Alami. Towards methodological principles for user studies in human-robot
     interaction.     In  <span 
class="ecti-1000">Test  Methods  and  Metrics  for  Effective  HRI  in</span>
     <span 
class="ecti-1000">Collaborative Human-Robot Teams Workshop, ACM/IEEE International</span>
     <span 
class="ecti-1000">Conference on Human-Robot Interaction</span>, 2019.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XBellerElaineM2002Rict"></a>[20] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Elaine&#x00A0;M Beller, Val Gebski, and Anthony&#x00A0;C Keech. Randomisation
     in clinical trials.   <span 
class="ecti-1000">Medical Journal of Australia</span>, 177(10):565&#8211;567, 2002.
     ISSN 0025-729X.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbeller2002randomisation"></a>[21] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Elaine&#x00A0;M Beller, Val Gebski, and Anthony&#x00A0;C Keech. Randomisation
     in clinical trials. <span 
class="ecti-1000">Medical Journal of Australia</span>, 177(10):565&#8211;567, 2002.
     </p>
                                                                  

                                                                  
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbest2014sage"></a>[22] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Henning Best and Christof Wolf.  <span 
class="ecti-1000">The SAGE handbook of regression</span>
     <span 
class="ecti-1000">analysis and causal inference</span>. Sage, 2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbetella2016affective"></a>[23] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alberto  Betella  and  Paul&#x00A0;FMJ  Verschure.   The  affective  slider:  A
     digital self-assessment scale for the measurement of human emotions. <span 
class="ecti-1000">PloS</span>
     <span 
class="ecti-1000">one</span>, 11(2), 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbethel2009robots"></a>[24] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Cindy&#x00A0;L Bethel. Robots without faces: non-verbal social human-robot
     interaction. 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbethel2010review"></a>[25] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Cindy&#x00A0;L  Bethel  and  Robin&#x00A0;R  Murphy.   Review  of  human  studies
     methods in HRI and recommendations.  <span 
class="ecti-1000">International Journal of Social</span>
     <span 
class="ecti-1000">Robotics</span>, 2(4):347&#8211;359, 2010.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbethel2007survey"></a>[26] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Cindy&#x00A0;L Bethel, Kristen Salomon, Robin&#x00A0;R Murphy, and Jennifer&#x00A0;L
     Burke. Survey of psychophysiology measurements applied to human-robot
     interaction.  In <span 
class="ecti-1000">RO-MAN 2007-The 16th IEEE International Symposium</span>
     <span 
class="ecti-1000">on Robot and Human Interactive Communication</span>, pages 732&#8211;737. IEEE,
     2007.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbethel2020conducting"></a>[27] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Cindy&#x00A0;L Bethel, Zachary Henkel, and Kenna Baugus.   Conducting
     studies in human-robot interaction.  In <span 
class="ecti-1000">Human-Robot Interaction</span>, pages
     91&#8211;124. Springer, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xblanca2017non"></a>[28] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mara&#x00A0;J  Blanca,  Rafael  Alarcn,  Jaume  Arnau,  Roser  Bono,  and
     Rebecca  Bendayan.   Non-normal  data:  Is  anova  still  a  valid  option?
     <span 
class="ecti-1000">Psicothema</span>, 29(4):552&#8211;557, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbodala2020modeling"></a>[29] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Indu&#x00A0;P  Bodala,  Bing&#x00A0;Cai  Kok,  Weicong  Sng,  and  Harold  Soh.
     Modeling  the  interplay  of  trust  and  attention  in  hri:  An  autonomous
     vehicle  study.    In  <span 
class="ecti-1000">Companion  of  the  2020  ACM/IEEE  International</span>
     <span 
class="ecti-1000">Conference on Human-Robot Interaction</span>, pages 145&#8211;147, 2020.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbradley1994measuring"></a>[30] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Margaret&#x00A0;M  Bradley  and  Peter&#x00A0;J  Lang.   Measuring  emotion:  the
     self-assessment manikin and the semantic differential. <span 
class="ecti-1000">Journal of behavior</span>
     <span 
class="ecti-1000">therapy and experimental psychiatry</span>, 25(1):49&#8211;59, 1994.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xbreazeal_effects_2005"></a>[31] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>C.&#x00A0;Breazeal, C.D. Kidd, A.L. Thomaz, G.&#x00A0;Hoffman, and M.&#x00A0;Berlin.
     Effects  of  nonverbal  communication  on  efficiency  and  robustness  in
     human-robot teamwork. In <span 
class="ecti-1000">2005 IEEE/RSJ International Conference on</span>
     <span 
class="ecti-1000">Intelligent Robots and Systems</span>, page 708&#8211;713, Aug 2005.  doi: 10.1109/
     IROS.2005.1545011.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xcardinal2013anova"></a>[32] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Rudolf&#x00A0;N  Cardinal  and  Michael&#x00A0;RF  Aitken.     <span 
class="ecti-1000">ANOVA  for  the</span>
     <span 
class="ecti-1000">behavioral sciences researcher</span>. Psychology Press, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xcarpinella2017robotic"></a>[33] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Colleen&#x00A0;M  Carpinella,  Alisa&#x00A0;B  Wyman,  Michael&#x00A0;A  Perez,  and
     Steven&#x00A0;J  Stroessner.     The  robotic  social  attributes  scale  (RoSAS)
     development  and  validation.   In  <span 
class="ecti-1000">Proceedings  of  the  2017  ACM/IEEE</span>
     <span 
class="ecti-1000">International  Conference  on  human-robot  interaction</span>,  pages  254&#8211;262,
     2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xceha2019expression"></a>[34] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jessy   Ceha,   Nalin   Chhibber,   Joslin   Goh,   Corina   McDonald,
     Pierre-Yves Oudeyer, Dana Kuli&#263;, and Edith Law. Expression of curiosity
     in  social  robots:  Design,  perception,  and  effects  on  behaviour.    In
     <span 
class="ecti-1000">Proceedings of the 2019 CHI Conference on Human Factors in Computing</span>
     <span 
class="ecti-1000">Systems</span>, pages 1&#8211;12, 2019.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xchan2013handover"></a>[35] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Wesley&#x00A0;P Chan, Chris&#x00A0;AC Parker, HF&#x00A0;Machiel Van&#x00A0;der Loos, and
     Elizabeth&#x00A0;A Croft.  A human-inspired object handover controller.  <span 
class="ecti-1000">The</span>
     <span 
class="ecti-1000">International Journal of Robotics Research</span>, 32(8):971&#8211;983, Jul 2013. ISSN
     0278-3649. doi: 10.1177/0278364913488806.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xchan2020AR"></a>[36] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Wesley&#x00A0;P.  Chan,  Geoffrey  Hanks,  Maram  Sakr,  Tiger  Zuo,  H.&#x00A0;F.
     Machiel&#x00A0;Van  der  Loos,  and  Elizabeth&#x00A0;A.  Croft.      An  augmented
     reality  human-robot  physical  collaboration  interface  design  for  shared,
     large-scale, labour-intensive manufacturing tasks. <span 
class="ecti-1000">IROS</span>, 2020.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xchandler2014nonnaivete"></a>[37] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jesse Chandler, Pam Mueller, and Gabriele Paolacci.   Nonnavet
     among amazon mechanical turk workers: Consequences and solutions for
     behavioral researchers. <span 
class="ecti-1000">Behavior research methods</span>, 46(1):112&#8211;130, 2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xchandler_non-naive_2015"></a>[38] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jesse   Chandler,   Gabriele   Paolacci,   Eyal   Peer,   Pam   Mueller,
     and   Kate   Ratliff.      Non-Nave   Participants   Can   Reduce   Effect
     Sizes.       <span 
class="ecti-1000">ACR  North  American  Advances</span>,   NA-43,   2015.       URL
     <a 
href="https://www.acrwebsite.org/volumes/1020052/volumes/v43/NA-43" class="url" ><span 
class="ectt-1000">https://www.acrwebsite.org/volumes/1020052/volumes/v43/NA-43</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xchatterjee2012handbook"></a>[39] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Samprit Chatterjee and Jeffrey&#x00A0;S. Simonoff.  <span 
class="ecti-1000">Handbook of Regression</span>
     <span 
class="ecti-1000">Analysis</span>. Wiley, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XChristensenLarryB.2007Em"></a>[40] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Larry&#x00A0;B. Christensen.  <span 
class="ecti-1000">Experimental methodology</span>.  Pearson/Allyn &amp;
     Bacon, Boston, 10th ed. edition, 2007. ISBN 0205484735.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xcollins_drawing_2019"></a>[41] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Emily&#x00A0;C.  Collins.   Drawing  parallels  in  human-other  interactions:
     a  trans-disciplinary  approach  to  developing  human-robot  interaction
     methodologies. 374(1771):20180433, 2019. ISSN 1471-2970. doi: 10.1098/
     rstb.2018.0433.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xcorreia2018exploring"></a>[42] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Filipa Correia, Carla Guerra, Samuel Mascarenhas, Francisco&#x00A0;S Melo,
     and Ana Paiva. Exploring the impact of fault justification in human-robot
     trust. In <span 
class="ecti-1000">Proceedings of the 17th International Conference on Autonomous</span>
     <span 
class="ecti-1000">Agents and MultiAgent Systems</span>, pages 507&#8211;513, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xcosta2017research"></a>[43] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Antnio&#x00A0;Pedro Costa, Francisl&#x00A0;Neri de&#x00A0;Sousa, Antnio Moreira, and
     Dayse&#x00A0;Neri de&#x00A0;Souza.  Research through design: qualitative analysis to
     evaluate the usability.  In <span 
class="ecti-1000">Computer supported qualitative research</span>, pages
     1&#8211;12. Springer, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xcumming2014new"></a>[44] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Geoff Cumming.   The new statistics: Why and how.   <span 
class="ecti-1000">Psychological</span>
     <span 
class="ecti-1000">science</span>, 25(1):7&#8211;29, 2014.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xdamacharla2018common"></a>[45] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Praveen  Damacharla,  Ahmad&#x00A0;Y  Javaid,  Jennie&#x00A0;J  Gallimore,  and
     Vijay&#x00A0;K Devabhaktuni. Common metrics to benchmark human-machine
     teams (hmt): A review. <span 
class="ecti-1000">IEEE Access</span>, 6:38637&#8211;38655, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xdautenhahn2007methodology"></a>[46] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kerstin  Dautenhahn.     Methodology  &amp;  themes  of  human-robot
     interaction: A growing research field.  <span 
class="ecti-1000">International Journal of Advanced</span>
     <span 
class="ecti-1000">Robotic Systems</span>, 4(1):15, 2007.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xdautenhahn2018some"></a>[47] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kerstin Dautenhahn.  Some brief thoughts on the past and future of
     human-robot interaction, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xdautenhahn2011new"></a>[48] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kerstin Dautenhahn and Joe Saunders. <span 
class="ecti-1000">New frontiers in human robot</span>
     <span 
class="ecti-1000">interaction</span>, volume&#x00A0;2. John Benjamins Publishing, 2011.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xde2016ethical"></a>[49] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Maartje&#x00A0;MA  de&#x00A0;Graaf.     An  ethical  evaluation  of  human&#8211;robot
     relationships. <span 
class="ecti-1000">International journal of social robotics</span>, 8(4):589&#8211;598, 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xde2016long"></a>[50] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Maartje&#x00A0;MA  de&#x00A0;Graaf,  Somaya&#x00A0;Ben  Allouch,  and  Jan&#x00A0;AGM  van
     Dijk.  Long-term evaluation of a social robot in real homes.  <span 
class="ecti-1000">Interaction</span>
     <span 
class="ecti-1000">studies</span>, 17(3):462&#8211;491, 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xdini2017measurement"></a>[51] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>A.&#x00A0;Dini, C.&#x00A0;Murko, S.&#x00A0;Yahyanejad, U.&#x00A0;Augsdrfer, M.&#x00A0;Hofbaur, and
     L.&#x00A0;Paletta.    Measurement  and  prediction  of  situation  awareness  in
     human-robot interaction based on a framework of probabilistic attention.
     In <span 
class="ecti-1000">2017 IEEE/RSJ International Conference on Intelligent Robots and</span>
     <span 
class="ecti-1000">Systems (IROS)</span>, pages 4354&#8211;4361, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xdix2020statistics"></a>[52] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>A.&#x00A0;Dix.       <span 
class="ecti-1000">Statistics   for   HCI:   Making   Sense   of   Quantitative</span>
     <span 
class="ecti-1000">Data</span>.     Morgan  &amp;  Claypool,  2020.     ISBN  9781681737447.     URL
     <a 
href="https://ieeexplore-ieee-org.ezproxy.lib.monash.edu.au/document/9070780" class="url" ><span 
class="ectt-1000">https://ieeexplore-ieee-org.ezproxy.lib.monash.edu.au/document/9070780</span></a>.
     </p>
                                                                  

                                                                  
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XDoigGordonS2005Raac"></a>[53] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Gordon&#x00A0;S Doig and Fiona Simpson.  Randomization and allocation
     concealment: a practical guide for researchers.  <span 
class="ecti-1000">Journal of Critical Care</span>,
     20(2):187&#8211;191, 2005. ISSN 0883-9441.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xdonmez2009evaluation"></a>[54] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Birsen Donmez, Patricia&#x00A0;E Pina, and Mary&#x00A0;L Cummings. Evaluation
     criteria  for  human-automation  performance  metrics.   In  <span 
class="ecti-1000">Performance</span>
     <span 
class="ecti-1000">evaluation and benchmarking of intelligent systems</span>, pages 21&#8211;40. Springer,
     2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xdragan2015effects"></a>[55] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Anca&#x00A0;D  Dragan,  Shira  Bauman,  Jodi  Forlizzi,  and  Siddhartha&#x00A0;S
     Srinivasa. Effects of robot motion on human-robot collaboration. In <span 
class="ecti-1000">2015</span>
     <span 
class="ecti-1000">10th ACM/IEEE International Conference on Human-Robot Interaction</span>
     <span 
class="ecti-1000">(HRI)</span>, pages 51&#8211;58. IEEE, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xedwards2008teacher"></a>[56] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Bosede&#x00A0;I.  Edwards,  Idris&#x00A0;O.  Muniru,  Nosiba  Khougali,  Adrian&#x00A0;D.
     Cheok, and Rui Prada.  A physically embodied robot teacher (pert) as
     a  facilitator  for  peer  learning.   In  <span 
class="ecti-1000">2018 IEEE Frontiers in Education</span>
     <span 
class="ecti-1000">Conference (FIE)</span>, page 1&#8211;9, Oct 2018. doi: 10.1109/FIE.2018.8658445.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xelo2008qualitative"></a>[57] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Satu Elo and Helvi Kyngs. The qualitative content analysis process.
     <span 
class="ecti-1000">Journal of advanced nursing</span>, 62(1):107&#8211;115, 2008.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xendsley1998comparative"></a>[58] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mica&#x00A0;R  Endsley,  Stephen&#x00A0;J  Selcon,  Thomas&#x00A0;D  Hardiman,  and
     Darryl&#x00A0;G Croft. A comparative analysis of sagat and sart for evaluations
     of  situation  awareness.     In  <span 
class="ecti-1000">Proceedings  of  the  human  factors  and</span>
     <span 
class="ecti-1000">ergonomics  society  annual  meeting</span>,  volume&#x00A0;42,  pages  82&#8211;86.  SAGE
     Publications Sage CA: Los Angeles, CA, 1998.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xerceg2008modern"></a>[59] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>David&#x00A0;M  Erceg-Hurn  and  Vikki&#x00A0;M  Mirosevich.    Modern  robust
     statistical methods: an easy way to maximize the accuracy and power of
     your research. <span 
class="ecti-1000">American Psychologist</span>, 63(7):591, 2008.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xetikan2016comparison"></a>[60] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ilker   Etikan,   Sulaiman&#x00A0;Abubakar   Musa,   and   Rukayya&#x00A0;Sunusi
                                                                  

                                                                  
     Alkassim.  Comparison of convenience sampling and purposive sampling.
     <span 
class="ecti-1000">American journal of theoretical and applied statistics</span>, 5(1):1&#8211;4, 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xetz2018introduction"></a>[61] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alexander Etz and Joachim Vandekerckhove. Introduction to bayesian
     inference for psychology.   <span 
class="ecti-1000">Psychonomic Bulletin &amp; Review</span>, 25(1):5&#8211;34,
     2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xetz2018become"></a>[62] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alexander  Etz,  Quentin&#x00A0;F  Gronau,  Fabian  Dablander,  Peter&#x00A0;A
     Edelsbrunner, and Beth Baribault.  How to become a bayesian in eight
     easy steps: An annotated reading list.  <span 
class="ecti-1000">Psychonomic Bulletin &amp; Review</span>,
     25(1):219&#8211;234, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xfaul2007g"></a>[63] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Franz Faul, Edgar Erdfelder, Albert-Georg Lang, and Axel Buchner.
     G* power 3: A flexible statistical power analysis program for the social,
     behavioral, and biomedical sciences.  <span 
class="ecti-1000">Behavior research methods</span>, 39(2):
     175&#8211;191, 2007.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xfeil2009human"></a>[64] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>David Feil-Seifer and Maja&#x00A0;J Mataric. Human robotinteraction. 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xfeir1974empirical"></a>[65] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Betty&#x00A0;J Feir-Walsh and Larry&#x00A0;E Toothaker. An empirical comparison
     of  the  anova  f-test,  normal  scores  test  and  kruskal-wallis  test  under
     violation of assumptions. <span 
class="ecti-1000">Educational and Psychological Measurement</span>, 34
     (4):789&#8211;799, 1974.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xfernaeus2010you"></a>[66] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ylva  Fernaeus,  Maria  Hkansson,  Mattias  Jacobsson,  and  Sara
     Ljungblad. How do you play with a robotic toy animal? a long-term study
     of pleo. In <span 
class="ecti-1000">Proceedings of the 9th international Conference on interaction</span>
     <span 
class="ecti-1000">Design and Children</span>, pages 39&#8211;48, 2010.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xfrederiksen2019audio"></a>[67] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Morten&#x00A0;Roed  Frederiksen  and  Kasper  Stoey.     Augmenting  the
     audio-based  expression  modality  of  a  non-affective  robot.    In  <span 
class="ecti-1000">2019</span>
     <span 
class="ecti-1000">8th  International  Conference  on  Affective  Computing  and  Intelligent</span>
     <span 
class="ecti-1000">Interaction (ACII)</span>,  page  144&#8211;149,  Sep  2019.   doi:  10.1109/ACII.2019.
     8925510.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xfreund2010statistical"></a>[68] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Rudolf Freund, Donna Mohr, and William Wilson. <span 
class="ecti-1000">Statistical Methods</span>.
     Elsevier, 2010.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xfry2006ethics"></a>[69] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Craig&#x00A0;L Fry, Wayne Hall, Alison Ritter, and Rebecca Jenkinson. The
     ethics of paying drug users who participate in research: A review and
     practical recommendations.   <span 
class="ecti-1000">Journal of Empirical Research on Human</span>
     <span 
class="ecti-1000">Research Ethics</span>, 1(4):21&#8211;35, 2006.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xgardner1986confidence"></a>[70] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Martin&#x00A0;J  Gardner  and  Douglas&#x00A0;G  Altman.    Confidence  intervals
     rather than p values: estimation rather than hypothesis testing.  <span 
class="ecti-1000">Br Med</span>
     <span 
class="ecti-1000">J (Clin Res Ed)</span>, 292(6522):746&#8211;750, 1986.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xgaudiello2016trust"></a>[71] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ilaria  Gaudiello,  Elisabetta  Zibetti,  Sbastien  Lefort,  Mohamed
     Chetouani, and Serena Ivaldi. Trust as indicator of robot functional and
     social acceptance. an experimental study on user conformation to icub
     answers. <span 
class="ecti-1000">Computers in Human Behavior</span>, 61:633&#8211;655, 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xgeorge2017updating"></a>[72] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Christopher&#x00A0;H George, S&#x00A0;Clare Stanford, Steve Alexander, Giuseppe
     Cirino,  James&#x00A0;R  Docherty,  Mark&#x00A0;A  Giembycz,  Daniel  Hoyer,  Paul&#x00A0;A
     Insel, Angelo&#x00A0;A Izzo, Yong Ji, et&#x00A0;al.  Updating the guidelines for data
     transparency in the british journal of pharmacology&#8211;data sharing and the
     use of scatter plots instead of bar charts. <span 
class="ecti-1000">British journal of pharmacology</span>,
     174(17):2801, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xgergle2014experimental"></a>[73] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Darren Gergle and Desney&#x00A0;S Tan.  Experimental research in hci.  In
     <span 
class="ecti-1000">Ways of Knowing in HCI</span>, pages 191&#8211;227. Springer, 2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xgielniak2012exageration"></a>[74] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Michael&#x00A0;J. Gielniak and Andrea&#x00A0;L. Thomaz.  Enhancing interaction
     through  exaggerated  motion  synthesis.     In  <span 
class="ecti-1000">2012  7th  ACM/IEEE</span>
     <span 
class="ecti-1000">International  Conference  on  Human-Robot  Interaction  (HRI)</span>,  page
     375&#8211;382, Mar 2012. doi: 10.1145/2157689.2157813.
     </p>
                                                                  

                                                                  
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xgoodrich2007human"></a>[75] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Michael  Goodrich  and  Alan  Schultz.   Human-robot  interaction:  A
     survey.   <span 
class="ecti-1000">Foundations  and  Trends  in  Human-Computer  Interaction</span>,  1:
     203&#8211;275, 01 2007. doi: 10.1561/1100000005.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xgreen2003learnability"></a>[76] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>A.&#x00A0;Green   and   K.S.   Eklundh.      Designing   for   learnability   in
     human-robot   communication.       <span 
class="ecti-1000">IEEE   Transactions   on   Industrial</span>
     <span 
class="ecti-1000">Electronics</span>, 50(4):644&#8211;650, Aug 2003. ISSN 1557-9948. doi: 10.1109/TIE.
     2003.814763.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xgrissom2000heterogeneity"></a>[77] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Robert&#x00A0;J Grissom. Heterogeneity of variance in clinical data. <span 
class="ecti-1000">Journal</span>
     <span 
class="ecti-1000">of consulting and clinical psychology</span>, 68(1):155, 2000.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhaddadi2013gestures"></a>[78] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Amir   Haddadi,   Elizabeth&#x00A0;A.   Croft,   Brian&#x00A0;T.   Gleeson,   Karon
     MacLean,  and  Javier  Alcazar.     Analysis  of  task-based  gestures  in
     human-robot  interaction.   In  <span 
class="ecti-1000">2013  IEEE  International  Conference  on</span>
     <span 
class="ecti-1000">Robotics and Automation</span>, page 2146&#8211;2152, May 2013. doi: 10.1109/ICRA.
     2013.6630865.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhamilton2020tradeoffs"></a>[79] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jared Hamilton, Nhan Tran, and Tom Williams.  Tradeoffs between
     effectiveness and social perception when using mixed reality to supplement
     gesturally limited robots. In <span 
class="ecti-1000">Proceedings of the 3rd International Workshop</span>
     <span 
class="ecti-1000">on Virtual, Augmented, and Mixed Reality for HRI</span>, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhancock2011meta"></a>[80] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Peter&#x00A0;A Hancock, Deborah&#x00A0;R Billings, Kristin&#x00A0;E Schaefer, Jessie&#x00A0;YC
     Chen, Ewart&#x00A0;J De&#x00A0;Visser, and Raja Parasuraman.  A meta-analysis of
     factors affecting trust in human-robot interaction. <span 
class="ecti-1000">Human factors</span>, 53(5):
     517&#8211;527, 2011.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xharmon2016discrete"></a>[81] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Cindy  Harmon-Jones,  Brock  Bastian,  and  Eddie  Harmon-Jones.
     The  discrete  emotions  questionnaire:  A  new  tool  for  measuring  state
     self-reported emotions. <span 
class="ecti-1000">PloS one</span>, 11(8), 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xharriott2015measuring"></a>[82] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Caroline&#x00A0;E  Harriott,  Tao  Zhang,  Glenna&#x00A0;L  Buford,  and  Julie&#x00A0;A
                                                                  

                                                                  
     Adams. Measuring human workload in a collaborative human-robot team.
     <span 
class="ecti-1000">Journal of Human-Robot Interaction</span>, 4(2):61&#8211;96, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xharwell1992summarizing"></a>[83] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Michael&#x00A0;R  Harwell,  Elaine&#x00A0;N  Rubinstein,  William&#x00A0;S  Hayes,  and
     Corley&#x00A0;C  Olds.    Summarizing  monte  carlo  results  in  methodological
     research: The one-and two-factor fixed effects anova cases.   <span 
class="ecti-1000">Journal of</span>
     <span 
class="ecti-1000">educational statistics</span>, 17(4):315&#8211;339, 1992.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhauser2016attentive"></a>[84] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>David&#x00A0;J  Hauser  and  Norbert  Schwarz.   Attentive  turkers:  Mturk
     participants perform better on online attention checks than do subject
     pool participants. <span 
class="ecti-1000">Behavior research methods</span>, 48(1):400&#8211;407, 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xheale2015validity"></a>[85] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Roberta  Heale  and  Alison  Twycross.    Validity  and  reliability  in
     quantitative studies. <span 
class="ecti-1000">Evidence-based nursing</span>, 18(3):66&#8211;67, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xheerink2009measuring"></a>[86] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Marcel  Heerink,  Ben  Krose,  Vanessa  Evers,  and  Bob  Wielinga.
     Measuring acceptance of an assistive social robot: a suggested toolkit. In
     <span 
class="ecti-1000">RO-MAN 2009-The 18th IEEE International Symposium on Robot and</span>
     <span 
class="ecti-1000">Human Interactive Communication</span>, pages 528&#8211;533. IEEE, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="XHigginsTrevor2003RiCT"></a>[87] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Trevor Higgins. Randomization in clinical trials: Theory and practice.
     <span 
class="ecti-1000">Current Medical Research and Opinion</span>, 19(1):67, 2003.  ISSN 03007995.
     URL <a 
href="http://search.proquest.com/docview/207967010/" class="url" ><span 
class="ectt-1000">http://search.proquest.com/docview/207967010/</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhilton2006statnote"></a>[88] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Anthony  Hilton  and  Richard&#x00A0;A  Armstrong.   Statnote  6:  post-hoc
     anova tests. <span 
class="ecti-1000">Microbiologist</span>, 2006:34&#8211;36, 2006.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhoffman2019anki"></a>[89] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Guy Hoffman.  Anki, jibo, and kuri: What we can learn from social
     robots that didn&#8217;t make it. <span 
class="ecti-1000">IEEE Spectrum</span>, 2019.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhoffman2019evaluating"></a>[90] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Guy Hoffman. Evaluating fluency in human&#8211;robot collaboration. <span 
class="ecti-1000">IEEE</span>
     <span 
class="ecti-1000">Transactions on Human-Machine Systems</span>, 49(3):209&#8211;218, 2019.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhoffman2007effects"></a>[91] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Guy Hoffman and Cynthia Breazeal.  Effects of anticipatory action
     on human-robot teamwork efficiency, fluency, and perception of team. In
     <span 
class="ecti-1000">Proceedings of the ACM/IEEE international conference on Human-robot</span>
     <span 
class="ecti-1000">interaction</span>, pages 1&#8211;8, 2007.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhoffman2020primer"></a>[92] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Guy Hoffman and Xuan Zhao.  A primer for conducting experiments
     in  human&#8211;robot  interaction.     <span 
class="ecti-1000">ACM  Transactions  on  Human-Robot</span>
     <span 
class="ecti-1000">Interaction (THRI)</span>, 10(1):1&#8211;31, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhohenstein2020ai"></a>[93] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jess Hohenstein and Malte Jung.  Ai as a moral crumple zone: The
     effects of ai-mediated communication on attribution and trust. <span 
class="ecti-1000">Computers</span>
     <span 
class="ecti-1000">in Human Behavior</span>, 106:106190, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhomack2001understanding"></a>[94] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Susan&#x00A0;R  Homack.   Understanding  what  anova  post  hoc  tests  are,
     really. 2001.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhomburg2018include"></a>[95] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Nadine Homburg. How to include humanoid robots into experimental
     research:  A  multi-step  approach.    In  <span 
class="ecti-1000">Proceedings  of  the  51st  Hawaii</span>
     <span 
class="ecti-1000">International Conference on System Sciences</span>, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhonig2018understanding"></a>[96] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Shanee  Honig  and  Tal  Oron-Gilad.   Understanding  and  resolving
     failures   in   human-robot   interaction:   Literature   review   and   model
     development. <span 
class="ecti-1000">Frontiers in psychology</span>, 9:861, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhornbaek2013some"></a>[97] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kasper     Hornbk.               Some     whys     and     hows     of
     experiments in human&#8211;computer interaction. <span 
class="ecti-1000">Foundations and Trends in</span>
     <span 
class="ecti-1000">Human-Computer Interaction</span>, 5(4):299&#8211;373, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhorstmann2019great"></a>[98] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Aike&#x00A0;C  Horstmann  and  Nicole&#x00A0;C  Krmer.    Great  expectations?
     relation of previous experiences with social robots in real life or in the
     media and expectancies based on qualitative and quantitative assessment.
     <span 
class="ecti-1000">Frontiers in psychology</span>, 10:939, 2019.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
 <a 
 id="Xhrobjartsson2009reporting"></a>[99] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Asbjrn Hrbjartsson, Julie Pildal, An-Wen Chan, Mette&#x00A0;T Haahr,
     Douglas&#x00A0;G Altman, and Peter&#x00A0;C Gtzsche. Reporting on blinding in trial
     protocols and corresponding publications was often inadequate but rarely
     contradictory. <span 
class="ecti-1000">Journal of clinical epidemiology</span>, 62(9):967&#8211;973, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xhsu1996multiple"></a>[100] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jason Hsu.  <span 
class="ecti-1000">Multiple comparisons: theory and methods</span>.  CRC Press,
     1996.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xhyland1981nature"></a>[101] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Michael Hyland. <span 
class="ecti-1000">The Nature of Hypothetical Constructs</span>, pages 42&#8211;58.
     Macmillan Education UK, London, 1981.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xirfan2018social"></a>[102] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Bahar    Irfan,    James    Kennedy,    Sverin    Lemaignan,    Fotios
     Papadopoulos, Emmanuel Senft, and Tony Belpaeme.  Social psychology
     and human-robot interaction: An uneasy marriage. In <span 
class="ecti-1000">Companion of the</span>
     <span 
class="ecti-1000">2018 ACM/IEEE International Conference on Human-Robot Interaction</span>,
     pages 13&#8211;20, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xjian2000foundations"></a>[103] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jiun-Yin  Jian,  Ann&#x00A0;M  Bisantz,  and  Colin&#x00A0;G  Drury.   Foundations
     for  an  empirically  determined  scale  of  trust  in  automated  systems.
     <span 
class="ecti-1000">International journal of cognitive ergonomics</span>, 4(1):53&#8211;71, 2000.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xjiang_mixed-initiative_2015"></a>[104] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Shu  Jiang  and  Ronald&#x00A0;C.  Arkin.    Mixed-initiative  human-robot
     interaction: Definition, taxonomy, and survey. In <span 
class="ecti-1000">2015 IEEE International</span>
     <span 
class="ecti-1000">Conference on Systems, Man, and Cybernetics</span>, pages 954&#8211;961, 2015. doi:
     10.1109/SMC.2015.174.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xbruke2016educational"></a>[105] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Robert                                                                          Burke
     Johnson; Larry B. Christensen;&#x00A0;Burke Johnson.  <span 
class="ecti-1000">Educational Research:</span>
     <span 
class="ecti-1000">Quantitative, Qualitative, and Mixed Approaches</span>. Sage Publications, Inc,
     hardcover edition, 2016. ISBN 1483391604,9781483391601.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xjohnston2010strategies"></a>[106] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>James&#x00A0;M Johnston and Henry&#x00A0;S Pennypacker. <span 
class="ecti-1000">Strategies and tactics</span>
     <span 
class="ecti-1000">of behavioral research</span>. Routledge, 2010.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkanda2009affective"></a>[107] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Takayuki Kanda, Masahiro Shiomi, Zenta Miyashita, Hiroshi Ishiguro,
     and Norihiro Hagita.  An affective guide robot in a shopping mall.  In
     <span 
class="ecti-1000">Proceedings of the 4th ACM/IEEE international conference on Human</span>
     <span 
class="ecti-1000">robot interaction</span>, pages 173&#8211;180, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkang2008issues"></a>[108] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Minsoo  Kang,  Brian&#x00A0;G  Ragan,  and  Jae-Hyeon  Park.    Issues  in
     outcomes research: an overview of randomization techniques for clinical
     trials. <span 
class="ecti-1000">Journal of athletic training</span>, 43(2):215&#8211;221, 2008.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrandomisation200"></a>[109] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Minsoo  Kang,  Brian&#x00A0;G.  Ragan,  and  Jae-Hyeon  Park.     Issues
     in   outcomes   research:   An   overview   of   randomization   techniques
     for  clinical  trials.     <span 
class="ecti-1000">Journal  of  Athletic  Training</span>,  43(2):215&#8211;221,  3
     2008.     ISSN  1062-6050.     doi:  10.4085/1062-6050-43.2.215.     URL
     <a 
href="https://doi.org/10.4085/1062-6050-43.2.215" class="url" ><span 
class="ectt-1000">https://doi.org/10.4085/1062-6050-43.2.215</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkaplan2019relationship"></a>[110] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alexandra&#x00A0;D  Kaplan,  Tracy  Sanders,  and  Peter&#x00A0;A  Hancock.   The
     relationship between extroversion and the tendency to anthropomorphize
     robots: A bayesian analysis. <span 
class="ecti-1000">Frontiers in Robotics and AI</span>, 5:135, 2019.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkaptein2012rethinking"></a>[111] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Maurits Kaptein and Judy Robertson. Rethinking statistical analysis
     methods for chi.  In <span 
class="ecti-1000">Proceedings of the SIGCHI Conference on Human</span>
     <span 
class="ecti-1000">Factors in Computing Systems</span>, pages 1105&#8211;1114, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkitagawa2020effectiveness"></a>[112] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Hiroki   Kitagawa,   Toshihito   Nomura,   Tanuza   Nazmul,   Keitaro
     Omori,  Norifumi  Shigemoto,  Takemasa  Sakaguchi,  and  Hiroki  Ohge.
     Effectiveness of 222-nm ultraviolet light on disinfecting sars-cov-2 surface
     contamination. <span 
class="ecti-1000">American journal of infection control</span>, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkitchenham2017robust"></a>[113] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Barbara Kitchenham, Lech Madeyski, David Budgen, Jacky Keung,
     Pearl Brereton, Stuart Charters, Shirley Gibbs, and Amnart Pohthong.
     Robust statistical methods for empirical software engineering.  <span 
class="ecti-1000">Empirical</span>
     <span 
class="ecti-1000">Software Engineering</span>, 22(2):579&#8211;630, 2017.
     </p>
                                                                  

                                                                  
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XKoay2009"></a>[114] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kheng  Koay,  Dag&#x00A0;Sverre  Syrdal,  Michael  Walters,  and  Kerstin
     Dautenhahn.  Five weeks in the robot house - exploratory human-robot
     interaction trials in a domestic setting.   page 219&#8211;226, Feb 2009.   doi:
     10.1109/ACHI.2009.62.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkrauth2000experimental"></a>[115] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Joachim Krauth. <span 
class="ecti-1000">Experimental design: a handbook and dictionary for</span>
     <span 
class="ecti-1000">medical and behavioral research</span>, volume&#x00A0;14. Elsevier, 2000.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkruschke2018bayesian"></a>[116] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>John&#x00A0;K Kruschke and Torrin&#x00A0;M Liddell. The bayesian new statistics:
     Hypothesis testing, estimation, meta-analysis, and power analysis from
     a bayesian perspective.  <span 
class="ecti-1000">Psychonomic Bulletin &amp; Review</span>, 25(1):178&#8211;206,
     2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xkulic2007affective"></a>[117] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>D.&#x00A0;Kulic and E.&#x00A0;A. Croft. Affective state estimation for human&#8211;robot
     interaction. <span 
class="ecti-1000">IEEE Transactions on Robotics</span>, 23(5):991&#8211;1000, 2007.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlane2014visual"></a>[118] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Justin&#x00A0;D  Lane  and  David&#x00A0;L  Gast.     Visual  analysis  in  single
     case   experimental   design   studies:   Brief   review   and   guidelines.
     <span 
class="ecti-1000">Neuropsychological rehabilitation</span>, 24(3-4):445&#8211;463, 2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlasota_survey_2017"></a>[119] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Przemyslaw&#x00A0;A.  Lasota,  Terrence  Fong,  and  Julie&#x00A0;A.  Shah.     A
     survey  of  methods  for  safe  human-robot  interaction.    5(4):261&#8211;349,
     2017.    ISSN  1935-8253,  1935-8261.    doi:  10.1561/2300000052.    URL
     <a 
href="https://www.nowpublishers.com/article/Details/ROB-052" class="url" ><span 
class="ectt-1000">https://www.nowpublishers.com/article/Details/ROB-052</span></a>.
     Publisher: Now Publishers, Inc.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlazar2017research"></a>[120] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jonathan Lazar, Jinjuan&#x00A0;Heidi Feng, and Harry Hochheiser. <span 
class="ecti-1000">Research</span>
     <span 
class="ecti-1000">methods in human-computer interaction</span>. Morgan Kaufmann, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlee2007habituation"></a>[121] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kheng Lee&#x00A0;Koay, Dag&#x00A0;Sverre Syrdal, Michael&#x00A0;L. Walters, and Kerstin
     Dautenhahn.  Living with robots: Investigating the habituation effect in
     participants&#8217; preferences during a longitudinal human-robot interaction
     study.  In <span 
class="ecti-1000">RO-MAN 2007 - The 16th IEEE International Symposium on</span>
                                                                  

                                                                  
     <span 
class="ecti-1000">Robot and Human Interactive Communication</span>, page 564&#8211;569, Aug 2007.
     doi: 10.1109/ROMAN.2007.4415149.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xleichtmann_how_2020"></a>[122] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Benedikt  Leichtmann  and  Verena  Nitsch.     How  much  distance
     do   humans   keep   toward   robots?   literature   review,   meta-analysis,
     and   theoretical   considerations   on   personal   space   in   human-robot
     interaction.        <span 
class="ecti-1000">Journal   of   Environmental   Psychology</span>,   68:101386,
     2020.     ISSN  02724944.     doi:  10.1016/j.jenvp.2019.101386.     URL
     <a 
href="https://linkinghub.elsevier.com/retrieve/pii/S0272494419303846" class="url" ><span 
class="ectt-1000">https://linkinghub.elsevier.com/retrieve/pii/S0272494419303846</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlemaignan2016learning"></a>[123] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Sverin Lemaignan, Alexis Jacq, Deanna Hood, Fernando Garcia, Ana
     Paiva, and Pierre Dillenbourg. Learning by teaching a robot: The case of
     handwriting. <span 
class="ecti-1000">IEEE Robotics &amp; Automation Magazine</span>, 23(2):56&#8211;66, 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlemmens2000ethics"></a>[124] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Trudo Lemmens and Benjamin Freedman.   Ethics review for sale?
     conflict of interest and commercial research review boards.  <span 
class="ecti-1000">The Milbank</span>
     <span 
class="ecti-1000">Quarterly</span>, 78(4):547&#8211;584, 2000.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xleong2006psychology"></a>[125] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Frederick&#x00A0;TL Leong and James&#x00A0;T Austin.  <span 
class="ecti-1000">The psychology research</span>
     <span 
class="ecti-1000">handbook: A guide for graduate students and research assistants</span>.  Sage,
     2006.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlittle2013nonparametric"></a>[126] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Todd&#x00A0;D. Little, Trent&#x00A0;D. Buskirk, Lisa&#x00A0;M. Willoughby, and Terry&#x00A0;T.
     Tomazic. Nonparametric statistical techniques. In <span 
class="ecti-1000">The Oxford Handbook</span>
     <span 
class="ecti-1000">of Quantitative Methods in Psychology: Vol. 2: Statistical Analysis</span>, pages
     1&#8211;68. Oxford University Press, 10 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xlitwin2003assess"></a>[127] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mark&#x00A0;S Litwin and Arlene Fink.  <span 
class="ecti-1000">How to assess and interpret survey</span>
     <span 
class="ecti-1000">psychometrics</span>, volume&#x00A0;8. Sage, 2003.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XLiu2018gesture"></a>[128] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Hongyi Liu and Lihui Wang.  Gesture recognition for human-robot
     collaboration: A review. <span 
class="ecti-1000">International Journal of Industrial Ergonomics</span>,
     68:355&#8211;367, Nov 2018. ISSN 01698141. doi: 10.1016/j.ergon.2017.02.004.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xly2018bayesian"></a>[129] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alexander  Ly,  Akash  Raj,  Alexander  Etz,  Maarten  Marsman,
     Quentin&#x00A0;F Gronau, and Eric-Jan Wagenmakers. Bayesian reanalyses from
     summary statistics: A guide for academic consumers. <span 
class="ecti-1000">Advances in Methods</span>
     <span 
class="ecti-1000">and Practices in Psychological Science</span>, 1(3):367&#8211;374, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmarvel2020towards"></a>[130] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jeremy&#x00A0;A  Marvel,  Shelly  Bagchi,  Megan  Zimmerman,  and  Brian
     Antonishek.    Towards  effective  interface  designs  for  collaborative  hri
     in  manufacturing:  Metrics  and  measures.     <span 
class="ecti-1000">ACM  Transactions  on</span>
     <span 
class="ecti-1000">Human-Robot Interaction (THRI)</span>, 9(4):1&#8211;55, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmauchly1940significance"></a>[131] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>John&#x00A0;W Mauchly. Significance test for sphericity of a normal n-variate
     distribution. <span 
class="ecti-1000">The Annals of Mathematical Statistics</span>, 11(2):204&#8211;209, 1940.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmaxwell2008sample"></a>[132] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Scott&#x00A0;E Maxwell, Ken Kelley, and Joseph&#x00A0;R Rausch.   Sample size
     planning  for  statistical  power  and  accuracy  in  parameter  estimation.
     <span 
class="ecti-1000">Annual review of psychology</span>, 59, 2008.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmaxwell2015psychology"></a>[133] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Scott&#x00A0;E  Maxwell,  Michael&#x00A0;Y  Lau,  and  George&#x00A0;S  Howard.     Is
     psychology  suffering  from  a  replication  crisis?  what  does  &#8220;failure  to
     replicate&#8221; really mean? <span 
class="ecti-1000">American Psychologist</span>, 70(6):487, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmcdonald2009handbook"></a>[134] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>John&#x00A0;H McDonald. <span 
class="ecti-1000">Handbook of biological statistics</span>, volume&#x00A0;2. 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmchugh2012interrater"></a>[135] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mary&#x00A0;L McHugh. Interrater reliability: the kappa statistic. <span 
class="ecti-1000">Biochemia</span>
     <span 
class="ecti-1000">medica: Biochemia medica</span>, 22(3):276&#8211;282, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmills2014qualitative"></a>[136] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jane Mills and Melanie Birks.  <span 
class="ecti-1000">Qualitative methodology: A practical</span>
     <span 
class="ecti-1000">guide</span>. Sage, 2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmirnig2017err"></a>[137] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Nicole Mirnig, Gerald Stollnberger, Markus Miksch, Susanne Stadler,
     Manuel Giuliani, and Manfred Tscheligi.  To err is robot: How humans
                                                                  

                                                                  
     assess and act toward an erroneous social robot. <span 
class="ecti-1000">Frontiers in Robotics and</span>
     <span 
class="ecti-1000">AI</span>, 4:21, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmorgan2017use"></a>[138] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Charity&#x00A0;J Morgan.  Use of proper statistical techniques for research
     studies with small samples. <span 
class="ecti-1000">American Journal of Physiology-Lung Cellular</span>
     <span 
class="ecti-1000">and Molecular Physiology</span>, 313(5):L873&#8211;L877, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xnoauthor_when_2018"></a>[139] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alana        Mothershaw.                       When        and        how
     you            should            reuse            research            participants.
     <a 
href="https://www.simpleusability.com/inspiration/2018/07/when-and-how-you-should-reuse-research-participants/" class="url" ><span 
class="ectt-1000">https://www.simpleusability.com/inspiration/2018/07/when-and-how-you-should-reuse-research-participants/</span></a>,
     7 2018. Accessed: 2020-07-27.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmoyle2017use"></a>[140] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Wendy  Moyle,  Cindy&#x00A0;J  Jones,  Jenny&#x00A0;E  Murfield,  Lukman  Thalib,
     Elizabeth&#x00A0;RA Beattie, David&#x00A0;KH Shum, Siobhan&#x00A0;T O&#8217;Dwyer, M&#x00A0;Cindy
     Mervin, and Brian&#x00A0;M Draper.   Use of a robotic seal as a therapeutic
     tool to improve dementia symptoms: a cluster-randomized controlled trial.
     <span 
class="ecti-1000">Journal of the American Medical Directors Association</span>, 18(9):766&#8211;773,
     2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmumm2011human"></a>[141] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jonathan Mumm and Bilge Mutlu. Human-robot proxemics: physical
     and psychological distancing in human-robot interaction.  In <span 
class="ecti-1000">Proceedings</span>
     <span 
class="ecti-1000">of the 6th international conference on Human-robot interaction</span>,  pages
     331&#8211;338, 2011.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xmurphy2013survey"></a>[142] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Robin&#x00A0;R  Murphy  and  Debra  Schreckenghost.    Survey  of  metrics
     for  human-robot  interaction.    In  <span 
class="ecti-1000">2013  8th  ACM/IEEE  International</span>
     <span 
class="ecti-1000">Conference on Human-Robot Interaction (HRI)</span>, pages 197&#8211;198. IEEE,
     2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xnimon2012statistical"></a>[143] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kim&#x00A0;F Nimon. Statistical assumptions of substantive analyses across
     the general linear model: a mini-review.  <span 
class="ecti-1000">Frontiers in psychology</span>, 3:322,
     2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xnomura2006measurement"></a>[144] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tatsuya Nomura, Tomohiro Suzuki, Takayuki Kanda, and Kensuke
                                                                  

                                                                  
     Kato.   Measurement  of  negative  attitudes  toward  robots.   <span 
class="ecti-1000">Interaction</span>
     <span 
class="ecti-1000">Studies</span>, 7(3):437&#8211;454, 2006.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xnovak2011psychophysiological"></a>[145] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Domen Novak, Matja&#382; Mihelj, and Marko Munih. Psychophysiological
     responses to different levels of cognitive and physical workload in haptic
     interaction. <span 
class="ecti-1000">Robotica</span>, 29(3):367&#8211;374, 2011.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xolsen2003metrics"></a>[146] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Dan&#x00A0;R  Olsen  and  Michael&#x00A0;A  Goodrich.    Metrics  for  evaluating
     human-robot  interactions.   In  <span 
class="ecti-1000">Proceedings  of  PERMIS</span>,  volume  2003,
     page&#x00A0;4, 2003.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xparker2010jstrips"></a>[147] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Chris  A.&#x00A0;C.  Parker  and  Elizabeth  Croft.    J-strips:  Haptic  joint
     limit  warnings  for  human-robot  interaction.    In  <span 
class="ecti-1000">Volume  8:  Dynamic</span>
     <span 
class="ecti-1000">Systems  and  Control,  Parts  A  and  B</span>,  page  789&#8211;795.  ASMEDC,  Jan
     2010.  ISBN 978-0-7918-4445-8.  doi: 10.1115/IMECE2010-40717.  URL
     <a 
href="https://asmedigitalcollection.asme.org/IMECE/proceedings/IMECE2010/44458/789/344324" class="url" ><span 
class="ectt-1000">https://asmedigitalcollection.asme.org/IMECE/proceedings/IMECE2010/44458/789/344324</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpatten2017understanding"></a>[148] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mildred&#x00A0;L  Patten  and  Michelle  Newhart.   <span 
class="ecti-1000">Understanding  research</span>
     <span 
class="ecti-1000">methods: An overview of the essentials</span>. Taylor &amp; Francis, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xphillips2018human"></a>[149] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Elizabeth Phillips, Xuan Zhao, Daniel Ullman, and Bertram&#x00A0;F Malle.
     What is human-like? decomposing robots&#8217; human-like appearance using
     the anthropomorphic robot (abot) database.  In <span 
class="ecti-1000">Proceedings of the 2018</span>
     <span 
class="ecti-1000">ACM/IEEE International Conference on Human-Robot Interaction</span>, pages
     105&#8211;113, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpitsch2009first"></a>[150] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Karola Pitsch, Hideaki Kuzuoka, Yuya Suzuki, Luise Sussenbach, Paul
     Luff, and Christian Heath.  &#8220;the first five seconds&#8221;: Contingent stepwise
     entry into an interaction as a means to secure sustained engagement in
     hri. In <span 
class="ecti-1000">RO-MAN 2009-The 18th IEEE International Symposium on Robot</span>
     <span 
class="ecti-1000">and Human Interactive Communication</span>, pages 985&#8211;991. IEEE, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xprewett2009workload"></a>[151] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Matthew&#x00A0;S Prewett, Kristin&#x00A0;N Saboe, Ryan&#x00A0;C Johnson, Michael&#x00A0;D
     Coovert, and Linda&#x00A0;R Elliott.  Workload in human-robot interaction: a
                                                                  

                                                                  
     review  of  manipulations  and  outcomes.   In  <span 
class="ecti-1000">Proceedings of the Human</span>
     <span 
class="ecti-1000">Factors  and  Ergonomics  Society  Annual  Meeting</span>,  volume&#x00A0;53,  pages
     1393&#8211;1397. SAGE Publications Sage CA: Los Angeles, CA, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xprewett2010managing"></a>[152] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Matthew&#x00A0;S  Prewett,  Ryan&#x00A0;C  Johnson,  Kristin&#x00A0;N  Saboe,  Linda&#x00A0;R
     Elliott,  and  Michael&#x00A0;D  Coovert.   Managing  workload  in  human&#8211;robot
     interaction: A review of empirical studies. <span 
class="ecti-1000">Computers in Human Behavior</span>,
     26(5):840&#8211;856, 2010.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xprice2015research"></a>[153] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Paul&#x00A0;C Price, Rajiv Jhangiani, I-Chant&#x00A0;A Chiang, et&#x00A0;al.  <span 
class="ecti-1000">Research</span>
     <span 
class="ecti-1000">methods in psychology</span>. BCCampus, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xpuljiz2018implementation"></a>[154] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>David  Puljiz,  Gleb  Gorbachev,  and  Bjrn  Hein.    Implementation
     of   augmented   reality   in   autonomous   warehouses:   challenges   and
     opportunities. <span 
class="ecti-1000">arXiv preprint arXiv:1806.00324</span>, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrahimi_research_1990"></a>[155] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mansour   Rahimi   and   Waldemar   Karwowski.        A   research
     paradigm    in    human-robot    interaction.           5(1):59&#8211;71,    1990.
     ISSN   0169-8141.        doi:   10.1016/0169-8141(90)90028-Z.        URL
     <a 
href="http://www.sciencedirect.com/science/article/pii/016981419090028Z" class="url" ><span 
class="ectt-1000">http://www.sciencedirect.com/science/article/pii/016981419090028Z</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xralph2018construct"></a>[156] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Paul  Ralph  and  Ewan  Tempero.    Construct  validity  in  software
     engineering research and software metrics.   In <span 
class="ecti-1000">Proceedings of the 22nd</span>
     <span 
class="ecti-1000">International  Conference  on  Evaluation  and  Assessment  in  Software</span>
     <span 
class="ecti-1000">Engineering 2018</span>, pages 13&#8211;23, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xramachandran2020mathematical"></a>[157] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kandethody&#x00A0;M Ramachandran and Chris&#x00A0;P Tsokos.   <span 
class="ecti-1000">Mathematical</span>
     <span 
class="ecti-1000">statistics with applications in R</span>. Academic Press, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XRau2013"></a>[158] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Pei-Luen&#x00A0;Patrick Rau, Ye&#x00A0;Li, and Jun Liu. Effects of a social robot&#8217;s
     autonomy                                                                              and
     group orientation on human decision-making, Dec 2013. ISSN 1687-5893.
     URL <a 
href="https://www.hindawi.com/journals/ahci/2013/263721/" class="url" ><span 
class="ectt-1000">https://www.hindawi.com/journals/ahci/2013/263721/</span></a>.  DOI:
     https://doi.org/10.1155/2013/263721.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrea2017wizard"></a>[159] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Daniel&#x00A0;J Rea, Denise Geiskkovitch, and James&#x00A0;E Young.  Wizard of
     awwws: Exploring psychological impact on the researchers in social hri
     experiments.  In <span 
class="ecti-1000">Proceedings of the Companion of the 2017 ACM/IEEE</span>
     <span 
class="ecti-1000">International Conference on Human-Robot Interaction</span>, pages 21&#8211;29, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xreason1990human"></a>[160] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>James Reason. <span 
class="ecti-1000">Human error</span>. Cambridge university press, 1990.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrehfeld2006training"></a>[161] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Sherri&#x00A0;A.  Rehfeld.   The  impact  of  mental  transformation  training
     across  levels  of  automation  on  spatial  awareness  in  human-robot
     interaction. 2006.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xriek2012wizard"></a>[162] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Laurel&#x00A0;D Riek.  Wizard of Oz studies in HRI: a systematic review
     and new reporting guidelines. <span 
class="ecti-1000">Journal of Human-Robot Interaction</span>, 1(1):
     119&#8211;136, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XRind_2011"></a>[163] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Alexander            Rind.                                 Some            whys
     and hows of experiments in human&#8211;computer interaction.  <span 
class="ecti-1000">Foundations</span>
     <span 
class="ecti-1000">and Trends</span><span 
class="ecti-1000"> in Human&#8211;Computer Interaction</span>, 5(4):299&#8211;373, 2011. ISSN
     1551-3955, 1551-3963. doi: 10.1561/1100000043.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrobinson2018measures"></a>[164] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Nicole&#x00A0;L Robinson, Jennifer Connolly, Genevieve&#x00A0;M Johnson, Yejee
     Kim, Leanne Hides, and David&#x00A0;J Kavanagh.  Measures of incentives and
     confidence in using a social robot. <span 
class="ecti-1000">Science Robotics</span>, 3(21):Article&#8211;number,
     2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrobinson2020robot"></a>[165] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Nicole&#x00A0;L Robinson, Teah-Neal Hicks, Gavin Suddrey, and David&#x00A0;J
     Kavanagh. The robot self-efficacy scale: Robot self-efficacy, likability and
     willingness to interact increases after a robot-delivered tutorial.  In <span 
class="ecti-1000">2020</span>
     <span 
class="ecti-1000">29th IEEE International Conference on Robot and Human Interactive</span>
     <span 
class="ecti-1000">Communication (RO-MAN)</span>, pages 272&#8211;277. IEEE, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrobinson2019psychosocial"></a>[166] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Nicole&#x00A0;Lee  Robinson,  Timothy&#x00A0;Vaughan  Cottier,  and  David&#x00A0;John
                                                                  

                                                                  
     Kavanagh. Psychosocial health interventions by social robots: systematic
     review  of  randomized  controlled  trials.    <span 
class="ecti-1000">Journal  of  medical  Internet</span>
     <span 
class="ecti-1000">research</span>, 21(5):e13203, 2019.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xroncone2017transparent"></a>[167] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>A.&#x00A0;Roncone,  O.&#x00A0;Mangin,  and  B.&#x00A0;Scassellati.     Transparent  role
     assignment and task allocation in human robot collaboration.  In <span 
class="ecti-1000">2017</span>
     <span 
class="ecti-1000">IEEE  International  Conference  on  Robotics  and  Automation  (ICRA)</span>,
     pages 1014&#8211;1021, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrosenthal-vonderputten_experimental_2013"></a>[168] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Astrid&#x00A0;M.   Rosenthal-von&#x00A0;der&#x00A0;Ptten,   Nicole&#x00A0;C.   Krmer,   Laura
     Hoffmann, Sabrina Sobieraj, and Sabrina&#x00A0;C. Eimler.  An experimental
     study on emotional reactions towards a robot.  <span 
class="ecti-1000">International Journal of</span>
     <span 
class="ecti-1000">Social Robotics</span>, 5(1):17&#8211;34, Jan 2013.   ISSN 1875-4805.   doi: 10.1007/
     s12369-012-0173-8.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrueben2020introduction"></a>[169] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Matthew Rueben, Shirley&#x00A0;A Elprama, Dimitrios Chrysostomou, and
     An&#x00A0;Jacobs.   Introduction  to  (re)  using  questionnaires  in  human-robot
     interaction  research.     In  <span 
class="ecti-1000">Human-Robot  Interaction</span>,  pages  125&#8211;144.
     Springer, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrussell2000log"></a>[170] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Craig&#x00A0;J Russell and Michelle&#x00A0;A Dean. To log or not to log: Bootstrap
     as an alternative to the parametric estimation of moderation effects in the
     presence of skewed dependent variables. <span 
class="ecti-1000">Organizational Research Methods</span>,
     3(2):166&#8211;185, 2000.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xrust2014modern"></a>[171] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>John Rust and Susan Golombok. <span 
class="ecti-1000">Modern psychometrics: The science</span>
     <span 
class="ecti-1000">of psychological assessment</span>. Routledge, 2014.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XRuxtonG.2017Acaa"></a>[172] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>G.&#x00A0;Ruxton.  Allocation concealment as a potentially useful aspect of
     randomised experiments. <span 
class="ecti-1000">Behavioral Ecology and Sociobiology</span>, 71(2):1&#8211;3,
     2017. ISSN 0340-5443.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsalkind2010encyclopedia"></a>[173] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Neil&#x00A0;J Salkind. <span 
class="ecti-1000">Encyclopedia of research design</span>, volume&#x00A0;1. Sage, 2010.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsalmon2009measuring"></a>[174] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Paul&#x00A0;M Salmon, Neville&#x00A0;A Stanton, Guy&#x00A0;H Walker, Daniel Jenkins,
     Darshna   Ladva,   Laura   Rafferty,   and   Mark   Young.      Measuring
     situation awareness in complex systems: Comparison of measures study.
     <span 
class="ecti-1000">International Journal of Industrial Ergonomics</span>, 39(3):490&#8211;500, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsandygulova2020cowriting"></a>[175] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Anara Sandygulova, Wafa Johal, Zhanel Zhexenova, Bolat Tleubayev,
     Aida   Zhanatkyzy,   Aizada   Turarova,   Zhansaule   Telisheva,   Anna
     CohenMiller,  Thibault  Asselborn,  and  Pierre  Dillenbourg.   Cowriting
     kazakh: Learning a new script with a robot.  In <span 
class="ecti-1000">Proceedings of the 2020</span>
     <span 
class="ecti-1000">ACM/IEEE International Conference on Human-Robot Interaction</span>, pages
     113&#8211;120, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xschad2019toward"></a>[176] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Daniel&#x00A0;J Schad, Michael Betancourt, and Shravan Vasishth.  Toward
     a  principled  bayesian  workflow  in  cognitive  science.    <span 
class="ecti-1000">arXiv  preprint</span>
     <span 
class="ecti-1000">arXiv:1904.12765</span>, 2019.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xschaefer2013perception"></a>[177] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>KRISTIN&#x00A0;E             SCHAEFER.                                      <span 
class="ecti-1000">THE</span>
     <span 
class="ecti-1000">PERCEPTION AND MEASUREMENT OF HUMAN-ROBOT TRUST</span>.
     PhD thesis, University of Central Florida Orlando, Florida, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xscheggi2014vibrotactile"></a>[178] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Stefano   Scheggi,   Fabio   Morbidi,   and   Domenico   Prattichizzo.
     Human-robot  formation  control  via  visual  and  vibrotactile  haptic
     feedback.  <span 
class="ecti-1000">IEEE Transactions on Haptics</span>, 7(4):499&#8211;511, Oct 2014.  ISSN
     2329-4051. doi: 10.1109/TOH.2014.2332173.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xscholtz2003theory"></a>[179] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jean Scholtz. Theory and evaluation of human robot interactions. In
     <span 
class="ecti-1000">36th Annual Hawaii International Conference on System Sciences, 2003.</span>
     <span 
class="ecti-1000">Proceedings of the</span>, pages 10&#8211;pp. IEEE, 2003.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xschrum2020four"></a>[180] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mariah&#x00A0;L Schrum, Michael Johnson, Muyleng Ghuy, and Matthew&#x00A0;C
     Gombolay.  Four years in review: Statistical practices of Likert scales in
     human-robot interaction studies. <span 
class="ecti-1000">arXiv preprint arXiv:2001.03231</span>, 2020.
     </p>
                                                                  

                                                                  
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xschulz2002allocation"></a>[181] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kenneth&#x00A0;F Schulz and David&#x00A0;A Grimes.  Allocation concealment in
     randomised trials: defending against deciphering. <span 
class="ecti-1000">The Lancet</span>, 359(9306):
     614&#8211;618, 2002.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsefidgar2012tamer"></a>[182] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Yasaman&#x00A0;Sadat    Sefidgar.          <span 
class="ecti-1000">TAMER:   touch-guided   anxiety</span>
     <span 
class="ecti-1000">management    via    engagement    with    a    robotic    pet    efficacy</span>
     <span 
class="ecti-1000">evaluation    and    the    first    steps    of    the    interaction    design</span>.
     PhD    thesis,    University    of    British    Columbia,    2012.         URL
     <a 
href="https://open.library.ubc.ca/cIRcle/collections/ubctheses/24/items/1.0052145" class="url" ><span 
class="ectt-1000">https://open.library.ubc.ca/cIRcle/collections/ubctheses/24/items/1.0052145</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xseibt_towards_2017"></a>[183] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Johanna Seibt.  Towards an ontology of simulated social interaction:
     Varieties  of  the  &#8220;as  if&#8221;   for  robots  and  humans.     In  Raul  Hakli
     and  Johanna  Seibt,  editors,  <span 
class="ecti-1000">Sociality  and  Normativity  for  Robots:</span>
     <span 
class="ecti-1000">Philosophical  Inquiries  into  Human-Robot  Interactions</span>,  Studies  in  the
     Philosophy of Sociality, pages 11&#8211;39. Springer International Publishing,
     2017. ISBN 978-3-319-53133-5. doi: 10.1007/978-3-319-53133-5_2. URL
     <a 
href="https://doi.org/10.1007/978-3-319-53133-5_2" class="url" ><span 
class="ectt-1000">https://doi.org/10.1007/978-3-319-53133-5_2</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsena2020quantifying"></a>[184] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Aran Sena and Matthew Howard.  Quantifying teaching behavior in
     robot learning from demonstration. <span 
class="ecti-1000">The International Journal of Robotics</span>
     <span 
class="ecti-1000">Research</span>, 39(1):54&#8211;72, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xshao2020user"></a>[185] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Mingyang Shao, Matt Snyder, Goldie Nejat, and Beno Benhabib. User
     affect elicitation with a socially emotional robot. <span 
class="ecti-1000">Robotics</span>, 9(2):44, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsheskin2020handbook"></a>[186] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>David&#x00A0;J  Sheskin.     <span 
class="ecti-1000">Handbook  of  parametric  and  nonparametric</span>
     <span 
class="ecti-1000">statistical procedures</span>. crc Press, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xshim_taxonomy_2013"></a>[187] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jaeeun Shim and Ronald&#x00A0;C. Arkin.  A taxonomy of robot deception
     and  its  benefits  in  HRI.   In  <span 
class="ecti-1000">2013  IEEE  International  Conference  on</span>
     <span 
class="ecti-1000">Systems, Man, and Cybernetics</span>, pages 2328&#8211;2335, 2013.   doi: 10.1109/
     SMC.2013.398. ISSN: 1062-922X.
     </p>
                                                                  

                                                                  
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xshort2010no"></a>[188] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Elaine Short, Justin Hart, Michelle Vu, and Brian Scassellati.   No
     fair!!  an  interaction  with  a  cheating  robot.   In  <span 
class="ecti-1000">2010  5th  ACM/IEEE</span>
     <span 
class="ecti-1000">International  Conference  on  Human-Robot  Interaction  (HRI)</span>,  pages
     219&#8211;226. IEEE, 2010.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsmith2012interpretative"></a>[189] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jonathan&#x00A0;A   Smith   and   Pnina   Shinebourne.        <span 
class="ecti-1000">Interpretative</span>
     <span 
class="ecti-1000">phenomenological analysis. </span>American Psychological Association, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsmith2008ethical"></a>[190] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Linda&#x00A0;J   Smith.      How   ethical   is   ethical   research?   recruiting
     marginalized, vulnerable groups into health services research.  <span 
class="ecti-1000">Journal of</span>
     <span 
class="ecti-1000">Advanced nursing</span>, 62(2):248&#8211;257, 2008.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsteinbauer2012survey"></a>[191] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Gerald Steinbauer.  A survey about faults of robots used in robocup.
     In <span 
class="ecti-1000">Robot Soccer World Cup</span>, pages 344&#8211;355. Springer, 2012.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsteinfeld_common_2006"></a>[192] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Aaron                                                                       Steinfeld,
     Terrence Fong, David Kaber, Michael Lewis, Jean Scholtz, Alan Schultz,
     and Michael Goodrich. Common metrics for human-robot interaction. In
     <span 
class="ecti-1000">Proceedings of the 1st ACM SIGCHI/SIGART conference on Human-robot</span>
     <span 
class="ecti-1000">interaction</span>, HRI &#8217;06, pages 33&#8211;40. Association for Computing Machinery,
     2006.   ISBN  978-1-59593-294-5.   doi:  10.1145/1121241.1121249.   URL
     <a 
href="https://doi.org/10.1145/1121241.1121249" class="url" ><span 
class="ectt-1000">https://doi.org/10.1145/1121241.1121249</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsullivan2013analyzing"></a>[193] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Gail&#x00A0;M   Sullivan   and   Anthony&#x00A0;R   Artino&#x00A0;Jr.      Analyzing   and
     interpreting  data  from  likert-type  scales.   <span 
class="ecti-1000">Journal of graduate medical</span>
     <span 
class="ecti-1000">education</span>, 5(4):541&#8211;542, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsung2009robots"></a>[194] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>JaYoung Sung, Henrik&#x00A0;I Christensen, and Rebecca&#x00A0;E Grinter. Robots
     in  the  wild:  understanding  long-term  use.   In  <span 
class="ecti-1000">Proceedings  of  the  4th</span>
     <span 
class="ecti-1000">ACM/IEEE international conference on Human robot interaction</span>, pages
     45&#8211;52, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xsuter2011introduction"></a>[195] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>W&#x00A0;Newton  Suter.   <span 
class="ecti-1000">Introduction  to  educational  research:  A  critical</span>
     <span 
class="ecti-1000">thinking approach</span>. SAGE publications, 2011.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xswanson2015research"></a>[196] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>David&#x00A0;M Swanson and Rebecca&#x00A0;A Betensky.   Research participant
     compensation:  a  matter  of  statistical  inference  as  well  as  ethics.
     <span 
class="ecti-1000">Contemporary clinical trials</span>, 45:265&#8211;269, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xszafir2019mediating"></a>[197] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Daniel  Szafir.    Mediating  human-robot  interactions  with  virtual,
     augmented,   and   mixed   reality.      In   <span 
class="ecti-1000">International  Conference  on</span>
     <span 
class="ecti-1000">Human-Computer Interaction</span>, pages 124&#8211;149. Springer, 2019.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xtabachnick2007using"></a>[198] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Barbara&#x00A0;G Tabachnick, Linda&#x00A0;S Fidell, and Jodie&#x00A0;B Ullman.  <span 
class="ecti-1000">Using</span>
     <span 
class="ecti-1000">multivariate statistics</span>, volume&#x00A0;5. Pearson Boston, MA, 2007.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xtakayama_influences_2009"></a>[199] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Leila  Takayama  and  Caroline  Pantofaru.    Influences  on  proxemic
     behaviors in human-robot interaction.  In <span 
class="ecti-1000">2009 IEEE/RSJ International</span>
     <span 
class="ecti-1000">Conference on Intelligent Robots and Systems</span>, page 5495&#8211;5502, Oct 2009.
     doi: 10.1109/IROS.2009.5354145.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xtanevska2020socially"></a>[200] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Ana Tanevska, Francesco Rea, Giulio Sandini, Lola Caamero, and
     Alessandra  Sciutti.   A  socially  adaptable  framework  for  human-robot
     interaction. <span 
class="ecti-1000">arXiv preprint arXiv:2003.11410</span>, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xtapus2009use"></a>[201] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Adriana  Tapus,  Cristian  Tapus,  and  Maja&#x00A0;J  Mataric.   The  use  of
     socially assistive robots in the design of intelligent cognitive therapies
     for  people  with  dementia.   In  <span 
class="ecti-1000">2009 IEEE international conference on</span>
     <span 
class="ecti-1000">rehabilitation robotics</span>, pages 924&#8211;929. IEEE, 2009.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xthabane2010tutorial"></a>[202] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lehana  Thabane,  Jinhui  Ma,  Rong  Chu,  Ji&#x00A0;Cheng,  Afisi  Ismaila,
     Lorena&#x00A0;P Rios, Reid Robson, Marroon Thabane, Lora Giangregorio, and
     Charles&#x00A0;H Goldsmith. A tutorial on pilot studies: the what, why and how.
     <span 
class="ecti-1000">BMC medical research methodology</span>, 10(1):1, 2010.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xthomas2017validity"></a>[203] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Kyle&#x00A0;A Thomas and Scott Clifford. Validity and mechanical turk: An
     assessment of exclusion methods and interactive experiments. <span 
class="ecti-1000">Computers</span>
     <span 
class="ecti-1000">in Human Behavior</span>, 77:184&#8211;197, 2017.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xthomaz_computational_2016"></a>[204] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Andrea Thomaz, Guy Hoffman, and Maya Cakmak.  Computational
     human-robot  interaction.   4(2):105&#8211;223,  2016.   ISSN  1935-8253.   doi:
     10.1561/2300000049. URL <a 
href="https://doi.org/10.1561/2300000049" class="url" ><span 
class="ectt-1000">https://doi.org/10.1561/2300000049</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xthompson1994common"></a>[205] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Bruce Thompson.  Common methodology mistakes in dissertations,
     revisited. 1994.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xthrun2004toward"></a>[206] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Sebastian Thrun.  Toward a framework for human-robot interaction.
     <span 
class="ecti-1000">Human&#8211;Computer Interaction</span>, 19(1-2):9&#8211;24, 2004.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xtiberio2013psychophysiological"></a>[207] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lorenza  Tiberio,  Amedeo  Cesta,  and  Marta  Olivetti&#x00A0;Belardinelli.
     Psychophysiological methods to evaluate user&#8217;s response in human robot
     interaction: a review and feasibility study. <span 
class="ecti-1000">Robotics</span>, 2(2):92&#8211;121, 2013.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xtozadore2018adaptation"></a>[208] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Daniel&#x00A0;C.  Tozadore,  Joao&#x00A0;P.H.  Valentini,  Victor&#x00A0;H.S.  Rodrigues,
     Fernando&#x00A0;M.L.  Vendrameto,  Rodrigo&#x00A0;G.  Zavarizz,  and  Roseli&#x00A0;A.F.
     Romero.    Towards  adaptation  and  personalization  in  task  based  on
     human-robot interaction.  In <span 
class="ecti-1000">2018 Latin American Robotic Symposium,</span>
     <span 
class="ecti-1000">2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on</span>
     <span 
class="ecti-1000">Robotics in Education (WRE)</span>, page 383&#8211;389, Nov 2018.  doi: 10.1109/
     LARS/SBR/WRE.2018.00075.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xtraeger2020vulnerable"></a>[209] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Margaret&#x00A0;L  Traeger,  Sarah&#x00A0;Strohkorb  Sebo,  Malte  Jung,  Brian
     Scassellati, and Nicholas&#x00A0;A Christakis. Vulnerable robots positively shape
     human conversational dynamics in a human&#8211;robot team.  <span 
class="ecti-1000">Proceedings of</span>
     <span 
class="ecti-1000">the National Academy of Sciences</span>, 117(12):6370&#8211;6375, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xvan2005data"></a>[210] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Jan Van&#x00A0;den Broeck, Solveig&#x00A0;Argeseanu Cunningham, Roger Eeckels,
     and Kobus Herbst. Data cleaning: detecting, diagnosing, and editing data
     abnormalities. <span 
class="ecti-1000">PLoS Med</span>, 2(10):e267, 2005.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xvan2002importance"></a>[211] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Edwin Van&#x00A0;Teijlingen and Vanora Hundley.  The importance of pilot
     studies. <span 
class="ecti-1000">Nursing Standard (through 2013)</span>, 16(40):33, 2002.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xvandekerckhove2018bayesian"></a>[212] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Joachim Vandekerckhove, Jeffrey&#x00A0;N Rouder, and John&#x00A0;K Kruschke.
     Bayesian methods for advancing psychological science, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xvenero2020experimental"></a>[213] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Renato&#x00A0;Paredes  Venero  and  Alex  Davila.    Experimental  research
     methodology and statistics insights.  In <span 
class="ecti-1000">Human-Robot Interaction</span>, pages
     333&#8211;353. Springer, 2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwalters2007robotic"></a>[214] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Michael&#x00A0;L  Walters,  Kerstin  Dautenhahn,  Sarah&#x00A0;N  Woods,  and
     Kheng&#x00A0;Lee Koay. Robotic etiquette: results from user studies involving a
     fetch and carry task.  In <span 
class="ecti-1000">2007 2nd ACM/IEEE International Conference</span>
     <span 
class="ecti-1000">on Human-Robot Interaction (HRI)</span>, pages 317&#8211;324. IEEE, 2007.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwang2010rome"></a>[215] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lin Wang, Pei-Luen&#x00A0;Patrick Rau, Vanessa Evers, Benjamin&#x00A0;Krisper
     Robinson,  and  Pamela  Hinds.   When  in  rome:  the  role  of  culture  &amp;
     context in adherence to robot recommendations. In <span 
class="ecti-1000">2010 5th ACM/IEEE</span>
     <span 
class="ecti-1000">International  Conference  on  Human-Robot  Interaction  (HRI)</span>,  pages
     359&#8211;366. IEEE, 2010.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwang2020arespace"></a>[216] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Y.&#x00A0;Wang,  F.&#x00A0;Guimbretire,  and  K.&#x00A0;E.  Green.   Are  space-making
     robots, agents? investigations on user perception of an embedded robotic
     surface.   In  <span 
class="ecti-1000">2020  29th  IEEE  International  Conference  on  Robot  and</span>
     <span 
class="ecti-1000">Human Interactive Communication (RO-MAN)</span>, pages 1230&#8211;1235, 2020.
     doi: 10.1109/RO-MAN47096.2020.9223532.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xweiss2015meta"></a>[217] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Astrid Weiss and Christoph Bartneck.   Meta analysis of the usage
     of the godspeed questionnaire series.  In <span 
class="ecti-1000">2015 24th IEEE International</span>
     <span 
class="ecti-1000">Symposium on Robot and Human Interactive Communication (RO-MAN)</span>,
     pages 381&#8211;388. IEEE, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xweiss2009usus"></a>[218] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Astrid  Weiss,  Regina  Bernhaupt,  Michael  Lankes,  and  Manfred
     Tscheligi.  The USUS evaluation framework for human-robot interaction.
     In  <span 
class="ecti-1000">AISB2009:  proceedings  of  the  symposium  on  new  frontiers  in</span>
     <span 
class="ecti-1000">human-robot interaction</span>, volume&#x00A0;4, pages 11&#8211;26, 2009.
                                                                  

                                                                  
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xweissgerber2015beyond"></a>[219] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tracey&#x00A0;L  Weissgerber,  Natasa&#x00A0;M  Milic,  Stacey&#x00A0;J  Winham,  and
     Vesna&#x00A0;D  Garovic.   Beyond  bar  and  line  graphs:  time  for  a  new  data
     presentation paradigm. <span 
class="ecti-1000">PLoS biology</span>, 13(4), 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xweissgerber2017data"></a>[220] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Tracey&#x00A0;L  Weissgerber,  Marko  Savic,  Stacey&#x00A0;J  Winham,  Dejana
     Stanisavljevic, Vesna&#x00A0;D Garovic, and Natasa&#x00A0;M Milic. Data visualization,
     bar  naked:  A  free  tool  for  creating  interactive  graphics.   <span 
class="ecti-1000">Journal  of</span>
     <span 
class="ecti-1000">Biological Chemistry</span>, 292(50):20592&#8211;20598, 2017.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwerner2020survey"></a>[221] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Franz Werner.  A survey on current practices in user evaluation of
     companion robots.  In <span 
class="ecti-1000">Human-Robot Interaction</span>, pages 65&#8211;88. Springer,
     2020.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwilcox2018guide"></a>[222] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Rand&#x00A0;R  Wilcox  and  Guillaume&#x00A0;A  Rousselet.   A  guide  to  robust
     statistical methods in neuroscience. <span 
class="ecti-1000">Current protocols in neuroscience</span>, 82
     (1):8&#8211;42, 2018.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xwogalter2006handbook"></a>[223] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Michael&#x00A0;S Wogalter. <span 
class="ecti-1000">Handbook of warnings</span>. CRC Press, 2006.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XWoodLesley2008Eeob"></a>[224] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Lesley Wood, Matthias Egger, Lise&#x00A0;Lotte Gluud, Kenneth&#x00A0;F Schulz,
     Peter                                  Jni,                                  Douglas&#x00A0;G
     Altman, Christian Gluud, Richard&#x00A0;M Martin, Anthony J&#x00A0;G Wood, and
     Jonathan A&#x00A0;C Sterne.   Empirical evidence of bias in treatment effect
     estimates in controlled trials with different interventions and outcomes:
     meta-epidemiological study. <span 
class="ecti-1000">BMJ</span>, 336(7644):601, 2008. ISSN 0959-8138.
     URL <a 
href="http://bmj.com/content/336/7644/601.full.pdf" class="url" ><span 
class="ectt-1000">http://bmj.com/content/336/7644/601.full.pdf</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XWrede2013"></a>[225] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Sebastian  Wrede,  Christian  Emmerich,  Ricarda  Grnberg,  Arne
     Nordmann, Agnes Swadzba, and Jochen Steil. A user study on kinesthetic
     teaching of redundant robots in task and configuration space.  <span 
class="ecti-1000">Journal</span>
     <span 
class="ecti-1000">of Human-Robot Interaction</span>, 2(1):56&#8211;81, Mar 2013. ISSN 21630364. doi:
     10.5898/JHRI.2.1.Wrede.
     </p>
                                                                  

                                                                  
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xyamashita2016path"></a>[226] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Yuki Yamashita, Hisashi Ishihara, Takashi Ikeda, and Minoru Asada.
     Path analysis for the halo effect of touch sensations of robots on their
     personality impressions.  In <span 
class="ecti-1000">International Conference on Social Robotics</span>,
     pages 502&#8211;512. Springer, 2016.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xyanco_classifying_2004"></a>[227] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>H.A.  Yanco  and  J.&#x00A0;Drury.    Classifying  human-robot  interaction:
     an  updated  taxonomy.    In  <span 
class="ecti-1000">2004  IEEE  International  Conference  on</span>
     <span 
class="ecti-1000">Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)</span>, volume&#x00A0;3,
     pages 2841&#8211;2846 vol.3, 2004.  doi: 10.1109/ICSMC.2004.1400763.  ISSN:
     1062-922X.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="XYancoJFR2015"></a>[228] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Holly&#x00A0;A  Yanco,  Adam  Norton,  Willard  Ober,  David  Shane,  Anna
     Skinner, and Jack Vice. Analysis of human-robot interaction at the darpa
     robotics challenge trials. <span 
class="ecti-1000">Journal of Field Robotics</span>, 2015.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xyongda2018speechAndGesture"></a>[229] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Deng  Yongda,  Li&#x00A0;Fang,  and  Xin  Huang.   Research  on  multimodal
     human-robot  interaction  based  on  speech  and  gesture.   <span 
class="ecti-1000">Computers  &amp;</span>
     <span 
class="ecti-1000">Electrical  Engineering</span>,  72:443&#8211;454,  Nov  2018.    ISSN  00457906.    doi:
     10.1016/j.compeleceng.2018.09.014.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xyoung_evaluating_2011"></a>[230] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>James&#x00A0;E. Young, JaYoung Sung, Amy Voida, Ehud Sharlin, Takeo
     Igarashi,  Henrik&#x00A0;I.  Christensen,  and  Rebecca&#x00A0;E.  Grinter.   Evaluating
     human-robot                                                                interaction.
     3(1):53&#8211;67, 2011. ISSN 1875-4805. doi: 10.1007/s12369-010-0081-8. URL
     <a 
href="https://doi.org/10.1007/s12369-010-0081-8" class="url" ><span 
class="ectt-1000">https://doi.org/10.1007/s12369-010-0081-8</span></a>.
     </p>
     <p class="bibitem" ><span class="biblabel">
<a 
 id="Xzafrani_towards_2019"></a>[231] <span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span>Oded    Zafrani    and    Galit    Nimrod.         Towards    a    Holistic
     Approach    to    Studying    Human-Robot    Interaction    in    Later
     Life.            <span 
class="ecti-1000">The    Gerontologist</span>,     59(1):e26&#8211;e36,     January     2019.
     ISSN     0016-9013.            doi:     10.1093/geront/gny077.            URL
     <a 
href="https://academic.oup.com/gerontologist/article/59/1/e26/5055003" class="url" ><span 
class="ectt-1000">https://academic.oup.com/gerontologist/article/59/1/e26/5055003</span></a>.
     Publisher: Oxford Academic.
</p>
     </div>
    
</body></html> 

                                                                  

                                                                  
                                                                  


